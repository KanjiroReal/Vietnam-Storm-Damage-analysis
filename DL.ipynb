{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CEPK3_hihBRf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from src.utils import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ppt-nRO6iYj-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/STORM_preprocessed_medianfill_1.csv', index_col=0) # 200 column\n",
        "from src.const import CATEGORICAL_TARGETS, ATTRIBUTES, LINEAR_TARGETS\n",
        "evaluate_dict = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1faU7Q2HUFo"
      },
      "source": [
        "| Mạng Deep Learning     | Điểm Mạnh                                                 | Điểm Yếu                                               | Khi Nào Dùng                                      |\n",
        "|------------------------|-----------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------|\n",
        "| **MLP (Multi-Layer Perceptron)** | - Đơn giản, dễ triển khai<br>- Hiệu quả cho dữ liệu nhỏ, đơn giản | - Không tốt cho dữ liệu có cấu trúc phức tạp<br>- Dễ overfitting với dữ liệu lớn | - Dữ liệu tabular (bảng)<br>- Khi không cần xử lý dữ liệu tuần tự hoặc không gian |\n",
        "| **RNN (Recurrent Neural Network)** | - Tốt cho dữ liệu tuần tự<br>- Phân tích chuỗi thời gian hiệu quả | - Khó huấn luyện do vanishing gradient<br>- Chậm khi xử lý chuỗi dài | - Dự báo chuỗi thời gian<br>- Phân tích lịch sử giao dịch hoặc chuỗi sự kiện |\n",
        "| **LSTM (Long Short-Term Memory)** | - Giải quyết vanishing gradient của RNN<br>- Ghi nhớ thông tin dài hạn tốt | - Tốn nhiều tài nguyên tính toán<br>- Khó tinh chỉnh | - Chuỗi thời gian dài<br>- Khi cần ghi nhớ các sự kiện quan trọng từ xa |\n",
        "| **GRU (Gated Recurrent Unit)** | - Nhẹ và nhanh hơn LSTM<br>- Hiệu quả với chuỗi ngắn | - Khả năng biểu diễn thông tin dài hạn kém hơn LSTM | - Khi cần tốc độ nhanh hơn LSTM<br>- Chuỗi thời gian ngắn |\n",
        "| **CNN (Convolutional Neural Network)** | - Khả năng trích xuất đặc trưng mạnh<br>- Phù hợp với dữ liệu hình ảnh và không gian | - Không hiệu quả với dữ liệu tuần tự | - Hồi quy trên hình ảnh (VD: dự đoán giá từ ảnh)<br>- Phân tích dữ liệu không gian |\n",
        "| **TabTransformer** | - Mô hình hóa quan hệ phức tạp với cơ chế attention.<br>- Tự động hóa trong việc học tham số. | - Tốn nhiều tài nguyên tính toán<br>- Cần nhiều dữ liệu để tránh overfitting | - Khi cần xây dựng mạng sâu<br>- Khi cần độ chính xác cao. |\n",
        "| **TabNet**             | - Hiệu quả với dữ liệu bảng (tabular)<br>- Có thể giải thích mô hình nhờ cơ chế chú ý (attention) | - Khó tinh chỉnh và tối ưu<br>- Cần nhiều dữ liệu hơn so với MLP | - Khi cần mô hình vừa mạnh vừa có thể giải thích<br>- Phù hợp với các bài toán dữ liệu bảng phức tạp |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Selection for Hurricane Data Regression\n",
        "\n",
        "1. **`TabNet`** \n",
        "- TabNet is highly effective for **structured tabular data**, such as historical hurricane records where features may include wind speed, pressure, sea surface temperature, and atmospheric conditions.\n",
        "- It uses a **sequential attention mechanism** that allows the model to focus on relevant features at different steps, improving interpretability.\n",
        "- Unlike traditional neural networks, TabNet balances both **accuracy and interpretability**, making it useful when understanding the contribution of each feature to predictions is essential.\n",
        "\n",
        "2. **`TabTransformer`**\n",
        "- TabTransformer is a powerful architecture for **tabular data**, designed to learn complex relationships between features through the **attention mechanism**.\n",
        "- It allows the model to automatically identify and learn interactions between different features, enhancing predictive capability for regression and classification tasks.\n",
        "- With the ability to handle **non-linear relationships** and **various types of data**, TabTransformer is a good choice for tasks that require high accuracy and deep understanding of the data.\n",
        "\n",
        "\n",
        "3. **`LSTM (Long Short-Term Memory)`**\n",
        "- LSTM networks are ideal for **time-series data** because they can **remember long-term dependencies** and handle sequential relationships effectively. \n",
        "- Unlike standard RNNs, LSTM avoids the **vanishing gradient problem**, making it suitable for learning patterns in long-term hurricane data.\n",
        "- Since hurricanes are influenced by **seasonal cycles and long-term climatic trends**, LSTM is a great choice to capture these **temporal dependencies** across years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuFr44h5jMpG"
      },
      "source": [
        "## 1. Target 1 : TotalDeaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zZBHiCmei0l2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[0]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd9rns0m0Og"
      },
      "source": [
        "### 1.1. TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4puM7fcnHwn",
        "outputId": "db745d5c-f147-4155-fb9f-b4389ba8c7dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 376441.50232| val_0_rmse: 100.54486|  0:00:00s\n",
            "epoch 1  | loss: 375458.58862| val_0_rmse: 99.87958|  0:00:00s\n",
            "epoch 2  | loss: 274127.84131| val_0_rmse: 99.18263|  0:00:00s\n",
            "epoch 3  | loss: 373156.69751| val_0_rmse: 98.65387|  0:00:00s\n",
            "epoch 4  | loss: 369642.80573| val_0_rmse: 98.07378|  0:00:00s\n",
            "epoch 5  | loss: 377426.93372| val_0_rmse: 96.40723|  0:00:00s\n",
            "epoch 6  | loss: 372532.89429| val_0_rmse: 94.22036|  0:00:01s\n",
            "epoch 7  | loss: 374651.86304| val_0_rmse: 94.0992 |  0:00:01s\n",
            "epoch 8  | loss: 313433.85742| val_0_rmse: 90.95168|  0:00:01s\n",
            "epoch 9  | loss: 367165.26538| val_0_rmse: 89.04234|  0:00:01s\n",
            "epoch 10 | loss: 360553.55435| val_0_rmse: 88.12437|  0:00:01s\n",
            "epoch 11 | loss: 370285.04657| val_0_rmse: 88.4688 |  0:00:01s\n",
            "epoch 12 | loss: 365319.63763| val_0_rmse: 87.54145|  0:00:01s\n",
            "epoch 13 | loss: 363844.06989| val_0_rmse: 85.31394|  0:00:01s\n",
            "epoch 14 | loss: 355700.70096| val_0_rmse: 83.03426|  0:00:02s\n",
            "epoch 15 | loss: 350343.12427| val_0_rmse: 82.41663|  0:00:02s\n",
            "epoch 16 | loss: 364876.83606| val_0_rmse: 80.13083|  0:00:02s\n",
            "epoch 17 | loss: 305341.38116| val_0_rmse: 81.4514 |  0:00:02s\n",
            "epoch 18 | loss: 358365.80225| val_0_rmse: 78.61832|  0:00:02s\n",
            "epoch 19 | loss: 154644.12576| val_0_rmse: 85.48906|  0:00:02s\n",
            "epoch 20 | loss: 349249.66309| val_0_rmse: 85.83013|  0:00:02s\n",
            "epoch 21 | loss: 343306.00293| val_0_rmse: 83.98897|  0:00:02s\n",
            "epoch 22 | loss: 333340.7168| val_0_rmse: 88.45027|  0:00:03s\n",
            "epoch 23 | loss: 336509.61609| val_0_rmse: 96.22062|  0:00:03s\n",
            "epoch 24 | loss: 148800.43115| val_0_rmse: 99.99744|  0:00:03s\n",
            "epoch 25 | loss: 310750.05286| val_0_rmse: 94.31458|  0:00:03s\n",
            "epoch 26 | loss: 349165.68384| val_0_rmse: 88.54662|  0:00:03s\n",
            "epoch 27 | loss: 311703.93408| val_0_rmse: 92.39184|  0:00:03s\n",
            "epoch 28 | loss: 154783.67627| val_0_rmse: 93.86344|  0:00:03s\n",
            "epoch 29 | loss: 351312.33044| val_0_rmse: 90.3229 |  0:00:04s\n",
            "epoch 30 | loss: 342923.58252| val_0_rmse: 87.10088|  0:00:04s\n",
            "epoch 31 | loss: 162420.40527| val_0_rmse: 77.52679|  0:00:04s\n",
            "epoch 32 | loss: 153464.57446| val_0_rmse: 88.99976|  0:00:04s\n",
            "epoch 33 | loss: 335025.08398| val_0_rmse: 84.10017|  0:00:04s\n",
            "epoch 34 | loss: 351519.84424| val_0_rmse: 89.96319|  0:00:04s\n",
            "epoch 35 | loss: 339255.54272| val_0_rmse: 93.67778|  0:00:04s\n",
            "epoch 36 | loss: 325069.43945| val_0_rmse: 96.14399|  0:00:04s\n",
            "epoch 37 | loss: 339105.0625| val_0_rmse: 100.87844|  0:00:04s\n",
            "epoch 38 | loss: 301246.69312| val_0_rmse: 104.21885|  0:00:05s\n",
            "epoch 39 | loss: 327352.36816| val_0_rmse: 109.99423|  0:00:05s\n",
            "epoch 40 | loss: 138565.83496| val_0_rmse: 110.98  |  0:00:05s\n",
            "epoch 41 | loss: 338841.48584| val_0_rmse: 115.26207|  0:00:05s\n",
            "epoch 42 | loss: 314692.90869| val_0_rmse: 117.64614|  0:00:05s\n",
            "epoch 43 | loss: 310093.27124| val_0_rmse: 115.69927|  0:00:05s\n",
            "epoch 44 | loss: 311450.15918| val_0_rmse: 120.14341|  0:00:05s\n",
            "epoch 45 | loss: 130951.73828| val_0_rmse: 123.13133|  0:00:05s\n",
            "epoch 46 | loss: 221570.77417| val_0_rmse: 124.40726|  0:00:05s\n",
            "epoch 47 | loss: 314095.03735| val_0_rmse: 129.63984|  0:00:06s\n",
            "epoch 48 | loss: 320651.06372| val_0_rmse: 142.66141|  0:00:06s\n",
            "epoch 49 | loss: 313871.67407| val_0_rmse: 153.46959|  0:00:06s\n",
            "epoch 50 | loss: 320841.78394| val_0_rmse: 162.2725|  0:00:06s\n",
            "epoch 51 | loss: 339254.19678| val_0_rmse: 168.53625|  0:00:06s\n",
            "epoch 52 | loss: 147840.74365| val_0_rmse: 136.91534|  0:00:06s\n",
            "epoch 53 | loss: 346078.14282| val_0_rmse: 109.01867|  0:00:06s\n",
            "epoch 54 | loss: 333107.24414| val_0_rmse: 108.36581|  0:00:06s\n",
            "epoch 55 | loss: 328938.22949| val_0_rmse: 112.14856|  0:00:06s\n",
            "epoch 56 | loss: 285660.47729| val_0_rmse: 112.87588|  0:00:07s\n",
            "epoch 57 | loss: 196483.04297| val_0_rmse: 110.99279|  0:00:07s\n",
            "epoch 58 | loss: 292412.62061| val_0_rmse: 110.5545|  0:00:07s\n",
            "epoch 59 | loss: 319187.69922| val_0_rmse: 117.14351|  0:00:07s\n",
            "epoch 60 | loss: 329055.74707| val_0_rmse: 124.2655|  0:00:07s\n",
            "epoch 61 | loss: 317496.1272| val_0_rmse: 128.02707|  0:00:07s\n",
            "epoch 62 | loss: 326490.97778| val_0_rmse: 136.74605|  0:00:07s\n",
            "epoch 63 | loss: 151370.47681| val_0_rmse: 129.04691|  0:00:07s\n",
            "epoch 64 | loss: 245925.2821| val_0_rmse: 100.74883|  0:00:08s\n",
            "epoch 65 | loss: 332819.4408| val_0_rmse: 97.28139|  0:00:08s\n",
            "epoch 66 | loss: 285508.91528| val_0_rmse: 101.96683|  0:00:08s\n",
            "epoch 67 | loss: 227355.58264| val_0_rmse: 101.37768|  0:00:08s\n",
            "epoch 68 | loss: 149477.36584| val_0_rmse: 102.78479|  0:00:08s\n",
            "epoch 69 | loss: 331591.1676| val_0_rmse: 102.10173|  0:00:08s\n",
            "epoch 70 | loss: 325769.64368| val_0_rmse: 101.76409|  0:00:08s\n",
            "epoch 71 | loss: 231241.60242| val_0_rmse: 107.39299|  0:00:08s\n",
            "epoch 72 | loss: 321544.79517| val_0_rmse: 125.58752|  0:00:08s\n",
            "epoch 73 | loss: 313899.15601| val_0_rmse: 131.71935|  0:00:09s\n",
            "epoch 74 | loss: 306877.2749| val_0_rmse: 139.79545|  0:00:09s\n",
            "epoch 75 | loss: 233794.86279| val_0_rmse: 143.1978|  0:00:09s\n",
            "epoch 76 | loss: 305979.32935| val_0_rmse: 151.95803|  0:00:09s\n",
            "epoch 77 | loss: 311268.02246| val_0_rmse: 149.59499|  0:00:09s\n",
            "epoch 78 | loss: 307660.69043| val_0_rmse: 163.9161|  0:00:09s\n",
            "epoch 79 | loss: 314361.13818| val_0_rmse: 169.87361|  0:00:09s\n",
            "epoch 80 | loss: 138742.00952| val_0_rmse: 176.1833|  0:00:09s\n",
            "epoch 81 | loss: 137899.2627| val_0_rmse: 127.41478|  0:00:10s\n",
            "epoch 82 | loss: 326605.26367| val_0_rmse: 91.22636|  0:00:10s\n",
            "epoch 83 | loss: 325324.5127| val_0_rmse: 101.876 |  0:00:10s\n",
            "epoch 84 | loss: 241608.39062| val_0_rmse: 113.07893|  0:00:10s\n",
            "epoch 85 | loss: 317742.02893| val_0_rmse: 122.83802|  0:00:10s\n",
            "epoch 86 | loss: 299433.97449| val_0_rmse: 140.66143|  0:00:10s\n",
            "epoch 87 | loss: 318949.18237| val_0_rmse: 184.51718|  0:00:10s\n",
            "epoch 88 | loss: 318466.46851| val_0_rmse: 184.18839|  0:00:11s\n",
            "epoch 89 | loss: 310489.44458| val_0_rmse: 181.33658|  0:00:11s\n",
            "epoch 90 | loss: 239661.35742| val_0_rmse: 193.4169|  0:00:11s\n",
            "epoch 91 | loss: 307167.71973| val_0_rmse: 201.98134|  0:00:11s\n",
            "epoch 92 | loss: 321618.98779| val_0_rmse: 212.26075|  0:00:11s\n",
            "epoch 93 | loss: 146085.28564| val_0_rmse: 221.96829|  0:00:11s\n",
            "epoch 94 | loss: 313057.74121| val_0_rmse: 205.184 |  0:00:11s\n",
            "epoch 95 | loss: 157824.28076| val_0_rmse: 182.8067|  0:00:11s\n",
            "epoch 96 | loss: 347701.30566| val_0_rmse: 145.46162|  0:00:12s\n",
            "epoch 97 | loss: 316321.06665| val_0_rmse: 151.93972|  0:00:12s\n",
            "epoch 98 | loss: 230354.1875| val_0_rmse: 159.3327|  0:00:12s\n",
            "epoch 99 | loss: 310648.12646| val_0_rmse: 153.81718|  0:00:12s\n",
            "epoch 100| loss: 277248.50757| val_0_rmse: 142.86521|  0:00:12s\n",
            "epoch 101| loss: 313579.73218| val_0_rmse: 134.70438|  0:00:12s\n",
            "epoch 102| loss: 317629.10547| val_0_rmse: 133.86522|  0:00:12s\n",
            "epoch 103| loss: 305915.40479| val_0_rmse: 139.46826|  0:00:13s\n",
            "epoch 104| loss: 313234.32251| val_0_rmse: 139.66579|  0:00:13s\n",
            "epoch 105| loss: 273774.92114| val_0_rmse: 148.32262|  0:00:13s\n",
            "epoch 106| loss: 316302.46387| val_0_rmse: 151.52377|  0:00:13s\n",
            "epoch 107| loss: 311841.10742| val_0_rmse: 145.20601|  0:00:13s\n",
            "epoch 108| loss: 218529.08984| val_0_rmse: 146.2136|  0:00:13s\n",
            "epoch 109| loss: 357523.3877| val_0_rmse: 151.48494|  0:00:13s\n",
            "epoch 110| loss: 252577.37915| val_0_rmse: 148.79555|  0:00:13s\n",
            "epoch 111| loss: 227209.39331| val_0_rmse: 140.79363|  0:00:13s\n",
            "epoch 112| loss: 136222.64209| val_0_rmse: 146.41422|  0:00:14s\n",
            "epoch 113| loss: 308475.65125| val_0_rmse: 151.00572|  0:00:14s\n",
            "epoch 114| loss: 136625.28247| val_0_rmse: 154.01454|  0:00:14s\n",
            "epoch 115| loss: 47428.17236| val_0_rmse: 148.67376|  0:00:14s\n",
            "epoch 116| loss: 134526.47803| val_0_rmse: 152.56831|  0:00:14s\n",
            "epoch 117| loss: 308621.41943| val_0_rmse: 165.91147|  0:00:14s\n",
            "epoch 118| loss: 327823.32227| val_0_rmse: 166.81165|  0:00:14s\n",
            "epoch 119| loss: 297623.12524| val_0_rmse: 172.29654|  0:00:14s\n",
            "epoch 120| loss: 135144.57056| val_0_rmse: 178.23029|  0:00:14s\n",
            "epoch 121| loss: 318241.75171| val_0_rmse: 175.81351|  0:00:15s\n",
            "epoch 122| loss: 262492.21558| val_0_rmse: 175.94085|  0:00:15s\n",
            "epoch 123| loss: 297035.44775| val_0_rmse: 151.34031|  0:00:15s\n",
            "epoch 124| loss: 303416.22632| val_0_rmse: 147.9868|  0:00:15s\n",
            "epoch 125| loss: 311362.85718| val_0_rmse: 151.04614|  0:00:15s\n",
            "epoch 126| loss: 222331.0957| val_0_rmse: 159.55261|  0:00:15s\n",
            "epoch 127| loss: 309173.99438| val_0_rmse: 162.06668|  0:00:15s\n",
            "epoch 128| loss: 130294.27588| val_0_rmse: 151.68009|  0:00:15s\n",
            "epoch 129| loss: 294559.42993| val_0_rmse: 145.04535|  0:00:16s\n",
            "epoch 130| loss: 300059.82202| val_0_rmse: 143.94707|  0:00:16s\n",
            "epoch 131| loss: 305297.90039| val_0_rmse: 145.87914|  0:00:16s\n",
            "\n",
            "Early stopping occurred at epoch 131 with best_epoch = 31 and best_val_0_rmse = 77.52679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=1000,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aKmijHWE6Px",
        "outputId": "bd38f02c-8242-425d-fc4d-a43ac8e1c572"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 48.32,\n",
              " 'mse': 6010.4,\n",
              " 'rmse': 77.53,\n",
              " 'mae_upperbound_tolerance': -32.68,\n",
              " 'rmse_upperbound_tolerance': -51.47,\n",
              " 'mse_upperbound_tolerance': -3747.03}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DoMqm61wcy_"
      },
      "source": [
        "### 1.2. TabTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-PoqYrJ4wNzQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgsT-glJvicR",
        "outputId": "45a40988-1ce8-4199-8ed5-6358cef36177"
      },
      "outputs": [],
      "source": [
        "# Define the TabTransformer model\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(self, num_features, dim_embedding=64, num_heads=4, num_layers=4):\n",
        "        super(TabTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(num_features, dim_embedding)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.regressor = nn.Linear(dim_embedding, 1)  # Change to regression output (1 value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Adding a sequence length dimension\n",
        "        x = self.transformer(x)\n",
        "        x = torch.mean(x, dim=1)  # Pooling\n",
        "        x = self.regressor(x)  # Single output for regression\n",
        "        return x\n",
        "\n",
        "    # Define a predict function compatible with scikit-learn style\n",
        "    def predict(model, X):\n",
        "        model.eval()  # Put model in evaluation mode\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.FloatTensor(X)\n",
        "            predictions = model(X_tensor).cpu().numpy()  # Convert predictions to numpy\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cih7FdU4wtqn",
        "outputId": "6a8824d3-9875-45cf-e87e-0b608f84ac7a"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "num_features = X_train_scaled.shape[1]\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = TabTransformer(num_features).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.MSELoss()  # Use MSELoss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=100, gamma=0.001)\n",
        "\n",
        "# Converting data to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)  # Ensure target is of shape [batch_size, 1]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor)\n",
        "    loss = criterion(output, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # if epoch % 10 == 0:\n",
        "    #     print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 47.11,\n",
              " 'mse': 9098.51,\n",
              " 'rmse': 95.39,\n",
              " 'mae_upperbound_tolerance': -31.47,\n",
              " 'rmse_upperbound_tolerance': -69.33,\n",
              " 'mse_upperbound_tolerance': -6835.14}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation using evaluate function\n",
        "y_test_tensor = y_test.values  # Ensure y_test is in NumPy array format\n",
        "\n",
        "# Call the evaluate function with the predict method we defined\n",
        "eval_values = evaluate(\n",
        "    model=model, \n",
        "    X_test=X_test_scaled, \n",
        "    y_test=y_test_tensor, \n",
        "    threshold=0.3,  # Adjust as needed\n",
        "    mode=\"regression\"\n",
        ")\n",
        "\n",
        "evaluate_dict[\"TabTransformer\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZaqw9CC6v25"
      },
      "source": [
        "### 1.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "wOnzPY_IE5lo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUiNFT7nE2Co",
        "outputId": "b59f0392-1c37-4ec9-8211-ba6d8778df0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10\n",
        "X_train_seq, y_train_seq = create_dataset(X_train_scaled, y_train, time_steps)\n",
        "X_test_seq, y_test_seq = create_dataset(X_test_scaled, y_test, time_steps)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    # Layer 1\n",
        "    LSTM(128, activation='relu', input_shape=(time_steps, X_train.shape[1]), \n",
        "         return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 2\n",
        "    LSTM(64, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 3\n",
        "    LSTM(32, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 4\n",
        "    LSTM(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Dense layers\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(8, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile mô hình với learning rate schedule\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=100,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_seq, y_test_seq),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Y2WbQOFHcm",
        "outputId": "735923cf-cef7-4d57-d0a7-d477edf9c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 48.41,\n",
              " 'mse': 11088.3,\n",
              " 'rmse': 105.3,\n",
              " 'mae_upperbound_tolerance': -33.24,\n",
              " 'rmse_upperbound_tolerance': -77.18,\n",
              " 'mse_upperbound_tolerance': -8452.82}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_seq, y_test_seq, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Ujlhh40euzAT",
        "outputId": "e835ab0d-cbbc-4acc-831e-d545481b52a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_efaf1_row0_col2, #T_efaf1_row0_col3, #T_efaf1_row0_col5, #T_efaf1_row0_col6, #T_efaf1_row1_col1, #T_efaf1_row1_col4 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_efaf1\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_efaf1_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_efaf1_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_efaf1_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_efaf1_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_efaf1_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_efaf1_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_efaf1_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_efaf1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_efaf1_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_efaf1_row0_col1\" class=\"data row0 col1\" >48.32</td>\n",
              "      <td id=\"T_efaf1_row0_col2\" class=\"data row0 col2\" >6010.40</td>\n",
              "      <td id=\"T_efaf1_row0_col3\" class=\"data row0 col3\" >77.53</td>\n",
              "      <td id=\"T_efaf1_row0_col4\" class=\"data row0 col4\" >-32.68</td>\n",
              "      <td id=\"T_efaf1_row0_col5\" class=\"data row0 col5\" >-51.47</td>\n",
              "      <td id=\"T_efaf1_row0_col6\" class=\"data row0 col6\" >-3747.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_efaf1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_efaf1_row1_col0\" class=\"data row1 col0\" >TabTransformer</td>\n",
              "      <td id=\"T_efaf1_row1_col1\" class=\"data row1 col1\" >47.11</td>\n",
              "      <td id=\"T_efaf1_row1_col2\" class=\"data row1 col2\" >9098.51</td>\n",
              "      <td id=\"T_efaf1_row1_col3\" class=\"data row1 col3\" >95.39</td>\n",
              "      <td id=\"T_efaf1_row1_col4\" class=\"data row1 col4\" >-31.47</td>\n",
              "      <td id=\"T_efaf1_row1_col5\" class=\"data row1 col5\" >-69.33</td>\n",
              "      <td id=\"T_efaf1_row1_col6\" class=\"data row1 col6\" >-6835.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_efaf1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_efaf1_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_efaf1_row2_col1\" class=\"data row2 col1\" >48.41</td>\n",
              "      <td id=\"T_efaf1_row2_col2\" class=\"data row2 col2\" >11088.30</td>\n",
              "      <td id=\"T_efaf1_row2_col3\" class=\"data row2 col3\" >105.30</td>\n",
              "      <td id=\"T_efaf1_row2_col4\" class=\"data row2 col4\" >-33.24</td>\n",
              "      <td id=\"T_efaf1_row2_col5\" class=\"data row2 col5\" >-77.18</td>\n",
              "      <td id=\"T_efaf1_row2_col6\" class=\"data row2 col6\" >-8452.82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1dfa5590740>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8WWhA7GlL4"
      },
      "source": [
        "## 2. Target 2 : NoInjured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "7nDb17DIGveo"
      },
      "outputs": [],
      "source": [
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[1]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23NDk8RjzFl7"
      },
      "source": [
        "### 2.1. TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k993xgTmHRnv",
        "outputId": "1ceb5d1b-400b-4b00-f0de-ca79553093e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 51176.87402| val_0_rmse: 419.46671|  0:00:00s\n",
            "epoch 1  | loss: 41880.02051| val_0_rmse: 419.15594|  0:00:00s\n",
            "epoch 2  | loss: 43763.24365| val_0_rmse: 418.67334|  0:00:00s\n",
            "epoch 3  | loss: 32999.54761| val_0_rmse: 417.40374|  0:00:00s\n",
            "epoch 4  | loss: 41008.11987| val_0_rmse: 415.96654|  0:00:00s\n",
            "epoch 5  | loss: 43509.63452| val_0_rmse: 415.42674|  0:00:00s\n",
            "epoch 6  | loss: 47634.48047| val_0_rmse: 414.59451|  0:00:00s\n",
            "epoch 7  | loss: 35576.15259| val_0_rmse: 416.31958|  0:00:00s\n",
            "epoch 8  | loss: 35163.25983| val_0_rmse: 415.82687|  0:00:01s\n",
            "epoch 9  | loss: 41436.69873| val_0_rmse: 414.20771|  0:00:01s\n",
            "epoch 10 | loss: 35672.5542| val_0_rmse: 412.48017|  0:00:01s\n",
            "epoch 11 | loss: 42418.5011| val_0_rmse: 410.94681|  0:00:01s\n",
            "epoch 12 | loss: 42225.78857| val_0_rmse: 410.6045|  0:00:01s\n",
            "epoch 13 | loss: 29692.9375| val_0_rmse: 410.91237|  0:00:01s\n",
            "epoch 14 | loss: 37681.67114| val_0_rmse: 411.19666|  0:00:01s\n",
            "epoch 15 | loss: 36882.25058| val_0_rmse: 410.53452|  0:00:02s\n",
            "epoch 16 | loss: 35383.07214| val_0_rmse: 410.74692|  0:00:02s\n",
            "epoch 17 | loss: 32890.95392| val_0_rmse: 410.27528|  0:00:02s\n",
            "epoch 18 | loss: 34934.38342| val_0_rmse: 410.24523|  0:00:02s\n",
            "epoch 19 | loss: 35082.21045| val_0_rmse: 408.45718|  0:00:02s\n",
            "epoch 20 | loss: 36401.43201| val_0_rmse: 407.54777|  0:00:02s\n",
            "epoch 21 | loss: 37058.03918| val_0_rmse: 406.59276|  0:00:02s\n",
            "epoch 22 | loss: 38375.1203| val_0_rmse: 402.64464|  0:00:02s\n",
            "epoch 23 | loss: 33260.5885| val_0_rmse: 395.78454|  0:00:03s\n",
            "epoch 24 | loss: 35079.8667| val_0_rmse: 394.81771|  0:00:03s\n",
            "epoch 25 | loss: 22224.27979| val_0_rmse: 395.04845|  0:00:03s\n",
            "epoch 26 | loss: 34370.70239| val_0_rmse: 393.58073|  0:00:03s\n",
            "epoch 27 | loss: 33937.43384| val_0_rmse: 394.17884|  0:00:03s\n",
            "epoch 28 | loss: 33337.86938| val_0_rmse: 398.50479|  0:00:03s\n",
            "epoch 29 | loss: 31974.66541| val_0_rmse: 407.42052|  0:00:03s\n",
            "epoch 30 | loss: 26231.62061| val_0_rmse: 412.67617|  0:00:04s\n",
            "epoch 31 | loss: 28714.83704| val_0_rmse: 413.19205|  0:00:04s\n",
            "epoch 32 | loss: 36156.06323| val_0_rmse: 412.77185|  0:00:04s\n",
            "epoch 33 | loss: 38720.45898| val_0_rmse: 412.56402|  0:00:04s\n",
            "epoch 34 | loss: 32102.61108| val_0_rmse: 411.93079|  0:00:04s\n",
            "epoch 35 | loss: 29691.41748| val_0_rmse: 411.43889|  0:00:04s\n",
            "epoch 36 | loss: 33805.34778| val_0_rmse: 411.77732|  0:00:04s\n",
            "epoch 37 | loss: 29341.46094| val_0_rmse: 412.36352|  0:00:04s\n",
            "epoch 38 | loss: 32764.28503| val_0_rmse: 412.18086|  0:00:05s\n",
            "epoch 39 | loss: 31983.60657| val_0_rmse: 411.23265|  0:00:05s\n",
            "epoch 40 | loss: 31470.11652| val_0_rmse: 410.92172|  0:00:05s\n",
            "epoch 41 | loss: 18681.86804| val_0_rmse: 411.77918|  0:00:05s\n",
            "epoch 42 | loss: 28257.11865| val_0_rmse: 411.90167|  0:00:05s\n",
            "epoch 43 | loss: 17831.68054| val_0_rmse: 409.86852|  0:00:05s\n",
            "epoch 44 | loss: 35684.45703| val_0_rmse: 407.70369|  0:00:05s\n",
            "epoch 45 | loss: 35444.94495| val_0_rmse: 406.11621|  0:00:05s\n",
            "epoch 46 | loss: 30713.15027| val_0_rmse: 405.30503|  0:00:06s\n",
            "epoch 47 | loss: 36018.27563| val_0_rmse: 405.80628|  0:00:06s\n",
            "epoch 48 | loss: 31877.98926| val_0_rmse: 406.53388|  0:00:06s\n",
            "epoch 49 | loss: 36208.7981| val_0_rmse: 408.17167|  0:00:06s\n",
            "epoch 50 | loss: 21654.21362| val_0_rmse: 411.97113|  0:00:06s\n",
            "epoch 51 | loss: 18299.51318| val_0_rmse: 412.15326|  0:00:06s\n",
            "epoch 52 | loss: 31273.90283| val_0_rmse: 411.85576|  0:00:06s\n",
            "epoch 53 | loss: 28204.24731| val_0_rmse: 413.01924|  0:00:07s\n",
            "epoch 54 | loss: 38028.27209| val_0_rmse: 413.02962|  0:00:07s\n",
            "epoch 55 | loss: 20077.5697| val_0_rmse: 411.95453|  0:00:07s\n",
            "epoch 56 | loss: 15698.51172| val_0_rmse: 412.05247|  0:00:07s\n",
            "epoch 57 | loss: 27020.24683| val_0_rmse: 409.59844|  0:00:07s\n",
            "epoch 58 | loss: 28902.7926| val_0_rmse: 411.81671|  0:00:07s\n",
            "epoch 59 | loss: 27366.37793| val_0_rmse: 414.40637|  0:00:07s\n",
            "epoch 60 | loss: 37489.61133| val_0_rmse: 417.01558|  0:00:08s\n",
            "epoch 61 | loss: 34430.40234| val_0_rmse: 416.78725|  0:00:08s\n",
            "epoch 62 | loss: 31038.45032| val_0_rmse: 416.09537|  0:00:08s\n",
            "epoch 63 | loss: 34631.6499| val_0_rmse: 413.35278|  0:00:08s\n",
            "epoch 64 | loss: 30188.08984| val_0_rmse: 406.91832|  0:00:08s\n",
            "epoch 65 | loss: 19307.80823| val_0_rmse: 403.63769|  0:00:08s\n",
            "epoch 66 | loss: 32381.13049| val_0_rmse: 403.26563|  0:00:08s\n",
            "epoch 67 | loss: 31989.65625| val_0_rmse: 405.56367|  0:00:09s\n",
            "epoch 68 | loss: 35170.0957| val_0_rmse: 410.631 |  0:00:09s\n",
            "epoch 69 | loss: 31910.21875| val_0_rmse: 411.65101|  0:00:09s\n",
            "epoch 70 | loss: 34030.49683| val_0_rmse: 410.89405|  0:00:09s\n",
            "epoch 71 | loss: 28785.198| val_0_rmse: 408.9553|  0:00:09s\n",
            "epoch 72 | loss: 30864.74231| val_0_rmse: 403.87621|  0:00:09s\n",
            "epoch 73 | loss: 34215.78503| val_0_rmse: 403.31369|  0:00:09s\n",
            "epoch 74 | loss: 34290.62561| val_0_rmse: 403.57923|  0:00:10s\n",
            "epoch 75 | loss: 29283.38892| val_0_rmse: 403.66895|  0:00:10s\n",
            "epoch 76 | loss: 26377.69897| val_0_rmse: 406.51209|  0:00:10s\n",
            "epoch 77 | loss: 30224.7998| val_0_rmse: 409.97646|  0:00:10s\n",
            "epoch 78 | loss: 34566.44849| val_0_rmse: 414.09033|  0:00:10s\n",
            "epoch 79 | loss: 32430.25537| val_0_rmse: 411.83428|  0:00:10s\n",
            "epoch 80 | loss: 29796.5896| val_0_rmse: 412.26317|  0:00:10s\n",
            "epoch 81 | loss: 23517.98364| val_0_rmse: 411.18655|  0:00:11s\n",
            "epoch 82 | loss: 22744.83801| val_0_rmse: 410.53375|  0:00:11s\n",
            "epoch 83 | loss: 31091.05957| val_0_rmse: 410.49791|  0:00:11s\n",
            "epoch 84 | loss: 19553.1001| val_0_rmse: 410.35723|  0:00:11s\n",
            "epoch 85 | loss: 34011.21362| val_0_rmse: 411.94588|  0:00:11s\n",
            "epoch 86 | loss: 29647.44629| val_0_rmse: 412.92142|  0:00:11s\n",
            "epoch 87 | loss: 34663.7251| val_0_rmse: 413.29277|  0:00:11s\n",
            "epoch 88 | loss: 34026.67456| val_0_rmse: 413.6566|  0:00:12s\n",
            "epoch 89 | loss: 28275.57605| val_0_rmse: 412.95591|  0:00:12s\n",
            "epoch 90 | loss: 27822.87891| val_0_rmse: 412.68651|  0:00:12s\n",
            "epoch 91 | loss: 31797.76538| val_0_rmse: 411.93329|  0:00:12s\n",
            "epoch 92 | loss: 31570.19092| val_0_rmse: 412.02439|  0:00:12s\n",
            "epoch 93 | loss: 32744.81787| val_0_rmse: 411.60489|  0:00:12s\n",
            "epoch 94 | loss: 21099.53906| val_0_rmse: 410.16014|  0:00:12s\n",
            "epoch 95 | loss: 23152.81494| val_0_rmse: 409.44937|  0:00:13s\n",
            "epoch 96 | loss: 31145.15234| val_0_rmse: 406.37037|  0:00:13s\n",
            "epoch 97 | loss: 20398.70898| val_0_rmse: 404.9072|  0:00:13s\n",
            "epoch 98 | loss: 18224.30859| val_0_rmse: 402.39479|  0:00:13s\n",
            "epoch 99 | loss: 28332.38623| val_0_rmse: 405.1376|  0:00:13s\n",
            "epoch 100| loss: 19353.74939| val_0_rmse: 406.01566|  0:00:13s\n",
            "epoch 101| loss: 28506.05579| val_0_rmse: 407.11151|  0:00:13s\n",
            "epoch 102| loss: 32152.96265| val_0_rmse: 407.48611|  0:00:14s\n",
            "epoch 103| loss: 28029.40625| val_0_rmse: 411.29819|  0:00:14s\n",
            "epoch 104| loss: 30504.08545| val_0_rmse: 412.38442|  0:00:14s\n",
            "epoch 105| loss: 32379.73438| val_0_rmse: 412.8076|  0:00:14s\n",
            "epoch 106| loss: 25504.53784| val_0_rmse: 413.71055|  0:00:14s\n",
            "epoch 107| loss: 34395.65723| val_0_rmse: 416.11832|  0:00:14s\n",
            "epoch 108| loss: 27033.44995| val_0_rmse: 418.52947|  0:00:14s\n",
            "epoch 109| loss: 13508.98083| val_0_rmse: 419.3145|  0:00:14s\n",
            "epoch 110| loss: 22116.64832| val_0_rmse: 418.56385|  0:00:15s\n",
            "epoch 111| loss: 25309.41162| val_0_rmse: 419.39589|  0:00:15s\n",
            "epoch 112| loss: 33795.56885| val_0_rmse: 419.7281|  0:00:15s\n",
            "epoch 113| loss: 22220.81323| val_0_rmse: 421.6177|  0:00:15s\n",
            "epoch 114| loss: 20289.52112| val_0_rmse: 421.56648|  0:00:15s\n",
            "epoch 115| loss: 31926.85059| val_0_rmse: 416.50518|  0:00:15s\n",
            "epoch 116| loss: 24146.05176| val_0_rmse: 404.43557|  0:00:15s\n",
            "epoch 117| loss: 30467.53918| val_0_rmse: 405.78222|  0:00:15s\n",
            "epoch 118| loss: 29885.80054| val_0_rmse: 407.88514|  0:00:15s\n",
            "epoch 119| loss: 31690.40094| val_0_rmse: 412.25465|  0:00:16s\n",
            "epoch 120| loss: 33841.23584| val_0_rmse: 415.80733|  0:00:16s\n",
            "epoch 121| loss: 36126.77319| val_0_rmse: 414.8173|  0:00:16s\n",
            "epoch 122| loss: 19335.66699| val_0_rmse: 413.18023|  0:00:16s\n",
            "epoch 123| loss: 26089.07886| val_0_rmse: 408.63008|  0:00:16s\n",
            "epoch 124| loss: 27023.07153| val_0_rmse: 405.51926|  0:00:16s\n",
            "epoch 125| loss: 30084.19604| val_0_rmse: 402.36788|  0:00:16s\n",
            "epoch 126| loss: 15662.28442| val_0_rmse: 401.71383|  0:00:16s\n",
            "\n",
            "Early stopping occurred at epoch 126 with best_epoch = 26 and best_val_0_rmse = 393.58073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=1000,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnNoTqhKHhwI",
        "outputId": "bb320e74-2a19-4ba5-9758-7e544876e6ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 122.22,\n",
              " 'mse': 154905.79,\n",
              " 'rmse': 393.58,\n",
              " 'mae_upperbound_tolerance': -88.02,\n",
              " 'rmse_upperbound_tolerance': -272.35,\n",
              " 'mse_upperbound_tolerance': -105915.95}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1qM7N4Ux48L"
      },
      "source": [
        "### 2.2. TabTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpUIFeqfx898",
        "outputId": "6e32cfe4-786c-45a0-b98e-f161cf8fe8a1"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "num_features = X_train_scaled.shape[1]\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = TabTransformer(num_features).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.MSELoss()  # Use MSELoss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=100, gamma=0.001)\n",
        "\n",
        "# Converting data to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)  # Ensure target is of shape [batch_size, 1]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor)\n",
        "    loss = criterion(output, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LInkQlhRySzJ",
        "outputId": "093d78be-b012-4c85-bb3f-43949f2fc407"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 104.48,\n",
              " 'mse': 173420.46,\n",
              " 'rmse': 416.44,\n",
              " 'mae_upperbound_tolerance': -70.28,\n",
              " 'rmse_upperbound_tolerance': -295.21,\n",
              " 'mse_upperbound_tolerance': -124430.62}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation using evaluate function\n",
        "y_test_tensor = y_test.values  # Ensure y_test is in NumPy array format\n",
        "\n",
        "# Call the evaluate function with the predict method we defined\n",
        "eval_values = evaluate(\n",
        "    model=model, \n",
        "    X_test=X_test_scaled, \n",
        "    y_test=y_test_tensor, \n",
        "    threshold=0.3,  # Adjust as needed\n",
        "    mode=\"regression\"\n",
        ")\n",
        "\n",
        "evaluate_dict[\"TabTransformer\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSRGFMTOykFb"
      },
      "source": [
        "### 2.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lvHQX96ylny",
        "outputId": "a7d70c9c-374a-41c3-e053-7d1458e7f17e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10\n",
        "X_train_seq, y_train_seq = create_dataset(X_train_scaled, y_train, time_steps)\n",
        "X_test_seq, y_test_seq = create_dataset(X_test_scaled, y_test, time_steps)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    # Layer 1\n",
        "    LSTM(128, activation='relu', input_shape=(time_steps, X_train.shape[1]), \n",
        "         return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 2\n",
        "    LSTM(64, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 3\n",
        "    LSTM(32, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 4\n",
        "    LSTM(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Dense layers\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(8, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile mô hình với learning rate schedule\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=100,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_seq, y_test_seq),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrJOug6Uyxi6",
        "outputId": "47997b3f-ed9f-49a4-9243-c50a24c0908f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 142.01,\n",
              " 'mse': 239771.11,\n",
              " 'rmse': 489.66,\n",
              " 'mae_upperbound_tolerance': -99.36,\n",
              " 'rmse_upperbound_tolerance': -349.08,\n",
              " 'mse_upperbound_tolerance': -173887.45}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_seq, y_test_seq, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Yw95neRXy4Y7",
        "outputId": "33b2c672-9cdb-4e7c-adc5-fac4b97046b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_152f8_row0_col2, #T_152f8_row0_col3, #T_152f8_row0_col5, #T_152f8_row0_col6, #T_152f8_row1_col1, #T_152f8_row1_col4 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_152f8\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_152f8_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_152f8_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_152f8_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_152f8_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_152f8_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_152f8_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_152f8_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_152f8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_152f8_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_152f8_row0_col1\" class=\"data row0 col1\" >122.22</td>\n",
              "      <td id=\"T_152f8_row0_col2\" class=\"data row0 col2\" >154905.79</td>\n",
              "      <td id=\"T_152f8_row0_col3\" class=\"data row0 col3\" >393.58</td>\n",
              "      <td id=\"T_152f8_row0_col4\" class=\"data row0 col4\" >-88.02</td>\n",
              "      <td id=\"T_152f8_row0_col5\" class=\"data row0 col5\" >-272.35</td>\n",
              "      <td id=\"T_152f8_row0_col6\" class=\"data row0 col6\" >-105915.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_152f8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_152f8_row1_col0\" class=\"data row1 col0\" >TabTransformer</td>\n",
              "      <td id=\"T_152f8_row1_col1\" class=\"data row1 col1\" >104.48</td>\n",
              "      <td id=\"T_152f8_row1_col2\" class=\"data row1 col2\" >173420.46</td>\n",
              "      <td id=\"T_152f8_row1_col3\" class=\"data row1 col3\" >416.44</td>\n",
              "      <td id=\"T_152f8_row1_col4\" class=\"data row1 col4\" >-70.28</td>\n",
              "      <td id=\"T_152f8_row1_col5\" class=\"data row1 col5\" >-295.21</td>\n",
              "      <td id=\"T_152f8_row1_col6\" class=\"data row1 col6\" >-124430.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_152f8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_152f8_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_152f8_row2_col1\" class=\"data row2 col1\" >142.01</td>\n",
              "      <td id=\"T_152f8_row2_col2\" class=\"data row2 col2\" >239771.11</td>\n",
              "      <td id=\"T_152f8_row2_col3\" class=\"data row2 col3\" >489.66</td>\n",
              "      <td id=\"T_152f8_row2_col4\" class=\"data row2 col4\" >-99.36</td>\n",
              "      <td id=\"T_152f8_row2_col5\" class=\"data row2 col5\" >-349.08</td>\n",
              "      <td id=\"T_152f8_row2_col6\" class=\"data row2 col6\" >-173887.45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1dfa5396630>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj0f8LzHH8uP"
      },
      "source": [
        "## 3. Target 3 : TotalDamageAdjusted(000US$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "2URG3A9BIDNv"
      },
      "outputs": [],
      "source": [
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[2]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z31iX3jz0Mr"
      },
      "source": [
        "### 3.1 TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBp4rhG-IIWw",
        "outputId": "d4fbc1cd-c6e6-4eb3-9704-1bcfa99dbaf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 80834277376.0| val_0_rmse: 111230.12582|  0:00:00s\n",
            "epoch 1  | loss: 71588611072.0| val_0_rmse: 111229.40948|  0:00:00s\n",
            "epoch 2  | loss: 74787424256.0| val_0_rmse: 111228.98446|  0:00:00s\n",
            "epoch 3  | loss: 58221058048.0| val_0_rmse: 111227.06913|  0:00:00s\n",
            "epoch 4  | loss: 66301756416.0| val_0_rmse: 111226.37414|  0:00:00s\n",
            "epoch 5  | loss: 69256152064.0| val_0_rmse: 111224.56016|  0:00:00s\n",
            "epoch 6  | loss: 79422047232.0| val_0_rmse: 111222.99023|  0:00:00s\n",
            "epoch 7  | loss: 67533618176.0| val_0_rmse: 111219.85812|  0:00:01s\n",
            "epoch 8  | loss: 66835597824.0| val_0_rmse: 111219.45679|  0:00:01s\n",
            "epoch 9  | loss: 71169016320.0| val_0_rmse: 111216.7922|  0:00:01s\n",
            "epoch 10 | loss: 65461849088.0| val_0_rmse: 111213.91041|  0:00:01s\n",
            "epoch 11 | loss: 72556908032.0| val_0_rmse: 111209.91132|  0:00:01s\n",
            "epoch 12 | loss: 71420956160.0| val_0_rmse: 111205.8656|  0:00:01s\n",
            "epoch 13 | loss: 75314334208.0| val_0_rmse: 111203.0334|  0:00:01s\n",
            "epoch 14 | loss: 72367283456.0| val_0_rmse: 111198.49997|  0:00:02s\n",
            "epoch 15 | loss: 61482630400.0| val_0_rmse: 111195.82362|  0:00:02s\n",
            "epoch 16 | loss: 62321979392.0| val_0_rmse: 111190.88415|  0:00:02s\n",
            "epoch 17 | loss: 67939318016.0| val_0_rmse: 111187.65591|  0:00:02s\n",
            "epoch 18 | loss: 74865619968.0| val_0_rmse: 111181.94386|  0:00:02s\n",
            "epoch 19 | loss: 65791080448.0| val_0_rmse: 111178.9051|  0:00:02s\n",
            "epoch 20 | loss: 72349287424.0| val_0_rmse: 111176.2129|  0:00:02s\n",
            "epoch 21 | loss: 73964510208.0| val_0_rmse: 111172.47047|  0:00:03s\n",
            "epoch 22 | loss: 70710339584.0| val_0_rmse: 111167.86199|  0:00:03s\n",
            "epoch 23 | loss: 72998544384.0| val_0_rmse: 111156.48346|  0:00:03s\n",
            "epoch 24 | loss: 69637264384.0| val_0_rmse: 111150.58081|  0:00:03s\n",
            "epoch 25 | loss: 73888274432.0| val_0_rmse: 111148.15846|  0:00:03s\n",
            "epoch 26 | loss: 54994511872.0| val_0_rmse: 111130.65409|  0:00:03s\n",
            "epoch 27 | loss: 71380950528.0| val_0_rmse: 111122.83759|  0:00:03s\n",
            "epoch 28 | loss: 72448652288.0| val_0_rmse: 111129.6427|  0:00:04s\n",
            "epoch 29 | loss: 66708634112.0| val_0_rmse: 111127.07773|  0:00:04s\n",
            "epoch 30 | loss: 78538004480.0| val_0_rmse: 111118.10327|  0:00:04s\n",
            "epoch 31 | loss: 71028066304.0| val_0_rmse: 111108.78881|  0:00:04s\n",
            "epoch 32 | loss: 75747348480.0| val_0_rmse: 111104.2334|  0:00:04s\n",
            "epoch 33 | loss: 62321982976.0| val_0_rmse: 111114.32604|  0:00:05s\n",
            "epoch 34 | loss: 66430896640.0| val_0_rmse: 111120.65604|  0:00:05s\n",
            "epoch 35 | loss: 76973990912.0| val_0_rmse: 111116.51713|  0:00:05s\n",
            "epoch 36 | loss: 77486182400.0| val_0_rmse: 111106.48588|  0:00:05s\n",
            "epoch 37 | loss: 72823698944.0| val_0_rmse: 111089.38196|  0:00:06s\n",
            "epoch 38 | loss: 58547960832.0| val_0_rmse: 111072.59214|  0:00:06s\n",
            "epoch 39 | loss: 79497234688.0| val_0_rmse: 111057.40393|  0:00:06s\n",
            "epoch 40 | loss: 76907774208.0| val_0_rmse: 111042.2693|  0:00:06s\n",
            "epoch 41 | loss: 58571419648.0| val_0_rmse: 111040.83545|  0:00:06s\n",
            "epoch 42 | loss: 68631631872.0| val_0_rmse: 111047.73652|  0:00:06s\n",
            "epoch 43 | loss: 76559139840.0| val_0_rmse: 111040.31789|  0:00:06s\n",
            "epoch 44 | loss: 67573677824.0| val_0_rmse: 110979.65092|  0:00:07s\n",
            "epoch 45 | loss: 74531934208.0| val_0_rmse: 110964.49666|  0:00:07s\n",
            "epoch 46 | loss: 70739461120.0| val_0_rmse: 110951.08403|  0:00:07s\n",
            "epoch 47 | loss: 69389285888.0| val_0_rmse: 110937.80417|  0:00:07s\n",
            "epoch 48 | loss: 69241845760.0| val_0_rmse: 110923.4663|  0:00:07s\n",
            "epoch 49 | loss: 77071841280.0| val_0_rmse: 110901.33277|  0:00:07s\n",
            "epoch 50 | loss: 64247735040.0| val_0_rmse: 110881.97558|  0:00:07s\n",
            "epoch 51 | loss: 55594684928.0| val_0_rmse: 110873.18276|  0:00:07s\n",
            "epoch 52 | loss: 55026292224.0| val_0_rmse: 110859.06436|  0:00:08s\n",
            "epoch 53 | loss: 78401127424.0| val_0_rmse: 110833.04468|  0:00:08s\n",
            "epoch 54 | loss: 79245910528.0| val_0_rmse: 110813.41116|  0:00:08s\n",
            "epoch 55 | loss: 66670285824.0| val_0_rmse: 110791.16939|  0:00:08s\n",
            "epoch 56 | loss: 68246264832.0| val_0_rmse: 110749.46526|  0:00:08s\n",
            "epoch 57 | loss: 68650950144.0| val_0_rmse: 110696.4182|  0:00:08s\n",
            "epoch 58 | loss: 73458408448.0| val_0_rmse: 110667.8158|  0:00:08s\n",
            "epoch 59 | loss: 78398124032.0| val_0_rmse: 110688.35971|  0:00:08s\n",
            "epoch 60 | loss: 71767926528.0| val_0_rmse: 110670.59489|  0:00:09s\n",
            "epoch 61 | loss: 79344068608.0| val_0_rmse: 110642.31095|  0:00:09s\n",
            "epoch 62 | loss: 69485454336.0| val_0_rmse: 110715.418|  0:00:09s\n",
            "epoch 63 | loss: 75878661120.0| val_0_rmse: 110735.04187|  0:00:09s\n",
            "epoch 64 | loss: 66757720576.0| val_0_rmse: 110712.63956|  0:00:09s\n",
            "epoch 65 | loss: 68264519680.0| val_0_rmse: 110674.48557|  0:00:09s\n",
            "epoch 66 | loss: 79144005632.0| val_0_rmse: 110657.86656|  0:00:09s\n",
            "epoch 67 | loss: 67715133184.0| val_0_rmse: 110701.07082|  0:00:10s\n",
            "epoch 68 | loss: 65396771840.0| val_0_rmse: 110624.71675|  0:00:10s\n",
            "epoch 69 | loss: 69591901184.0| val_0_rmse: 110618.55988|  0:00:10s\n",
            "epoch 70 | loss: 68499985408.0| val_0_rmse: 110592.39299|  0:00:10s\n",
            "epoch 71 | loss: 65623320064.0| val_0_rmse: 110528.06029|  0:00:10s\n",
            "epoch 72 | loss: 76860219392.0| val_0_rmse: 110521.62418|  0:00:10s\n",
            "epoch 73 | loss: 78865899776.0| val_0_rmse: 110532.92783|  0:00:10s\n",
            "epoch 74 | loss: 76928542720.0| val_0_rmse: 110506.45582|  0:00:10s\n",
            "epoch 75 | loss: 62750684160.0| val_0_rmse: 110468.59457|  0:00:11s\n",
            "epoch 76 | loss: 76053073920.0| val_0_rmse: 110398.47307|  0:00:11s\n",
            "epoch 77 | loss: 61969806080.0| val_0_rmse: 110309.8475|  0:00:11s\n",
            "epoch 78 | loss: 70638442240.0| val_0_rmse: 110270.38272|  0:00:11s\n",
            "epoch 79 | loss: 60084745216.0| val_0_rmse: 110280.83728|  0:00:11s\n",
            "epoch 80 | loss: 75413356544.0| val_0_rmse: 110288.72931|  0:00:11s\n",
            "epoch 81 | loss: 73154498560.0| val_0_rmse: 110296.65816|  0:00:11s\n",
            "epoch 82 | loss: 74754054144.0| val_0_rmse: 110288.31578|  0:00:11s\n",
            "epoch 83 | loss: 66126390272.0| val_0_rmse: 110261.94003|  0:00:11s\n",
            "epoch 84 | loss: 65899717632.0| val_0_rmse: 110237.84158|  0:00:12s\n",
            "epoch 85 | loss: 75913819136.0| val_0_rmse: 110176.27697|  0:00:12s\n",
            "epoch 86 | loss: 71216305152.0| val_0_rmse: 110127.4764|  0:00:12s\n",
            "epoch 87 | loss: 71845545216.0| val_0_rmse: 110118.60678|  0:00:12s\n",
            "epoch 88 | loss: 74519265792.0| val_0_rmse: 110113.31409|  0:00:12s\n",
            "epoch 89 | loss: 71674893312.0| val_0_rmse: 110237.70214|  0:00:12s\n",
            "epoch 90 | loss: 72472463360.0| val_0_rmse: 110217.55587|  0:00:12s\n",
            "epoch 91 | loss: 79953707264.0| val_0_rmse: 110084.48933|  0:00:12s\n",
            "epoch 92 | loss: 65710239744.0| val_0_rmse: 110166.41579|  0:00:13s\n",
            "epoch 93 | loss: 72742767104.0| val_0_rmse: 110066.88811|  0:00:13s\n",
            "epoch 94 | loss: 73466309632.0| val_0_rmse: 110143.20126|  0:00:13s\n",
            "epoch 95 | loss: 68898066944.0| val_0_rmse: 110101.34634|  0:00:13s\n",
            "epoch 96 | loss: 72310904832.0| val_0_rmse: 110077.79927|  0:00:13s\n",
            "epoch 97 | loss: 73864062976.0| val_0_rmse: 109971.95009|  0:00:13s\n",
            "epoch 98 | loss: 68589188096.0| val_0_rmse: 110005.761|  0:00:13s\n",
            "epoch 99 | loss: 64843800064.0| val_0_rmse: 110041.91669|  0:00:13s\n",
            "epoch 100| loss: 60707009536.0| val_0_rmse: 110030.36816|  0:00:13s\n",
            "epoch 101| loss: 74722992896.0| val_0_rmse: 110008.26047|  0:00:14s\n",
            "epoch 102| loss: 69130356736.0| val_0_rmse: 110007.85231|  0:00:14s\n",
            "epoch 103| loss: 67014327296.0| val_0_rmse: 109968.85259|  0:00:14s\n",
            "epoch 104| loss: 70657378304.0| val_0_rmse: 109928.2382|  0:00:14s\n",
            "epoch 105| loss: 66998253568.0| val_0_rmse: 109908.32401|  0:00:14s\n",
            "epoch 106| loss: 63418952704.0| val_0_rmse: 109908.17194|  0:00:14s\n",
            "epoch 107| loss: 77279053824.0| val_0_rmse: 110046.21691|  0:00:14s\n",
            "epoch 108| loss: 70087574528.0| val_0_rmse: 110035.20747|  0:00:14s\n",
            "epoch 109| loss: 75444754432.0| val_0_rmse: 110035.06314|  0:00:15s\n",
            "epoch 110| loss: 62787353600.0| val_0_rmse: 110052.99697|  0:00:15s\n",
            "epoch 111| loss: 52242803456.0| val_0_rmse: 110028.60264|  0:00:15s\n",
            "epoch 112| loss: 73495706624.0| val_0_rmse: 110005.57189|  0:00:15s\n",
            "epoch 113| loss: 73695251968.0| val_0_rmse: 109915.02418|  0:00:15s\n",
            "epoch 114| loss: 60356977408.0| val_0_rmse: 109727.63709|  0:00:15s\n",
            "epoch 115| loss: 61014487040.0| val_0_rmse: 109712.65901|  0:00:15s\n",
            "epoch 116| loss: 72091302144.0| val_0_rmse: 109693.61876|  0:00:15s\n",
            "epoch 117| loss: 76432147456.0| val_0_rmse: 109777.60179|  0:00:16s\n",
            "epoch 118| loss: 72871352320.0| val_0_rmse: 109867.73458|  0:00:16s\n",
            "epoch 119| loss: 73520946176.0| val_0_rmse: 109832.32682|  0:00:16s\n",
            "epoch 120| loss: 76270855168.0| val_0_rmse: 109837.84153|  0:00:16s\n",
            "epoch 121| loss: 76387654656.0| val_0_rmse: 109856.69177|  0:00:16s\n",
            "epoch 122| loss: 57890760704.0| val_0_rmse: 109503.10511|  0:00:16s\n",
            "epoch 123| loss: 67447314432.0| val_0_rmse: 109285.33879|  0:00:16s\n",
            "epoch 124| loss: 72070291456.0| val_0_rmse: 109212.12083|  0:00:16s\n",
            "epoch 125| loss: 73389548544.0| val_0_rmse: 109172.65345|  0:00:17s\n",
            "epoch 126| loss: 52099402752.0| val_0_rmse: 109141.29156|  0:00:17s\n",
            "epoch 127| loss: 78106786816.0| val_0_rmse: 109091.99912|  0:00:17s\n",
            "epoch 128| loss: 75307233280.0| val_0_rmse: 109048.84033|  0:00:17s\n",
            "epoch 129| loss: 74187832320.0| val_0_rmse: 109041.45205|  0:00:17s\n",
            "epoch 130| loss: 78890046464.0| val_0_rmse: 108743.10488|  0:00:17s\n",
            "epoch 131| loss: 72142195712.0| val_0_rmse: 108717.19151|  0:00:17s\n",
            "epoch 132| loss: 78314484736.0| val_0_rmse: 108689.6892|  0:00:17s\n",
            "epoch 133| loss: 57170423040.0| val_0_rmse: 108695.97282|  0:00:18s\n",
            "epoch 134| loss: 78649460736.0| val_0_rmse: 108764.78849|  0:00:18s\n",
            "epoch 135| loss: 65893450752.0| val_0_rmse: 108944.96845|  0:00:18s\n",
            "epoch 136| loss: 73191941120.0| val_0_rmse: 108917.78913|  0:00:18s\n",
            "epoch 137| loss: 46620576256.0| val_0_rmse: 108874.48077|  0:00:18s\n",
            "epoch 138| loss: 75653898240.0| val_0_rmse: 108881.11856|  0:00:18s\n",
            "epoch 139| loss: 64856430592.0| val_0_rmse: 108896.20537|  0:00:18s\n",
            "epoch 140| loss: 77295477760.0| val_0_rmse: 108833.20781|  0:00:19s\n",
            "epoch 141| loss: 64406352384.0| val_0_rmse: 108759.46788|  0:00:19s\n",
            "epoch 142| loss: 67434048000.0| val_0_rmse: 108684.85806|  0:00:19s\n",
            "epoch 143| loss: 69583019008.0| val_0_rmse: 108650.73835|  0:00:19s\n",
            "epoch 144| loss: 72760034304.0| val_0_rmse: 108574.64332|  0:00:19s\n",
            "epoch 145| loss: 56134294528.0| val_0_rmse: 108463.22058|  0:00:19s\n",
            "epoch 146| loss: 57603214336.0| val_0_rmse: 108501.7578|  0:00:19s\n",
            "epoch 147| loss: 73767363584.0| val_0_rmse: 108492.73364|  0:00:19s\n",
            "epoch 148| loss: 70867004672.0| val_0_rmse: 108464.73338|  0:00:19s\n",
            "epoch 149| loss: 70042130432.0| val_0_rmse: 108350.38322|  0:00:20s\n",
            "epoch 150| loss: 60275208192.0| val_0_rmse: 108448.82914|  0:00:20s\n",
            "epoch 151| loss: 72755831808.0| val_0_rmse: 108411.40993|  0:00:20s\n",
            "epoch 152| loss: 66878945792.0| val_0_rmse: 108392.44378|  0:00:20s\n",
            "epoch 153| loss: 67200592896.0| val_0_rmse: 108374.90331|  0:00:20s\n",
            "epoch 154| loss: 67492777984.0| val_0_rmse: 108445.06362|  0:00:20s\n",
            "epoch 155| loss: 65205313536.0| val_0_rmse: 108393.75661|  0:00:20s\n",
            "epoch 156| loss: 71116156928.0| val_0_rmse: 108230.96129|  0:00:20s\n",
            "epoch 157| loss: 74282755072.0| val_0_rmse: 108281.11647|  0:00:20s\n",
            "epoch 158| loss: 77493186560.0| val_0_rmse: 108302.45184|  0:00:21s\n",
            "epoch 159| loss: 76381110272.0| val_0_rmse: 108285.60933|  0:00:21s\n",
            "epoch 160| loss: 73863315456.0| val_0_rmse: 108218.60696|  0:00:21s\n",
            "epoch 161| loss: 77057047040.0| val_0_rmse: 108097.5583|  0:00:21s\n",
            "epoch 162| loss: 57283688448.0| val_0_rmse: 107600.04386|  0:00:21s\n",
            "epoch 163| loss: 60080462592.0| val_0_rmse: 107540.47749|  0:00:21s\n",
            "epoch 164| loss: 60728257536.0| val_0_rmse: 107322.1953|  0:00:21s\n",
            "epoch 165| loss: 70615535616.0| val_0_rmse: 107355.02456|  0:00:21s\n",
            "epoch 166| loss: 72398869504.0| val_0_rmse: 107384.15128|  0:00:22s\n",
            "epoch 167| loss: 63434862080.0| val_0_rmse: 107414.7761|  0:00:22s\n",
            "epoch 168| loss: 77564807168.0| val_0_rmse: 107434.42087|  0:00:22s\n",
            "epoch 169| loss: 65989558272.0| val_0_rmse: 107762.83288|  0:00:22s\n",
            "epoch 170| loss: 59126115328.0| val_0_rmse: 107801.85176|  0:00:22s\n",
            "epoch 171| loss: 70095659008.0| val_0_rmse: 107837.74887|  0:00:22s\n",
            "epoch 172| loss: 64712623616.0| val_0_rmse: 107830.0453|  0:00:22s\n",
            "epoch 173| loss: 69907167232.0| val_0_rmse: 107797.86969|  0:00:22s\n",
            "epoch 174| loss: 61599488000.0| val_0_rmse: 107773.42082|  0:00:23s\n",
            "epoch 175| loss: 74879154688.0| val_0_rmse: 107749.11439|  0:00:23s\n",
            "epoch 176| loss: 71803506176.0| val_0_rmse: 107813.64062|  0:00:23s\n",
            "epoch 177| loss: 68117292032.0| val_0_rmse: 107230.09712|  0:00:23s\n",
            "epoch 178| loss: 74567418368.0| val_0_rmse: 107062.13202|  0:00:23s\n",
            "epoch 179| loss: 77897796608.0| val_0_rmse: 107196.79419|  0:00:23s\n",
            "epoch 180| loss: 72481624576.0| val_0_rmse: 107604.72622|  0:00:23s\n",
            "epoch 181| loss: 70813254656.0| val_0_rmse: 107587.9186|  0:00:23s\n",
            "epoch 182| loss: 64423599360.0| val_0_rmse: 107535.54067|  0:00:24s\n",
            "epoch 183| loss: 71134631936.0| val_0_rmse: 107515.66881|  0:00:24s\n",
            "epoch 184| loss: 62403561472.0| val_0_rmse: 107490.89602|  0:00:24s\n",
            "epoch 185| loss: 72709267456.0| val_0_rmse: 107429.6655|  0:00:24s\n",
            "epoch 186| loss: 69565110272.0| val_0_rmse: 107156.4196|  0:00:24s\n",
            "epoch 187| loss: 72376408064.0| val_0_rmse: 106890.83148|  0:00:24s\n",
            "epoch 188| loss: 72497636352.0| val_0_rmse: 106589.62325|  0:00:24s\n",
            "epoch 189| loss: 75959435264.0| val_0_rmse: 106368.85562|  0:00:24s\n",
            "epoch 190| loss: 76825550848.0| val_0_rmse: 106554.33851|  0:00:24s\n",
            "epoch 191| loss: 60162624512.0| val_0_rmse: 107299.40527|  0:00:25s\n",
            "epoch 192| loss: 72753803264.0| val_0_rmse: 107997.07174|  0:00:25s\n",
            "epoch 193| loss: 70383507968.0| val_0_rmse: 108072.97158|  0:00:25s\n",
            "epoch 194| loss: 66764388352.0| val_0_rmse: 107791.53366|  0:00:25s\n",
            "epoch 195| loss: 66311559168.0| val_0_rmse: 107795.36769|  0:00:25s\n",
            "epoch 196| loss: 64902235136.0| val_0_rmse: 107676.34813|  0:00:25s\n",
            "epoch 197| loss: 72628851712.0| val_0_rmse: 107996.16389|  0:00:25s\n",
            "epoch 198| loss: 72589241344.0| val_0_rmse: 108148.80434|  0:00:25s\n",
            "epoch 199| loss: 69353041408.0| val_0_rmse: 108180.49049|  0:00:25s\n",
            "epoch 200| loss: 57034761216.0| val_0_rmse: 107721.70405|  0:00:26s\n",
            "epoch 201| loss: 74260099584.0| val_0_rmse: 107393.69309|  0:00:26s\n",
            "epoch 202| loss: 76265035776.0| val_0_rmse: 107943.60207|  0:00:26s\n",
            "epoch 203| loss: 68099324416.0| val_0_rmse: 108036.77464|  0:00:26s\n",
            "epoch 204| loss: 73397888000.0| val_0_rmse: 107984.81652|  0:00:26s\n",
            "epoch 205| loss: 56661947776.0| val_0_rmse: 107945.0607|  0:00:26s\n",
            "epoch 206| loss: 58903975936.0| val_0_rmse: 107903.93329|  0:00:26s\n",
            "epoch 207| loss: 64602024192.0| val_0_rmse: 107894.42194|  0:00:26s\n",
            "epoch 208| loss: 71753822208.0| val_0_rmse: 107845.89831|  0:00:27s\n",
            "epoch 209| loss: 67591267328.0| val_0_rmse: 107873.65697|  0:00:27s\n",
            "epoch 210| loss: 67254500352.0| val_0_rmse: 107833.01229|  0:00:27s\n",
            "epoch 211| loss: 71044933632.0| val_0_rmse: 107951.05952|  0:00:27s\n",
            "epoch 212| loss: 63399399424.0| val_0_rmse: 107963.81127|  0:00:27s\n",
            "epoch 213| loss: 65551438848.0| val_0_rmse: 107959.74904|  0:00:27s\n",
            "epoch 214| loss: 65818970112.0| val_0_rmse: 108010.03644|  0:00:27s\n",
            "epoch 215| loss: 63373904128.0| val_0_rmse: 107830.09216|  0:00:27s\n",
            "epoch 216| loss: 67457818624.0| val_0_rmse: 107609.59058|  0:00:27s\n",
            "epoch 217| loss: 53965092352.0| val_0_rmse: 107551.96255|  0:00:28s\n",
            "epoch 218| loss: 62844665856.0| val_0_rmse: 107655.52389|  0:00:28s\n",
            "epoch 219| loss: 75643905024.0| val_0_rmse: 107503.43934|  0:00:28s\n",
            "epoch 220| loss: 55497411584.0| val_0_rmse: 106508.61675|  0:00:28s\n",
            "epoch 221| loss: 71207385088.0| val_0_rmse: 106544.50259|  0:00:28s\n",
            "epoch 222| loss: 67618673664.0| val_0_rmse: 105906.5446|  0:00:28s\n",
            "epoch 223| loss: 73799502848.0| val_0_rmse: 105388.64507|  0:00:28s\n",
            "epoch 224| loss: 66061275392.0| val_0_rmse: 105133.12548|  0:00:28s\n",
            "epoch 225| loss: 73801568256.0| val_0_rmse: 105135.88059|  0:00:28s\n",
            "epoch 226| loss: 59794411520.0| val_0_rmse: 105089.86436|  0:00:29s\n",
            "epoch 227| loss: 61149490176.0| val_0_rmse: 105367.25094|  0:00:29s\n",
            "epoch 228| loss: 59607342080.0| val_0_rmse: 105818.81465|  0:00:29s\n",
            "epoch 229| loss: 74260231168.0| val_0_rmse: 105809.15828|  0:00:29s\n",
            "epoch 230| loss: 65937384448.0| val_0_rmse: 105633.30868|  0:00:29s\n",
            "epoch 231| loss: 76776715264.0| val_0_rmse: 105461.62755|  0:00:29s\n",
            "epoch 232| loss: 67722981376.0| val_0_rmse: 105293.15648|  0:00:29s\n",
            "epoch 233| loss: 70539085056.0| val_0_rmse: 105419.58315|  0:00:29s\n",
            "epoch 234| loss: 69115888128.0| val_0_rmse: 105496.75572|  0:00:29s\n",
            "epoch 235| loss: 67250812160.0| val_0_rmse: 105849.83538|  0:00:30s\n",
            "epoch 236| loss: 64528237056.0| val_0_rmse: 105919.80244|  0:00:30s\n",
            "epoch 237| loss: 75353495552.0| val_0_rmse: 106033.02357|  0:00:30s\n",
            "epoch 238| loss: 74027694080.0| val_0_rmse: 106113.07355|  0:00:30s\n",
            "epoch 239| loss: 60471630336.0| val_0_rmse: 106113.31069|  0:00:30s\n",
            "epoch 240| loss: 64388400128.0| val_0_rmse: 106055.63862|  0:00:30s\n",
            "epoch 241| loss: 72610574336.0| val_0_rmse: 105834.58737|  0:00:30s\n",
            "epoch 242| loss: 60637902848.0| val_0_rmse: 105689.24335|  0:00:30s\n",
            "epoch 243| loss: 58113946368.0| val_0_rmse: 105740.78266|  0:00:31s\n",
            "epoch 244| loss: 61643610624.0| val_0_rmse: 105909.19522|  0:00:31s\n",
            "epoch 245| loss: 71885473280.0| val_0_rmse: 106438.31653|  0:00:31s\n",
            "epoch 246| loss: 65510694912.0| val_0_rmse: 106687.78274|  0:00:31s\n",
            "epoch 247| loss: 73942079488.0| val_0_rmse: 106532.47686|  0:00:31s\n",
            "epoch 248| loss: 70020127232.0| val_0_rmse: 106364.3391|  0:00:31s\n",
            "epoch 249| loss: 58867497984.0| val_0_rmse: 106289.19678|  0:00:31s\n",
            "epoch 250| loss: 60776314880.0| val_0_rmse: 106249.36491|  0:00:31s\n",
            "epoch 251| loss: 64835268096.0| val_0_rmse: 106387.34135|  0:00:31s\n",
            "epoch 252| loss: 72856427520.0| val_0_rmse: 106539.69398|  0:00:32s\n",
            "epoch 253| loss: 64384984064.0| val_0_rmse: 106223.1134|  0:00:32s\n",
            "epoch 254| loss: 61336854016.0| val_0_rmse: 105584.25658|  0:00:32s\n",
            "epoch 255| loss: 67871405568.0| val_0_rmse: 105532.89459|  0:00:32s\n",
            "epoch 256| loss: 59911366656.0| val_0_rmse: 105694.35974|  0:00:32s\n",
            "epoch 257| loss: 60860233728.0| val_0_rmse: 105201.58543|  0:00:32s\n",
            "epoch 258| loss: 56819153408.0| val_0_rmse: 105228.71694|  0:00:32s\n",
            "epoch 259| loss: 57209880064.0| val_0_rmse: 105673.33086|  0:00:32s\n",
            "epoch 260| loss: 70250244864.0| val_0_rmse: 105704.47724|  0:00:33s\n",
            "epoch 261| loss: 49308405248.0| val_0_rmse: 105650.30848|  0:00:33s\n",
            "epoch 262| loss: 65027180544.0| val_0_rmse: 105689.07315|  0:00:33s\n",
            "epoch 263| loss: 56637349376.0| val_0_rmse: 105169.18213|  0:00:33s\n",
            "epoch 264| loss: 70535471616.0| val_0_rmse: 105403.52376|  0:00:33s\n",
            "epoch 265| loss: 71259749376.0| val_0_rmse: 106112.52906|  0:00:33s\n",
            "epoch 266| loss: 65713101824.0| val_0_rmse: 106324.81561|  0:00:33s\n",
            "epoch 267| loss: 69565705216.0| val_0_rmse: 106571.96972|  0:00:33s\n",
            "epoch 268| loss: 68425802752.0| val_0_rmse: 106619.77112|  0:00:33s\n",
            "epoch 269| loss: 58218370048.0| val_0_rmse: 106557.19109|  0:00:34s\n",
            "epoch 270| loss: 66258075648.0| val_0_rmse: 106197.28618|  0:00:34s\n",
            "epoch 271| loss: 66400122368.0| val_0_rmse: 105638.37659|  0:00:34s\n",
            "epoch 272| loss: 65700792832.0| val_0_rmse: 104906.47916|  0:00:34s\n",
            "epoch 273| loss: 56069624320.0| val_0_rmse: 104325.61109|  0:00:34s\n",
            "epoch 274| loss: 62064962048.0| val_0_rmse: 103527.78748|  0:00:34s\n",
            "epoch 275| loss: 64124755968.0| val_0_rmse: 103343.40941|  0:00:34s\n",
            "epoch 276| loss: 74786959360.0| val_0_rmse: 103473.66498|  0:00:34s\n",
            "epoch 277| loss: 59001817088.0| val_0_rmse: 103412.57428|  0:00:35s\n",
            "epoch 278| loss: 53695642112.0| val_0_rmse: 103495.89975|  0:00:35s\n",
            "epoch 279| loss: 62470704896.0| val_0_rmse: 103561.94174|  0:00:35s\n",
            "epoch 280| loss: 59182820864.0| val_0_rmse: 103908.56834|  0:00:35s\n",
            "epoch 281| loss: 68511502336.0| val_0_rmse: 104159.2371|  0:00:35s\n",
            "epoch 282| loss: 50390095360.0| val_0_rmse: 104120.1148|  0:00:35s\n",
            "epoch 283| loss: 61161591808.0| val_0_rmse: 104065.99325|  0:00:35s\n",
            "epoch 284| loss: 59495689216.0| val_0_rmse: 103099.00489|  0:00:35s\n",
            "epoch 285| loss: 70988963840.0| val_0_rmse: 103057.54976|  0:00:36s\n",
            "epoch 286| loss: 64557618176.0| val_0_rmse: 103114.55482|  0:00:36s\n",
            "epoch 287| loss: 74860998656.0| val_0_rmse: 103456.38491|  0:00:36s\n",
            "epoch 288| loss: 62272709120.0| val_0_rmse: 103585.47223|  0:00:36s\n",
            "epoch 289| loss: 60928902144.0| val_0_rmse: 103637.93199|  0:00:36s\n",
            "epoch 290| loss: 52868570112.0| val_0_rmse: 103644.87703|  0:00:36s\n",
            "epoch 291| loss: 58464244736.0| val_0_rmse: 103561.95285|  0:00:36s\n",
            "epoch 292| loss: 69092748288.0| val_0_rmse: 103770.32496|  0:00:36s\n",
            "epoch 293| loss: 67775001600.0| val_0_rmse: 103576.03729|  0:00:37s\n",
            "epoch 294| loss: 67510742528.0| val_0_rmse: 102509.51556|  0:00:37s\n",
            "epoch 295| loss: 59748781312.0| val_0_rmse: 102509.87157|  0:00:37s\n",
            "epoch 296| loss: 59360215552.0| val_0_rmse: 102868.64402|  0:00:37s\n",
            "epoch 297| loss: 66226940416.0| val_0_rmse: 103523.75133|  0:00:37s\n",
            "epoch 298| loss: 69977106432.0| val_0_rmse: 103697.49443|  0:00:37s\n",
            "epoch 299| loss: 73067511296.0| val_0_rmse: 103225.15643|  0:00:37s\n",
            "epoch 300| loss: 61977288704.0| val_0_rmse: 102187.69535|  0:00:37s\n",
            "epoch 301| loss: 70925384704.0| val_0_rmse: 103178.32111|  0:00:37s\n",
            "epoch 302| loss: 56310890496.0| val_0_rmse: 104288.72318|  0:00:38s\n",
            "epoch 303| loss: 68422851584.0| val_0_rmse: 105037.34926|  0:00:38s\n",
            "epoch 304| loss: 61371704832.0| val_0_rmse: 105123.27058|  0:00:38s\n",
            "epoch 305| loss: 56165218816.0| val_0_rmse: 105145.77557|  0:00:38s\n",
            "epoch 306| loss: 69613039616.0| val_0_rmse: 105093.70042|  0:00:38s\n",
            "epoch 307| loss: 64493650688.0| val_0_rmse: 105064.54663|  0:00:38s\n",
            "epoch 308| loss: 68029830400.0| val_0_rmse: 104872.59325|  0:00:38s\n",
            "epoch 309| loss: 73946053376.0| val_0_rmse: 104790.57966|  0:00:38s\n",
            "epoch 310| loss: 63429359616.0| val_0_rmse: 103432.62059|  0:00:38s\n",
            "epoch 311| loss: 73547679744.0| val_0_rmse: 102341.04686|  0:00:39s\n",
            "epoch 312| loss: 68054390784.0| val_0_rmse: 101998.42575|  0:00:39s\n",
            "epoch 313| loss: 68826843136.0| val_0_rmse: 101929.64336|  0:00:39s\n",
            "epoch 314| loss: 70947697152.0| val_0_rmse: 101819.12659|  0:00:39s\n",
            "epoch 315| loss: 65635198976.0| val_0_rmse: 101672.25892|  0:00:39s\n",
            "epoch 316| loss: 71652281856.0| val_0_rmse: 101672.31253|  0:00:39s\n",
            "epoch 317| loss: 68015729920.0| val_0_rmse: 102162.32041|  0:00:39s\n",
            "epoch 318| loss: 66977297920.0| val_0_rmse: 103222.47462|  0:00:39s\n",
            "epoch 319| loss: 69381517312.0| val_0_rmse: 102751.98638|  0:00:40s\n",
            "epoch 320| loss: 68151431168.0| val_0_rmse: 102479.90252|  0:00:40s\n",
            "epoch 321| loss: 65932878848.0| val_0_rmse: 102495.87993|  0:00:40s\n",
            "epoch 322| loss: 63439734784.0| val_0_rmse: 102506.88736|  0:00:40s\n",
            "epoch 323| loss: 67473221376.0| val_0_rmse: 102095.4455|  0:00:40s\n",
            "epoch 324| loss: 60710332416.0| val_0_rmse: 102378.03037|  0:00:40s\n",
            "epoch 325| loss: 62984720896.0| val_0_rmse: 102127.71551|  0:00:40s\n",
            "epoch 326| loss: 71967585280.0| val_0_rmse: 102814.26877|  0:00:40s\n",
            "epoch 327| loss: 66766306304.0| val_0_rmse: 102342.33567|  0:00:41s\n",
            "epoch 328| loss: 70391143424.0| val_0_rmse: 101813.11003|  0:00:41s\n",
            "epoch 329| loss: 62239012864.0| val_0_rmse: 101392.81135|  0:00:41s\n",
            "epoch 330| loss: 59209444864.0| val_0_rmse: 101253.04162|  0:00:41s\n",
            "epoch 331| loss: 61470116352.0| val_0_rmse: 101125.85724|  0:00:41s\n",
            "epoch 332| loss: 65340718080.0| val_0_rmse: 101149.74536|  0:00:41s\n",
            "epoch 333| loss: 66363652096.0| val_0_rmse: 101117.10523|  0:00:41s\n",
            "epoch 334| loss: 68357250048.0| val_0_rmse: 102222.04914|  0:00:41s\n",
            "epoch 335| loss: 71767056896.0| val_0_rmse: 103138.0644|  0:00:42s\n",
            "epoch 336| loss: 61664105216.0| val_0_rmse: 103285.99679|  0:00:42s\n",
            "epoch 337| loss: 71858605568.0| val_0_rmse: 102579.64084|  0:00:42s\n",
            "epoch 338| loss: 63079841792.0| val_0_rmse: 102522.00517|  0:00:42s\n",
            "epoch 339| loss: 63256382464.0| val_0_rmse: 102951.91399|  0:00:42s\n",
            "epoch 340| loss: 69507792384.0| val_0_rmse: 102264.52091|  0:00:42s\n",
            "epoch 341| loss: 65015297024.0| val_0_rmse: 101667.52002|  0:00:42s\n",
            "epoch 342| loss: 71742315520.0| val_0_rmse: 101215.7008|  0:00:42s\n",
            "epoch 343| loss: 66999246848.0| val_0_rmse: 100568.87634|  0:00:42s\n",
            "epoch 344| loss: 59217174784.0| val_0_rmse: 100280.33564|  0:00:43s\n",
            "epoch 345| loss: 55574509568.0| val_0_rmse: 100344.88799|  0:00:43s\n",
            "epoch 346| loss: 56809563648.0| val_0_rmse: 100586.69581|  0:00:43s\n",
            "epoch 347| loss: 52521604608.0| val_0_rmse: 99809.41705|  0:00:43s\n",
            "epoch 348| loss: 67243635712.0| val_0_rmse: 99953.54185|  0:00:43s\n",
            "epoch 349| loss: 57553021696.0| val_0_rmse: 99845.04719|  0:00:43s\n",
            "epoch 350| loss: 65217960832.0| val_0_rmse: 99980.20327|  0:00:43s\n",
            "epoch 351| loss: 63644610816.0| val_0_rmse: 101089.20747|  0:00:43s\n",
            "epoch 352| loss: 65578820608.0| val_0_rmse: 101858.01884|  0:00:44s\n",
            "epoch 353| loss: 58650723584.0| val_0_rmse: 101714.83508|  0:00:44s\n",
            "epoch 354| loss: 59227828224.0| val_0_rmse: 100264.92001|  0:00:44s\n",
            "epoch 355| loss: 68200441856.0| val_0_rmse: 99579.84082|  0:00:44s\n",
            "epoch 356| loss: 55288719872.0| val_0_rmse: 99534.21893|  0:00:44s\n",
            "epoch 357| loss: 69505008640.0| val_0_rmse: 99841.55335|  0:00:44s\n",
            "epoch 358| loss: 68086227968.0| val_0_rmse: 101583.22134|  0:00:44s\n",
            "epoch 359| loss: 71774048256.0| val_0_rmse: 102346.54644|  0:00:44s\n",
            "epoch 360| loss: 60102484992.0| val_0_rmse: 102178.93555|  0:00:45s\n",
            "epoch 361| loss: 66411040768.0| val_0_rmse: 101994.37587|  0:00:45s\n",
            "epoch 362| loss: 64674708480.0| val_0_rmse: 102064.92537|  0:00:45s\n",
            "epoch 363| loss: 64537797632.0| val_0_rmse: 102755.77574|  0:00:45s\n",
            "epoch 364| loss: 61684160512.0| val_0_rmse: 103011.62149|  0:00:45s\n",
            "epoch 365| loss: 67644268544.0| val_0_rmse: 102983.9429|  0:00:45s\n",
            "epoch 366| loss: 66126854144.0| val_0_rmse: 102829.69887|  0:00:45s\n",
            "epoch 367| loss: 56285059840.0| val_0_rmse: 102706.88225|  0:00:45s\n",
            "epoch 368| loss: 63987034112.0| val_0_rmse: 102953.25861|  0:00:45s\n",
            "epoch 369| loss: 65411844096.0| val_0_rmse: 103031.72422|  0:00:46s\n",
            "epoch 370| loss: 58706779648.0| val_0_rmse: 102803.9931|  0:00:46s\n",
            "epoch 371| loss: 53048675456.0| val_0_rmse: 101626.6791|  0:00:46s\n",
            "epoch 372| loss: 64728084992.0| val_0_rmse: 101534.21851|  0:00:46s\n",
            "epoch 373| loss: 68634478080.0| val_0_rmse: 101712.70501|  0:00:46s\n",
            "epoch 374| loss: 50521182208.0| val_0_rmse: 101612.16666|  0:00:46s\n",
            "epoch 375| loss: 59658678016.0| val_0_rmse: 101145.21699|  0:00:46s\n",
            "epoch 376| loss: 54199546880.0| val_0_rmse: 100725.86787|  0:00:46s\n",
            "epoch 377| loss: 63396475904.0| val_0_rmse: 100973.58282|  0:00:47s\n",
            "epoch 378| loss: 60004301824.0| val_0_rmse: 101527.1621|  0:00:47s\n",
            "epoch 379| loss: 61947097088.0| val_0_rmse: 101725.79288|  0:00:47s\n",
            "epoch 380| loss: 65216804864.0| val_0_rmse: 101480.64938|  0:00:47s\n",
            "epoch 381| loss: 50960041472.0| val_0_rmse: 101502.70719|  0:00:47s\n",
            "epoch 382| loss: 54540553216.0| val_0_rmse: 101494.39839|  0:00:47s\n",
            "epoch 383| loss: 59406784000.0| val_0_rmse: 101485.8099|  0:00:47s\n",
            "epoch 384| loss: 67693270528.0| val_0_rmse: 101374.81944|  0:00:47s\n",
            "epoch 385| loss: 50368671488.0| val_0_rmse: 100866.16888|  0:00:47s\n",
            "epoch 386| loss: 50260022272.0| val_0_rmse: 100701.58407|  0:00:48s\n",
            "epoch 387| loss: 57441899520.0| val_0_rmse: 100841.84293|  0:00:48s\n",
            "epoch 388| loss: 48727759872.0| val_0_rmse: 100982.65708|  0:00:48s\n",
            "epoch 389| loss: 59308592128.0| val_0_rmse: 100970.41429|  0:00:48s\n",
            "epoch 390| loss: 61973269504.0| val_0_rmse: 101071.21142|  0:00:48s\n",
            "epoch 391| loss: 60818987008.0| val_0_rmse: 100803.91522|  0:00:48s\n",
            "epoch 392| loss: 64889403392.0| val_0_rmse: 100564.47184|  0:00:48s\n",
            "epoch 393| loss: 55275521024.0| val_0_rmse: 98857.5961|  0:00:49s\n",
            "epoch 394| loss: 52386439168.0| val_0_rmse: 98909.44423|  0:00:49s\n",
            "epoch 395| loss: 49805203200.0| val_0_rmse: 99272.01041|  0:00:49s\n",
            "epoch 396| loss: 66263381504.0| val_0_rmse: 98525.35795|  0:00:49s\n",
            "epoch 397| loss: 52554946560.0| val_0_rmse: 98376.71536|  0:00:49s\n",
            "epoch 398| loss: 63583623168.0| val_0_rmse: 98775.49988|  0:00:49s\n",
            "epoch 399| loss: 64201157632.0| val_0_rmse: 100293.23782|  0:00:49s\n",
            "epoch 400| loss: 56722481152.0| val_0_rmse: 99437.49655|  0:00:49s\n",
            "epoch 401| loss: 59647784448.0| val_0_rmse: 100154.09245|  0:00:50s\n",
            "epoch 402| loss: 55782609408.0| val_0_rmse: 99864.33835|  0:00:50s\n",
            "epoch 403| loss: 62358934528.0| val_0_rmse: 99613.13783|  0:00:50s\n",
            "epoch 404| loss: 61468570112.0| val_0_rmse: 100074.22727|  0:00:50s\n",
            "epoch 405| loss: 58927225856.0| val_0_rmse: 100236.59671|  0:00:50s\n",
            "epoch 406| loss: 68137254400.0| val_0_rmse: 100289.9964|  0:00:50s\n",
            "epoch 407| loss: 60815352832.0| val_0_rmse: 100539.51392|  0:00:50s\n",
            "epoch 408| loss: 63240735232.0| val_0_rmse: 100949.25853|  0:00:50s\n",
            "epoch 409| loss: 61238629376.0| val_0_rmse: 101721.3309|  0:00:50s\n",
            "epoch 410| loss: 60694871040.0| val_0_rmse: 102198.00462|  0:00:51s\n",
            "epoch 411| loss: 55491237376.0| val_0_rmse: 102067.15499|  0:00:51s\n",
            "epoch 412| loss: 58350369792.0| val_0_rmse: 101864.69525|  0:00:51s\n",
            "epoch 413| loss: 57569096192.0| val_0_rmse: 102008.0564|  0:00:51s\n",
            "epoch 414| loss: 55354215424.0| val_0_rmse: 101978.93335|  0:00:51s\n",
            "epoch 415| loss: 59347190784.0| val_0_rmse: 101623.8875|  0:00:51s\n",
            "epoch 416| loss: 57627128320.0| val_0_rmse: 99934.94054|  0:00:51s\n",
            "epoch 417| loss: 68459429888.0| val_0_rmse: 99164.09793|  0:00:51s\n",
            "epoch 418| loss: 67477121024.0| val_0_rmse: 98942.65008|  0:00:51s\n",
            "epoch 419| loss: 43982603776.0| val_0_rmse: 97356.65467|  0:00:52s\n",
            "epoch 420| loss: 57611788288.0| val_0_rmse: 96955.71625|  0:00:52s\n",
            "epoch 421| loss: 54564087808.0| val_0_rmse: 96859.77426|  0:00:52s\n",
            "epoch 422| loss: 61680082944.0| val_0_rmse: 96833.57169|  0:00:52s\n",
            "epoch 423| loss: 65766346752.0| val_0_rmse: 96831.39541|  0:00:52s\n",
            "epoch 424| loss: 63732128768.0| val_0_rmse: 96969.63357|  0:00:52s\n",
            "epoch 425| loss: 63098061824.0| val_0_rmse: 97427.38027|  0:00:52s\n",
            "epoch 426| loss: 68541422592.0| val_0_rmse: 97819.50366|  0:00:53s\n",
            "epoch 427| loss: 67344242176.0| val_0_rmse: 98108.8524|  0:00:53s\n",
            "epoch 428| loss: 61742129152.0| val_0_rmse: 98106.55947|  0:00:53s\n",
            "epoch 429| loss: 67311475712.0| val_0_rmse: 98222.1613|  0:00:53s\n",
            "epoch 430| loss: 61629299712.0| val_0_rmse: 98162.65209|  0:00:53s\n",
            "epoch 431| loss: 62278221824.0| val_0_rmse: 97735.93177|  0:00:53s\n",
            "epoch 432| loss: 56741042176.0| val_0_rmse: 97973.8677|  0:00:53s\n",
            "epoch 433| loss: 68566337536.0| val_0_rmse: 97622.17502|  0:00:53s\n",
            "epoch 434| loss: 69851073536.0| val_0_rmse: 97250.35313|  0:00:53s\n",
            "epoch 435| loss: 57296386048.0| val_0_rmse: 97491.49373|  0:00:54s\n",
            "epoch 436| loss: 56319931392.0| val_0_rmse: 97352.57614|  0:00:54s\n",
            "epoch 437| loss: 59976662016.0| val_0_rmse: 97469.42957|  0:00:54s\n",
            "epoch 438| loss: 48253005824.0| val_0_rmse: 96644.79806|  0:00:54s\n",
            "epoch 439| loss: 63834858496.0| val_0_rmse: 96608.61734|  0:00:54s\n",
            "epoch 440| loss: 57802560512.0| val_0_rmse: 96763.9183|  0:00:54s\n",
            "epoch 441| loss: 60144978944.0| val_0_rmse: 96890.78571|  0:00:54s\n",
            "epoch 442| loss: 56686425088.0| val_0_rmse: 96361.53185|  0:00:54s\n",
            "epoch 443| loss: 58749499392.0| val_0_rmse: 95457.03984|  0:00:54s\n",
            "epoch 444| loss: 53495690752.0| val_0_rmse: 96182.50282|  0:00:55s\n",
            "epoch 445| loss: 59030522432.0| val_0_rmse: 97469.87193|  0:00:55s\n",
            "epoch 446| loss: 53727760896.0| val_0_rmse: 99457.75458|  0:00:55s\n",
            "epoch 447| loss: 65758184960.0| val_0_rmse: 98885.5718|  0:00:55s\n",
            "epoch 448| loss: 66350314496.0| val_0_rmse: 98548.04786|  0:00:55s\n",
            "epoch 449| loss: 66917846016.0| val_0_rmse: 98575.22754|  0:00:55s\n",
            "epoch 450| loss: 60896904704.0| val_0_rmse: 100182.96678|  0:00:55s\n",
            "epoch 451| loss: 68333529088.0| val_0_rmse: 100536.3687|  0:00:55s\n",
            "epoch 452| loss: 54849018368.0| val_0_rmse: 99104.25955|  0:00:56s\n",
            "epoch 453| loss: 52425624320.0| val_0_rmse: 98488.66233|  0:00:56s\n",
            "epoch 454| loss: 51935476736.0| val_0_rmse: 97920.52114|  0:00:56s\n",
            "epoch 455| loss: 62972119552.0| val_0_rmse: 98670.87552|  0:00:56s\n",
            "epoch 456| loss: 52529225728.0| val_0_rmse: 98853.83328|  0:00:56s\n",
            "epoch 457| loss: 57680232448.0| val_0_rmse: 99209.39737|  0:00:56s\n",
            "epoch 458| loss: 56736056960.0| val_0_rmse: 99186.789|  0:00:56s\n",
            "epoch 459| loss: 55925142016.0| val_0_rmse: 97169.35068|  0:00:56s\n",
            "epoch 460| loss: 58079531520.0| val_0_rmse: 96361.36081|  0:00:57s\n",
            "epoch 461| loss: 66968711168.0| val_0_rmse: 96131.23744|  0:00:57s\n",
            "epoch 462| loss: 64820279296.0| val_0_rmse: 96168.30726|  0:00:57s\n",
            "epoch 463| loss: 65759508480.0| val_0_rmse: 96197.5931|  0:00:57s\n",
            "epoch 464| loss: 52241635328.0| val_0_rmse: 96784.60019|  0:00:57s\n",
            "epoch 465| loss: 61914092288.0| val_0_rmse: 97692.87515|  0:00:57s\n",
            "epoch 466| loss: 68849903616.0| val_0_rmse: 99190.25006|  0:00:57s\n",
            "epoch 467| loss: 60728262656.0| val_0_rmse: 100108.8506|  0:00:57s\n",
            "epoch 468| loss: 59710878720.0| val_0_rmse: 100537.20998|  0:00:57s\n",
            "epoch 469| loss: 57434380288.0| val_0_rmse: 101008.40845|  0:00:58s\n",
            "epoch 470| loss: 58652634112.0| val_0_rmse: 100975.26755|  0:00:58s\n",
            "epoch 471| loss: 64794422400.0| val_0_rmse: 100076.07171|  0:00:58s\n",
            "epoch 472| loss: 55335460352.0| val_0_rmse: 98521.15893|  0:00:58s\n",
            "epoch 473| loss: 48193583872.0| val_0_rmse: 97013.09705|  0:00:58s\n",
            "epoch 474| loss: 57490744832.0| val_0_rmse: 96349.03255|  0:00:58s\n",
            "epoch 475| loss: 61391838208.0| val_0_rmse: 97302.6838|  0:00:58s\n",
            "epoch 476| loss: 58551307776.0| val_0_rmse: 97741.77932|  0:00:58s\n",
            "epoch 477| loss: 49778833408.0| val_0_rmse: 97689.48142|  0:00:58s\n",
            "epoch 478| loss: 59300069376.0| val_0_rmse: 98069.1312|  0:00:59s\n",
            "epoch 479| loss: 43799253504.0| val_0_rmse: 98703.83922|  0:00:59s\n",
            "epoch 480| loss: 63942365184.0| val_0_rmse: 99386.27942|  0:00:59s\n",
            "epoch 481| loss: 55590305792.0| val_0_rmse: 99516.56626|  0:00:59s\n",
            "epoch 482| loss: 56903221888.0| val_0_rmse: 99906.44486|  0:00:59s\n",
            "epoch 483| loss: 52370338304.0| val_0_rmse: 99490.74084|  0:00:59s\n",
            "epoch 484| loss: 51943933952.0| val_0_rmse: 99406.59325|  0:00:59s\n",
            "epoch 485| loss: 66112911360.0| val_0_rmse: 99152.94072|  0:00:59s\n",
            "epoch 486| loss: 55996544000.0| val_0_rmse: 98664.41441|  0:01:00s\n",
            "epoch 487| loss: 54409186816.0| val_0_rmse: 98375.13582|  0:01:00s\n",
            "epoch 488| loss: 59535784960.0| val_0_rmse: 97654.50104|  0:01:00s\n",
            "epoch 489| loss: 51523634688.0| val_0_rmse: 97539.18953|  0:01:00s\n",
            "epoch 490| loss: 63737909248.0| val_0_rmse: 96380.74065|  0:01:00s\n",
            "epoch 491| loss: 55714002432.0| val_0_rmse: 95993.79189|  0:01:00s\n",
            "epoch 492| loss: 64453109760.0| val_0_rmse: 95644.47048|  0:01:00s\n",
            "epoch 493| loss: 55819934208.0| val_0_rmse: 95851.09323|  0:01:00s\n",
            "epoch 494| loss: 66025820160.0| val_0_rmse: 94968.7602|  0:01:01s\n",
            "epoch 495| loss: 59923716096.0| val_0_rmse: 94763.0598|  0:01:01s\n",
            "epoch 496| loss: 50075145216.0| val_0_rmse: 94819.77675|  0:01:01s\n",
            "epoch 497| loss: 64278009088.0| val_0_rmse: 95459.45276|  0:01:01s\n",
            "epoch 498| loss: 52451749376.0| val_0_rmse: 96132.21678|  0:01:01s\n",
            "epoch 499| loss: 59627855872.0| val_0_rmse: 96535.85649|  0:01:01s\n",
            "epoch 500| loss: 56747119616.0| val_0_rmse: 96861.29544|  0:01:01s\n",
            "epoch 501| loss: 56494384128.0| val_0_rmse: 97059.5276|  0:01:01s\n",
            "epoch 502| loss: 57003403264.0| val_0_rmse: 97303.83885|  0:01:02s\n",
            "epoch 503| loss: 64648860160.0| val_0_rmse: 97298.60416|  0:01:02s\n",
            "epoch 504| loss: 47753434624.0| val_0_rmse: 97068.07437|  0:01:02s\n",
            "epoch 505| loss: 57997649920.0| val_0_rmse: 96482.40905|  0:01:02s\n",
            "epoch 506| loss: 55326258688.0| val_0_rmse: 96110.62032|  0:01:02s\n",
            "epoch 507| loss: 55661837568.0| val_0_rmse: 96162.84485|  0:01:02s\n",
            "epoch 508| loss: 58803049472.0| val_0_rmse: 96224.2051|  0:01:02s\n",
            "epoch 509| loss: 65607474176.0| val_0_rmse: 95810.53601|  0:01:02s\n",
            "epoch 510| loss: 47430872064.0| val_0_rmse: 95439.79281|  0:01:02s\n",
            "epoch 511| loss: 65949544448.0| val_0_rmse: 94595.00376|  0:01:03s\n",
            "epoch 512| loss: 56829921280.0| val_0_rmse: 94071.84034|  0:01:03s\n",
            "epoch 513| loss: 54521324800.0| val_0_rmse: 93997.59414|  0:01:03s\n",
            "epoch 514| loss: 50746875392.0| val_0_rmse: 94817.53134|  0:01:03s\n",
            "epoch 515| loss: 41957820928.0| val_0_rmse: 94522.79585|  0:01:03s\n",
            "epoch 516| loss: 57093142528.0| val_0_rmse: 95417.40295|  0:01:03s\n",
            "epoch 517| loss: 63890800640.0| val_0_rmse: 97963.69944|  0:01:03s\n",
            "epoch 518| loss: 63796577280.0| val_0_rmse: 99097.6429|  0:01:03s\n",
            "epoch 519| loss: 60845652992.0| val_0_rmse: 99946.8041|  0:01:04s\n",
            "epoch 520| loss: 57564843008.0| val_0_rmse: 99658.71685|  0:01:04s\n",
            "epoch 521| loss: 63370863616.0| val_0_rmse: 99561.07615|  0:01:04s\n",
            "epoch 522| loss: 62067480576.0| val_0_rmse: 99716.04054|  0:01:04s\n",
            "epoch 523| loss: 65740517376.0| val_0_rmse: 99137.63047|  0:01:04s\n",
            "epoch 524| loss: 57170019328.0| val_0_rmse: 98110.0735|  0:01:04s\n",
            "epoch 525| loss: 59540355072.0| val_0_rmse: 97865.42004|  0:01:04s\n",
            "epoch 526| loss: 66293495552.0| val_0_rmse: 97594.07361|  0:01:04s\n",
            "epoch 527| loss: 62530042368.0| val_0_rmse: 96686.98903|  0:01:05s\n",
            "epoch 528| loss: 66754664960.0| val_0_rmse: 95286.54235|  0:01:05s\n",
            "epoch 529| loss: 66445841920.0| val_0_rmse: 94114.17666|  0:01:05s\n",
            "epoch 530| loss: 54935813632.0| val_0_rmse: 94452.56366|  0:01:05s\n",
            "epoch 531| loss: 64158285824.0| val_0_rmse: 95171.3618|  0:01:05s\n",
            "epoch 532| loss: 59314128384.0| val_0_rmse: 95569.73961|  0:01:05s\n",
            "epoch 533| loss: 53160187392.0| val_0_rmse: 96423.28841|  0:01:05s\n",
            "epoch 534| loss: 66673380352.0| val_0_rmse: 96911.86439|  0:01:05s\n",
            "epoch 535| loss: 55493637120.0| val_0_rmse: 97569.71491|  0:01:05s\n",
            "epoch 536| loss: 57194575872.0| val_0_rmse: 97915.47818|  0:01:06s\n",
            "epoch 537| loss: 65900177920.0| val_0_rmse: 98576.77206|  0:01:06s\n",
            "epoch 538| loss: 49674719232.0| val_0_rmse: 99220.95121|  0:01:06s\n",
            "epoch 539| loss: 55047111424.0| val_0_rmse: 99111.97065|  0:01:06s\n",
            "epoch 540| loss: 53750545408.0| val_0_rmse: 99047.1865|  0:01:06s\n",
            "epoch 541| loss: 53680047872.0| val_0_rmse: 98825.53876|  0:01:06s\n",
            "epoch 542| loss: 57181327360.0| val_0_rmse: 98967.649|  0:01:06s\n",
            "epoch 543| loss: 56273138176.0| val_0_rmse: 99258.30917|  0:01:06s\n",
            "epoch 544| loss: 64619322368.0| val_0_rmse: 99546.81163|  0:01:06s\n",
            "epoch 545| loss: 46768744960.0| val_0_rmse: 99314.68836|  0:01:07s\n",
            "epoch 546| loss: 55998881280.0| val_0_rmse: 99072.339|  0:01:07s\n",
            "epoch 547| loss: 46300450048.0| val_0_rmse: 98740.1939|  0:01:07s\n",
            "epoch 548| loss: 61840306688.0| val_0_rmse: 98701.14889|  0:01:07s\n",
            "epoch 549| loss: 56684059648.0| val_0_rmse: 99067.76681|  0:01:07s\n",
            "epoch 550| loss: 59978390528.0| val_0_rmse: 99460.8802|  0:01:07s\n",
            "epoch 551| loss: 59590622720.0| val_0_rmse: 99652.69938|  0:01:07s\n",
            "epoch 552| loss: 59169363456.0| val_0_rmse: 99269.91097|  0:01:07s\n",
            "epoch 553| loss: 60486204928.0| val_0_rmse: 99007.52342|  0:01:07s\n",
            "epoch 554| loss: 54215592960.0| val_0_rmse: 97317.00762|  0:01:08s\n",
            "epoch 555| loss: 58626963456.0| val_0_rmse: 96378.11546|  0:01:08s\n",
            "epoch 556| loss: 62735508480.0| val_0_rmse: 96593.61803|  0:01:08s\n",
            "epoch 557| loss: 49704588288.0| val_0_rmse: 97117.8424|  0:01:08s\n",
            "epoch 558| loss: 47106447872.0| val_0_rmse: 97112.61959|  0:01:08s\n",
            "epoch 559| loss: 57953270784.0| val_0_rmse: 97163.46231|  0:01:08s\n",
            "epoch 560| loss: 55630446592.0| val_0_rmse: 97778.46999|  0:01:08s\n",
            "epoch 561| loss: 53545841664.0| val_0_rmse: 98172.82672|  0:01:08s\n",
            "epoch 562| loss: 61946353152.0| val_0_rmse: 97968.10988|  0:01:09s\n",
            "epoch 563| loss: 50793525504.0| val_0_rmse: 97651.56089|  0:01:09s\n",
            "epoch 564| loss: 59431642112.0| val_0_rmse: 97724.16703|  0:01:09s\n",
            "epoch 565| loss: 64343130112.0| val_0_rmse: 96923.14076|  0:01:09s\n",
            "epoch 566| loss: 56675639296.0| val_0_rmse: 96772.51567|  0:01:09s\n",
            "epoch 567| loss: 57987961600.0| val_0_rmse: 96837.37127|  0:01:09s\n",
            "epoch 568| loss: 54579117568.0| val_0_rmse: 95915.53556|  0:01:09s\n",
            "epoch 569| loss: 57625219072.0| val_0_rmse: 94316.60464|  0:01:09s\n",
            "epoch 570| loss: 50158727424.0| val_0_rmse: 93635.52831|  0:01:09s\n",
            "epoch 571| loss: 59479675904.0| val_0_rmse: 93434.6513|  0:01:10s\n",
            "epoch 572| loss: 52778840064.0| val_0_rmse: 93436.28329|  0:01:10s\n",
            "epoch 573| loss: 53400647168.0| val_0_rmse: 93776.13929|  0:01:10s\n",
            "epoch 574| loss: 63782788096.0| val_0_rmse: 96320.57351|  0:01:10s\n",
            "epoch 575| loss: 55556277248.0| val_0_rmse: 95141.6053|  0:01:10s\n",
            "epoch 576| loss: 58334135296.0| val_0_rmse: 96942.44358|  0:01:10s\n",
            "epoch 577| loss: 53926305792.0| val_0_rmse: 97881.87413|  0:01:10s\n",
            "epoch 578| loss: 61326308352.0| val_0_rmse: 96729.96077|  0:01:10s\n",
            "epoch 579| loss: 49258124800.0| val_0_rmse: 95288.18799|  0:01:11s\n",
            "epoch 580| loss: 55289440768.0| val_0_rmse: 94842.79144|  0:01:11s\n",
            "epoch 581| loss: 56546976832.0| val_0_rmse: 94683.02984|  0:01:11s\n",
            "epoch 582| loss: 49927709184.0| val_0_rmse: 94362.88779|  0:01:11s\n",
            "epoch 583| loss: 46986163712.0| val_0_rmse: 93628.99885|  0:01:11s\n",
            "epoch 584| loss: 59953039360.0| val_0_rmse: 93678.97972|  0:01:11s\n",
            "epoch 585| loss: 55165840384.0| val_0_rmse: 93807.30429|  0:01:11s\n",
            "epoch 586| loss: 60200212480.0| val_0_rmse: 93889.10873|  0:01:11s\n",
            "epoch 587| loss: 55883351808.0| val_0_rmse: 93329.33065|  0:01:11s\n",
            "epoch 588| loss: 44806349312.0| val_0_rmse: 93267.49635|  0:01:12s\n",
            "epoch 589| loss: 64737483776.0| val_0_rmse: 93131.22044|  0:01:12s\n",
            "epoch 590| loss: 38974204928.0| val_0_rmse: 92609.51812|  0:01:12s\n",
            "epoch 591| loss: 53893596160.0| val_0_rmse: 93137.36478|  0:01:12s\n",
            "epoch 592| loss: 57938055168.0| val_0_rmse: 93520.04661|  0:01:12s\n",
            "epoch 593| loss: 45299407104.0| val_0_rmse: 93421.57606|  0:01:12s\n",
            "epoch 594| loss: 53876952576.0| val_0_rmse: 93646.81035|  0:01:12s\n",
            "epoch 595| loss: 55809728000.0| val_0_rmse: 93603.48507|  0:01:12s\n",
            "epoch 596| loss: 52799968768.0| val_0_rmse: 94522.77291|  0:01:13s\n",
            "epoch 597| loss: 57473661952.0| val_0_rmse: 95381.62085|  0:01:13s\n",
            "epoch 598| loss: 53159212800.0| val_0_rmse: 96032.59841|  0:01:13s\n",
            "epoch 599| loss: 56462532096.0| val_0_rmse: 95352.06417|  0:01:13s\n",
            "epoch 600| loss: 49446108672.0| val_0_rmse: 93249.94581|  0:01:13s\n",
            "epoch 601| loss: 59079638528.0| val_0_rmse: 93840.42224|  0:01:13s\n",
            "epoch 602| loss: 58199104512.0| val_0_rmse: 93627.57415|  0:01:13s\n",
            "epoch 603| loss: 52735021056.0| val_0_rmse: 93981.65651|  0:01:13s\n",
            "epoch 604| loss: 59535430400.0| val_0_rmse: 94021.74627|  0:01:14s\n",
            "epoch 605| loss: 62267456512.0| val_0_rmse: 94056.80258|  0:01:14s\n",
            "epoch 606| loss: 50519587840.0| val_0_rmse: 94148.78257|  0:01:14s\n",
            "epoch 607| loss: 60431686656.0| val_0_rmse: 94530.03897|  0:01:14s\n",
            "epoch 608| loss: 53112672256.0| val_0_rmse: 94034.55656|  0:01:14s\n",
            "epoch 609| loss: 35515785216.0| val_0_rmse: 95030.68892|  0:01:14s\n",
            "epoch 610| loss: 42685983232.0| val_0_rmse: 95686.91199|  0:01:14s\n",
            "epoch 611| loss: 58752536576.0| val_0_rmse: 96253.76146|  0:01:14s\n",
            "epoch 612| loss: 49119645696.0| val_0_rmse: 96105.51132|  0:01:15s\n",
            "epoch 613| loss: 54914941440.0| val_0_rmse: 96171.28869|  0:01:15s\n",
            "epoch 614| loss: 59330127872.0| val_0_rmse: 96401.54946|  0:01:15s\n",
            "epoch 615| loss: 62307389440.0| val_0_rmse: 95408.47267|  0:01:15s\n",
            "epoch 616| loss: 52292133376.0| val_0_rmse: 95120.58724|  0:01:15s\n",
            "epoch 617| loss: 60067534848.0| val_0_rmse: 95389.84328|  0:01:15s\n",
            "epoch 618| loss: 60059499520.0| val_0_rmse: 95782.54735|  0:01:15s\n",
            "epoch 619| loss: 50643443200.0| val_0_rmse: 96290.44841|  0:01:15s\n",
            "epoch 620| loss: 51843518976.0| val_0_rmse: 95576.6445|  0:01:16s\n",
            "epoch 621| loss: 53825418752.0| val_0_rmse: 96084.76103|  0:01:16s\n",
            "epoch 622| loss: 52531036160.0| val_0_rmse: 97394.03393|  0:01:16s\n",
            "epoch 623| loss: 53439263744.0| val_0_rmse: 97644.26198|  0:01:16s\n",
            "epoch 624| loss: 57124616704.0| val_0_rmse: 97124.6786|  0:01:16s\n",
            "epoch 625| loss: 51459055616.0| val_0_rmse: 95385.36676|  0:01:16s\n",
            "epoch 626| loss: 58818225152.0| val_0_rmse: 95114.53338|  0:01:16s\n",
            "epoch 627| loss: 51139293952.0| val_0_rmse: 95796.34959|  0:01:16s\n",
            "epoch 628| loss: 55371728896.0| val_0_rmse: 96548.99506|  0:01:16s\n",
            "epoch 629| loss: 43267168256.0| val_0_rmse: 97413.37113|  0:01:17s\n",
            "epoch 630| loss: 47661416448.0| val_0_rmse: 98227.852|  0:01:17s\n",
            "epoch 631| loss: 57117381632.0| val_0_rmse: 98100.87915|  0:01:17s\n",
            "epoch 632| loss: 51510547456.0| val_0_rmse: 97835.61546|  0:01:17s\n",
            "epoch 633| loss: 59179682816.0| val_0_rmse: 97279.07629|  0:01:17s\n",
            "epoch 634| loss: 59069847552.0| val_0_rmse: 97426.73123|  0:01:17s\n",
            "epoch 635| loss: 57310716160.0| val_0_rmse: 97829.10308|  0:01:17s\n",
            "epoch 636| loss: 60762152960.0| val_0_rmse: 97585.79386|  0:01:17s\n",
            "epoch 637| loss: 59124680448.0| val_0_rmse: 97231.51041|  0:01:18s\n",
            "epoch 638| loss: 47150585856.0| val_0_rmse: 96688.31749|  0:01:18s\n",
            "epoch 639| loss: 55692528128.0| val_0_rmse: 96107.10761|  0:01:18s\n",
            "epoch 640| loss: 61193467904.0| val_0_rmse: 95579.40901|  0:01:18s\n",
            "epoch 641| loss: 53549134848.0| val_0_rmse: 95747.03753|  0:01:18s\n",
            "epoch 642| loss: 60747051008.0| val_0_rmse: 96060.27599|  0:01:18s\n",
            "epoch 643| loss: 49542812672.0| val_0_rmse: 95421.98309|  0:01:18s\n",
            "epoch 644| loss: 49665665536.0| val_0_rmse: 95464.5251|  0:01:18s\n",
            "epoch 645| loss: 55872227840.0| val_0_rmse: 95704.92812|  0:01:18s\n",
            "epoch 646| loss: 43819838464.0| val_0_rmse: 95778.65396|  0:01:19s\n",
            "epoch 647| loss: 54336093184.0| val_0_rmse: 96527.19359|  0:01:19s\n",
            "epoch 648| loss: 52183131904.0| val_0_rmse: 96936.45733|  0:01:19s\n",
            "epoch 649| loss: 50747855360.0| val_0_rmse: 96952.07979|  0:01:19s\n",
            "epoch 650| loss: 58511005184.0| val_0_rmse: 96732.10809|  0:01:19s\n",
            "epoch 651| loss: 56491054080.0| val_0_rmse: 96283.67503|  0:01:19s\n",
            "epoch 652| loss: 62632274432.0| val_0_rmse: 96224.20995|  0:01:19s\n",
            "epoch 653| loss: 53652832256.0| val_0_rmse: 95706.33998|  0:01:19s\n",
            "epoch 654| loss: 48327307264.0| val_0_rmse: 95536.9199|  0:01:20s\n",
            "epoch 655| loss: 59723488512.0| val_0_rmse: 94938.46134|  0:01:20s\n",
            "epoch 656| loss: 57441808384.0| val_0_rmse: 94398.48768|  0:01:20s\n",
            "epoch 657| loss: 49163966464.0| val_0_rmse: 92929.81806|  0:01:20s\n",
            "epoch 658| loss: 38389261824.0| val_0_rmse: 92135.89141|  0:01:20s\n",
            "epoch 659| loss: 57993370112.0| val_0_rmse: 93870.96753|  0:01:20s\n",
            "epoch 660| loss: 57810385920.0| val_0_rmse: 94213.45203|  0:01:20s\n",
            "epoch 661| loss: 50834398208.0| val_0_rmse: 94187.74198|  0:01:21s\n",
            "epoch 662| loss: 51625130496.0| val_0_rmse: 93826.56149|  0:01:21s\n",
            "epoch 663| loss: 53543887872.0| val_0_rmse: 93071.67241|  0:01:21s\n",
            "epoch 664| loss: 45999787008.0| val_0_rmse: 92260.46597|  0:01:21s\n",
            "epoch 665| loss: 44258381824.0| val_0_rmse: 92007.22244|  0:01:21s\n",
            "epoch 666| loss: 51439124992.0| val_0_rmse: 91810.61063|  0:01:21s\n",
            "epoch 667| loss: 54860172800.0| val_0_rmse: 91790.8416|  0:01:22s\n",
            "epoch 668| loss: 45045834240.0| val_0_rmse: 93626.04931|  0:01:22s\n",
            "epoch 669| loss: 53120521216.0| val_0_rmse: 94449.88823|  0:01:22s\n",
            "epoch 670| loss: 51853486080.0| val_0_rmse: 93963.8019|  0:01:22s\n",
            "epoch 671| loss: 55138098688.0| val_0_rmse: 93243.87329|  0:01:22s\n",
            "epoch 672| loss: 43689954560.0| val_0_rmse: 92142.56617|  0:01:22s\n",
            "epoch 673| loss: 60163369728.0| val_0_rmse: 90712.95708|  0:01:22s\n",
            "epoch 674| loss: 53098525696.0| val_0_rmse: 91277.69428|  0:01:22s\n",
            "epoch 675| loss: 46536704768.0| val_0_rmse: 91777.38123|  0:01:22s\n",
            "epoch 676| loss: 52620201472.0| val_0_rmse: 91790.91641|  0:01:23s\n",
            "epoch 677| loss: 55103954944.0| val_0_rmse: 91751.41724|  0:01:23s\n",
            "epoch 678| loss: 43069366784.0| val_0_rmse: 91898.36241|  0:01:23s\n",
            "epoch 679| loss: 57015462336.0| val_0_rmse: 92353.29567|  0:01:23s\n",
            "epoch 680| loss: 59977905152.0| val_0_rmse: 92814.59421|  0:01:23s\n",
            "epoch 681| loss: 53567782400.0| val_0_rmse: 92614.77871|  0:01:23s\n",
            "epoch 682| loss: 58171962368.0| val_0_rmse: 92622.27761|  0:01:23s\n",
            "epoch 683| loss: 56130817536.0| val_0_rmse: 92854.82283|  0:01:23s\n",
            "epoch 684| loss: 50552090624.0| val_0_rmse: 92668.0442|  0:01:24s\n",
            "epoch 685| loss: 56627759104.0| val_0_rmse: 92736.24204|  0:01:24s\n",
            "epoch 686| loss: 55341958912.0| val_0_rmse: 93475.52553|  0:01:24s\n",
            "epoch 687| loss: 47582433280.0| val_0_rmse: 94314.05992|  0:01:24s\n",
            "epoch 688| loss: 38505467392.0| val_0_rmse: 95451.35068|  0:01:24s\n",
            "epoch 689| loss: 59823943680.0| val_0_rmse: 95155.46791|  0:01:24s\n",
            "epoch 690| loss: 46978063360.0| val_0_rmse: 94580.91003|  0:01:24s\n",
            "epoch 691| loss: 59599581184.0| val_0_rmse: 94186.77508|  0:01:24s\n",
            "epoch 692| loss: 55504760832.0| val_0_rmse: 93700.03238|  0:01:25s\n",
            "epoch 693| loss: 44485158400.0| val_0_rmse: 93005.03921|  0:01:25s\n",
            "epoch 694| loss: 60547012096.0| val_0_rmse: 93196.86368|  0:01:25s\n",
            "epoch 695| loss: 49401993728.0| val_0_rmse: 92827.45564|  0:01:25s\n",
            "epoch 696| loss: 53694031360.0| val_0_rmse: 92599.69331|  0:01:25s\n",
            "epoch 697| loss: 57178186240.0| val_0_rmse: 93833.23679|  0:01:25s\n",
            "epoch 698| loss: 50987229696.0| val_0_rmse: 94955.39009|  0:01:25s\n",
            "epoch 699| loss: 49837987328.0| val_0_rmse: 95156.06714|  0:01:25s\n",
            "epoch 700| loss: 58569969664.0| val_0_rmse: 95568.08411|  0:01:26s\n",
            "epoch 701| loss: 54505106944.0| val_0_rmse: 95659.95897|  0:01:26s\n",
            "epoch 702| loss: 58738742784.0| val_0_rmse: 95695.80972|  0:01:26s\n",
            "epoch 703| loss: 54814130176.0| val_0_rmse: 95217.47604|  0:01:26s\n",
            "epoch 704| loss: 56361374208.0| val_0_rmse: 93878.60565|  0:01:26s\n",
            "epoch 705| loss: 53450658816.0| val_0_rmse: 93583.11572|  0:01:26s\n",
            "epoch 706| loss: 50049788416.0| val_0_rmse: 92590.47752|  0:01:26s\n",
            "epoch 707| loss: 58251968512.0| val_0_rmse: 94363.11264|  0:01:26s\n",
            "epoch 708| loss: 50452836864.0| val_0_rmse: 94534.2982|  0:01:27s\n",
            "epoch 709| loss: 54642103808.0| val_0_rmse: 94278.08718|  0:01:27s\n",
            "epoch 710| loss: 46226207744.0| val_0_rmse: 94509.61381|  0:01:27s\n",
            "epoch 711| loss: 41879174656.0| val_0_rmse: 94829.27022|  0:01:27s\n",
            "epoch 712| loss: 50165141504.0| val_0_rmse: 94864.81485|  0:01:27s\n",
            "epoch 713| loss: 54595208960.0| val_0_rmse: 94417.41215|  0:01:27s\n",
            "epoch 714| loss: 45953995264.0| val_0_rmse: 94202.13808|  0:01:27s\n",
            "epoch 715| loss: 57567730240.0| val_0_rmse: 93925.47602|  0:01:27s\n",
            "epoch 716| loss: 49030992896.0| val_0_rmse: 94300.02658|  0:01:27s\n",
            "epoch 717| loss: 50768790528.0| val_0_rmse: 93902.98591|  0:01:28s\n",
            "epoch 718| loss: 51570437120.0| val_0_rmse: 92237.78697|  0:01:28s\n",
            "epoch 719| loss: 58745043968.0| val_0_rmse: 89409.88684|  0:01:28s\n",
            "epoch 720| loss: 54135945216.0| val_0_rmse: 89696.97259|  0:01:28s\n",
            "epoch 721| loss: 47961314176.0| val_0_rmse: 89833.55709|  0:01:28s\n",
            "epoch 722| loss: 52200391168.0| val_0_rmse: 89456.62915|  0:01:28s\n",
            "epoch 723| loss: 55034866176.0| val_0_rmse: 89693.15105|  0:01:28s\n",
            "epoch 724| loss: 47025139712.0| val_0_rmse: 89250.07595|  0:01:28s\n",
            "epoch 725| loss: 44683791872.0| val_0_rmse: 90154.96935|  0:01:29s\n",
            "epoch 726| loss: 55058439168.0| val_0_rmse: 91619.39773|  0:01:29s\n",
            "epoch 727| loss: 42107573888.0| val_0_rmse: 91849.69337|  0:01:29s\n",
            "epoch 728| loss: 58368266240.0| val_0_rmse: 90953.75364|  0:01:29s\n",
            "epoch 729| loss: 35573099520.0| val_0_rmse: 90341.45478|  0:01:29s\n",
            "epoch 730| loss: 40287284992.0| val_0_rmse: 89445.77014|  0:01:29s\n",
            "epoch 731| loss: 45707991040.0| val_0_rmse: 91778.48021|  0:01:29s\n",
            "epoch 732| loss: 58859301376.0| val_0_rmse: 93695.03886|  0:01:29s\n",
            "epoch 733| loss: 49936997376.0| val_0_rmse: 94768.48411|  0:01:29s\n",
            "epoch 734| loss: 48176784384.0| val_0_rmse: 94735.94613|  0:01:30s\n",
            "epoch 735| loss: 44452956160.0| val_0_rmse: 94230.56143|  0:01:30s\n",
            "epoch 736| loss: 56123897344.0| val_0_rmse: 94110.07991|  0:01:30s\n",
            "epoch 737| loss: 54763706880.0| val_0_rmse: 95005.54225|  0:01:30s\n",
            "epoch 738| loss: 47619865600.0| val_0_rmse: 95162.17403|  0:01:30s\n",
            "epoch 739| loss: 57116664320.0| val_0_rmse: 94920.82061|  0:01:30s\n",
            "epoch 740| loss: 44530370560.0| val_0_rmse: 94667.48269|  0:01:30s\n",
            "epoch 741| loss: 43367258880.0| val_0_rmse: 93913.12662|  0:01:31s\n",
            "epoch 742| loss: 47654851584.0| val_0_rmse: 93458.79539|  0:01:31s\n",
            "epoch 743| loss: 48973570048.0| val_0_rmse: 92813.29201|  0:01:31s\n",
            "epoch 744| loss: 51530333184.0| val_0_rmse: 91935.46781|  0:01:31s\n",
            "epoch 745| loss: 55454110720.0| val_0_rmse: 91682.76604|  0:01:31s\n",
            "epoch 746| loss: 43627887360.0| val_0_rmse: 91627.80813|  0:01:31s\n",
            "epoch 747| loss: 49241395200.0| val_0_rmse: 92189.47389|  0:01:31s\n",
            "epoch 748| loss: 41288587008.0| val_0_rmse: 91657.41181|  0:01:31s\n",
            "epoch 749| loss: 51856553984.0| val_0_rmse: 90765.93624|  0:01:31s\n",
            "epoch 750| loss: 55752127488.0| val_0_rmse: 90290.06659|  0:01:32s\n",
            "epoch 751| loss: 50172258304.0| val_0_rmse: 88498.74184|  0:01:32s\n",
            "epoch 752| loss: 53995273728.0| val_0_rmse: 88990.64125|  0:01:32s\n",
            "epoch 753| loss: 51559020544.0| val_0_rmse: 89176.98371|  0:01:32s\n",
            "epoch 754| loss: 50710982656.0| val_0_rmse: 89683.10116|  0:01:32s\n",
            "epoch 755| loss: 46850307072.0| val_0_rmse: 91849.2141|  0:01:32s\n",
            "epoch 756| loss: 53068008960.0| val_0_rmse: 91902.98066|  0:01:32s\n",
            "epoch 757| loss: 36773758720.0| val_0_rmse: 91827.51638|  0:01:32s\n",
            "epoch 758| loss: 55190681088.0| val_0_rmse: 92484.57169|  0:01:33s\n",
            "epoch 759| loss: 43295922944.0| val_0_rmse: 92417.0389|  0:01:33s\n",
            "epoch 760| loss: 49051965952.0| val_0_rmse: 91502.73632|  0:01:33s\n",
            "epoch 761| loss: 57603749376.0| val_0_rmse: 90280.9686|  0:01:33s\n",
            "epoch 762| loss: 56600931840.0| val_0_rmse: 89733.26905|  0:01:33s\n",
            "epoch 763| loss: 46559801344.0| val_0_rmse: 90162.09657|  0:01:33s\n",
            "epoch 764| loss: 50558609408.0| val_0_rmse: 90298.01071|  0:01:33s\n",
            "epoch 765| loss: 42114054016.0| val_0_rmse: 90002.36812|  0:01:33s\n",
            "epoch 766| loss: 41187598336.0| val_0_rmse: 90023.84636|  0:01:33s\n",
            "epoch 767| loss: 56145274368.0| val_0_rmse: 90969.23803|  0:01:34s\n",
            "epoch 768| loss: 49538994688.0| val_0_rmse: 92700.9136|  0:01:34s\n",
            "epoch 769| loss: 51606184960.0| val_0_rmse: 93924.67454|  0:01:34s\n",
            "epoch 770| loss: 44768691712.0| val_0_rmse: 95723.29193|  0:01:34s\n",
            "epoch 771| loss: 56353887232.0| val_0_rmse: 96680.73064|  0:01:34s\n",
            "epoch 772| loss: 50594321152.0| val_0_rmse: 96046.55541|  0:01:34s\n",
            "epoch 773| loss: 45275907072.0| val_0_rmse: 94362.7098|  0:01:34s\n",
            "epoch 774| loss: 44432663552.0| val_0_rmse: 93917.01649|  0:01:35s\n",
            "epoch 775| loss: 38318382592.0| val_0_rmse: 93976.26504|  0:01:35s\n",
            "epoch 776| loss: 39541779776.0| val_0_rmse: 93962.32534|  0:01:35s\n",
            "epoch 777| loss: 44842872832.0| val_0_rmse: 91664.41474|  0:01:35s\n",
            "epoch 778| loss: 48182800384.0| val_0_rmse: 91483.37223|  0:01:35s\n",
            "epoch 779| loss: 43505028032.0| val_0_rmse: 91684.35468|  0:01:35s\n",
            "epoch 780| loss: 52656757760.0| val_0_rmse: 91184.25162|  0:01:35s\n",
            "epoch 781| loss: 54758441216.0| val_0_rmse: 90463.09661|  0:01:35s\n",
            "epoch 782| loss: 52487665664.0| val_0_rmse: 90969.87706|  0:01:35s\n",
            "epoch 783| loss: 56884429824.0| val_0_rmse: 92742.61598|  0:01:36s\n",
            "epoch 784| loss: 47743357440.0| val_0_rmse: 93870.74964|  0:01:36s\n",
            "epoch 785| loss: 51566821376.0| val_0_rmse: 94551.31883|  0:01:36s\n",
            "epoch 786| loss: 43787270656.0| val_0_rmse: 93890.77647|  0:01:36s\n",
            "epoch 787| loss: 52600341504.0| val_0_rmse: 93327.15858|  0:01:36s\n",
            "epoch 788| loss: 54458842624.0| val_0_rmse: 92694.80539|  0:01:36s\n",
            "epoch 789| loss: 46205584384.0| val_0_rmse: 90744.5546|  0:01:36s\n",
            "epoch 790| loss: 46528468480.0| val_0_rmse: 89809.91403|  0:01:36s\n",
            "epoch 791| loss: 56018421760.0| val_0_rmse: 90445.55814|  0:01:36s\n",
            "epoch 792| loss: 51226580480.0| val_0_rmse: 91467.77105|  0:01:37s\n",
            "epoch 793| loss: 50117257728.0| val_0_rmse: 91470.72833|  0:01:37s\n",
            "epoch 794| loss: 53144577536.0| val_0_rmse: 91474.32185|  0:01:37s\n",
            "epoch 795| loss: 48615820032.0| val_0_rmse: 90950.93953|  0:01:37s\n",
            "epoch 796| loss: 51694782976.0| val_0_rmse: 89256.08594|  0:01:37s\n",
            "epoch 797| loss: 54812283392.0| val_0_rmse: 88265.77133|  0:01:37s\n",
            "epoch 798| loss: 44134139648.0| val_0_rmse: 88608.65979|  0:01:37s\n",
            "epoch 799| loss: 51173183232.0| val_0_rmse: 89348.27672|  0:01:37s\n",
            "epoch 800| loss: 50205308672.0| val_0_rmse: 89664.43095|  0:01:38s\n",
            "epoch 801| loss: 52031273216.0| val_0_rmse: 89042.2376|  0:01:38s\n",
            "epoch 802| loss: 52798788096.0| val_0_rmse: 88846.4368|  0:01:38s\n",
            "epoch 803| loss: 39357039872.0| val_0_rmse: 89312.92885|  0:01:38s\n",
            "epoch 804| loss: 48153255424.0| val_0_rmse: 89463.19387|  0:01:38s\n",
            "epoch 805| loss: 51570658304.0| val_0_rmse: 88934.3752|  0:01:38s\n",
            "epoch 806| loss: 45643671040.0| val_0_rmse: 88396.15611|  0:01:38s\n",
            "epoch 807| loss: 44697778688.0| val_0_rmse: 88661.24097|  0:01:38s\n",
            "epoch 808| loss: 35239520256.0| val_0_rmse: 88472.95413|  0:01:39s\n",
            "epoch 809| loss: 44695332864.0| val_0_rmse: 89699.44512|  0:01:39s\n",
            "epoch 810| loss: 46089983744.0| val_0_rmse: 90160.00289|  0:01:39s\n",
            "epoch 811| loss: 43574755712.0| val_0_rmse: 90455.90665|  0:01:39s\n",
            "epoch 812| loss: 47646564864.0| val_0_rmse: 89414.32617|  0:01:39s\n",
            "epoch 813| loss: 47804666880.0| val_0_rmse: 88163.40784|  0:01:39s\n",
            "epoch 814| loss: 47495000064.0| val_0_rmse: 87402.89557|  0:01:39s\n",
            "epoch 815| loss: 52370340352.0| val_0_rmse: 87963.4711|  0:01:39s\n",
            "epoch 816| loss: 44388656384.0| val_0_rmse: 88440.79871|  0:01:40s\n",
            "epoch 817| loss: 55784925184.0| val_0_rmse: 89107.16954|  0:01:40s\n",
            "epoch 818| loss: 38554735616.0| val_0_rmse: 89680.4585|  0:01:40s\n",
            "epoch 819| loss: 55776446976.0| val_0_rmse: 89672.92599|  0:01:40s\n",
            "epoch 820| loss: 36556303872.0| val_0_rmse: 89070.61724|  0:01:40s\n",
            "epoch 821| loss: 51008406528.0| val_0_rmse: 88962.1133|  0:01:40s\n",
            "epoch 822| loss: 55012491264.0| val_0_rmse: 88276.37212|  0:01:40s\n",
            "epoch 823| loss: 51116338944.0| val_0_rmse: 88100.50114|  0:01:40s\n",
            "epoch 824| loss: 40662904320.0| val_0_rmse: 88687.02026|  0:01:40s\n",
            "epoch 825| loss: 50397086720.0| val_0_rmse: 88759.21572|  0:01:41s\n",
            "epoch 826| loss: 46388443904.0| val_0_rmse: 88727.27867|  0:01:41s\n",
            "epoch 827| loss: 48887888896.0| val_0_rmse: 88766.21142|  0:01:41s\n",
            "epoch 828| loss: 51028591872.0| val_0_rmse: 89870.93453|  0:01:41s\n",
            "epoch 829| loss: 50156579840.0| val_0_rmse: 90049.45091|  0:01:41s\n",
            "epoch 830| loss: 36334604288.0| val_0_rmse: 88715.28589|  0:01:41s\n",
            "epoch 831| loss: 40778495872.0| val_0_rmse: 88843.72147|  0:01:41s\n",
            "epoch 832| loss: 44685541888.0| val_0_rmse: 89041.69831|  0:01:41s\n",
            "epoch 833| loss: 39156918784.0| val_0_rmse: 89524.13138|  0:01:42s\n",
            "epoch 834| loss: 50268751104.0| val_0_rmse: 91041.58009|  0:01:42s\n",
            "epoch 835| loss: 51510406656.0| val_0_rmse: 91482.79695|  0:01:42s\n",
            "epoch 836| loss: 52555752448.0| val_0_rmse: 92874.87621|  0:01:42s\n",
            "epoch 837| loss: 32297123328.0| val_0_rmse: 92676.75691|  0:01:42s\n",
            "epoch 838| loss: 46938876928.0| val_0_rmse: 92354.4786|  0:01:42s\n",
            "epoch 839| loss: 53607394816.0| val_0_rmse: 90862.19188|  0:01:42s\n",
            "epoch 840| loss: 48048973440.0| val_0_rmse: 89484.73865|  0:01:42s\n",
            "epoch 841| loss: 44719470592.0| val_0_rmse: 88810.59895|  0:01:43s\n",
            "epoch 842| loss: 43861493248.0| val_0_rmse: 87724.58401|  0:01:43s\n",
            "epoch 843| loss: 51192790528.0| val_0_rmse: 87302.22526|  0:01:43s\n",
            "epoch 844| loss: 51900652544.0| val_0_rmse: 87155.13505|  0:01:43s\n",
            "epoch 845| loss: 45688988672.0| val_0_rmse: 86467.87394|  0:01:43s\n",
            "epoch 846| loss: 50537997824.0| val_0_rmse: 86357.86296|  0:01:43s\n",
            "epoch 847| loss: 42890799488.0| val_0_rmse: 85985.32505|  0:01:43s\n",
            "epoch 848| loss: 46124064768.0| val_0_rmse: 85408.53958|  0:01:43s\n",
            "epoch 849| loss: 52242434560.0| val_0_rmse: 86075.80481|  0:01:43s\n",
            "epoch 850| loss: 48531992064.0| val_0_rmse: 86507.07855|  0:01:44s\n",
            "epoch 851| loss: 40309042688.0| val_0_rmse: 87654.40167|  0:01:44s\n",
            "epoch 852| loss: 50521574400.0| val_0_rmse: 88485.85863|  0:01:44s\n",
            "epoch 853| loss: 41438089728.0| val_0_rmse: 89405.32163|  0:01:44s\n",
            "epoch 854| loss: 49351754752.0| val_0_rmse: 90420.05681|  0:01:44s\n",
            "epoch 855| loss: 52227685376.0| val_0_rmse: 91061.78837|  0:01:44s\n",
            "epoch 856| loss: 40028260864.0| val_0_rmse: 92068.7818|  0:01:44s\n",
            "epoch 857| loss: 52136931840.0| val_0_rmse: 92570.12529|  0:01:44s\n",
            "epoch 858| loss: 51446960640.0| val_0_rmse: 92291.67418|  0:01:45s\n",
            "epoch 859| loss: 48764935936.0| val_0_rmse: 92252.27339|  0:01:45s\n",
            "epoch 860| loss: 42425744384.0| val_0_rmse: 90826.47915|  0:01:45s\n",
            "epoch 861| loss: 44986542848.0| val_0_rmse: 88853.33393|  0:01:45s\n",
            "epoch 862| loss: 48273949952.0| val_0_rmse: 89138.24024|  0:01:45s\n",
            "epoch 863| loss: 53096413440.0| val_0_rmse: 91366.23644|  0:01:45s\n",
            "epoch 864| loss: 52929321984.0| val_0_rmse: 91770.42135|  0:01:45s\n",
            "epoch 865| loss: 48169818624.0| val_0_rmse: 91108.96822|  0:01:45s\n",
            "epoch 866| loss: 44889180672.0| val_0_rmse: 91395.51303|  0:01:45s\n",
            "epoch 867| loss: 44882514944.0| val_0_rmse: 91265.61707|  0:01:46s\n",
            "epoch 868| loss: 51706419968.0| val_0_rmse: 89537.4495|  0:01:46s\n",
            "epoch 869| loss: 48374850048.0| val_0_rmse: 87963.04342|  0:01:46s\n",
            "epoch 870| loss: 51764608512.0| val_0_rmse: 86477.96586|  0:01:46s\n",
            "epoch 871| loss: 39433629696.0| val_0_rmse: 85493.37202|  0:01:46s\n",
            "epoch 872| loss: 44003372288.0| val_0_rmse: 85535.75993|  0:01:46s\n",
            "epoch 873| loss: 35158121984.0| val_0_rmse: 86198.23969|  0:01:46s\n",
            "epoch 874| loss: 42520927616.0| val_0_rmse: 86868.28929|  0:01:46s\n",
            "epoch 875| loss: 50201801728.0| val_0_rmse: 87333.96339|  0:01:47s\n",
            "epoch 876| loss: 44677784576.0| val_0_rmse: 88397.98944|  0:01:47s\n",
            "epoch 877| loss: 44514105344.0| val_0_rmse: 88786.95686|  0:01:47s\n",
            "epoch 878| loss: 54889761280.0| val_0_rmse: 88372.69416|  0:01:47s\n",
            "epoch 879| loss: 46923246848.0| val_0_rmse: 89693.44274|  0:01:47s\n",
            "epoch 880| loss: 31544777728.0| val_0_rmse: 89182.17306|  0:01:47s\n",
            "epoch 881| loss: 45344524672.0| val_0_rmse: 91214.62077|  0:01:47s\n",
            "epoch 882| loss: 41517632512.0| val_0_rmse: 90872.67336|  0:01:47s\n",
            "epoch 883| loss: 51910822912.0| val_0_rmse: 91442.23929|  0:01:47s\n",
            "epoch 884| loss: 47194241024.0| val_0_rmse: 92140.72287|  0:01:48s\n",
            "epoch 885| loss: 48899656448.0| val_0_rmse: 92971.27738|  0:01:48s\n",
            "epoch 886| loss: 46991178496.0| val_0_rmse: 93849.30177|  0:01:48s\n",
            "epoch 887| loss: 46228712960.0| val_0_rmse: 93828.90341|  0:01:48s\n",
            "epoch 888| loss: 40743139328.0| val_0_rmse: 93310.36685|  0:01:48s\n",
            "epoch 889| loss: 51036107776.0| val_0_rmse: 93211.18001|  0:01:48s\n",
            "epoch 890| loss: 45115404288.0| val_0_rmse: 93207.35446|  0:01:48s\n",
            "epoch 891| loss: 53693170944.0| val_0_rmse: 92408.00871|  0:01:48s\n",
            "epoch 892| loss: 44596869120.0| val_0_rmse: 91897.05767|  0:01:48s\n",
            "epoch 893| loss: 46785721344.0| val_0_rmse: 91758.55842|  0:01:49s\n",
            "epoch 894| loss: 38946581504.0| val_0_rmse: 89354.18879|  0:01:49s\n",
            "epoch 895| loss: 50329475584.0| val_0_rmse: 89895.9504|  0:01:49s\n",
            "epoch 896| loss: 50769426432.0| val_0_rmse: 90495.50116|  0:01:49s\n",
            "epoch 897| loss: 47412944896.0| val_0_rmse: 90888.78745|  0:01:49s\n",
            "epoch 898| loss: 52651872256.0| val_0_rmse: 91045.29047|  0:01:49s\n",
            "epoch 899| loss: 48477833216.0| val_0_rmse: 91653.30171|  0:01:49s\n",
            "epoch 900| loss: 45515585536.0| val_0_rmse: 92470.9302|  0:01:49s\n",
            "epoch 901| loss: 46903762432.0| val_0_rmse: 93162.42114|  0:01:50s\n",
            "epoch 902| loss: 50040088576.0| val_0_rmse: 92858.25618|  0:01:50s\n",
            "epoch 903| loss: 47044775424.0| val_0_rmse: 91270.81674|  0:01:50s\n",
            "epoch 904| loss: 47687523584.0| val_0_rmse: 90454.0781|  0:01:50s\n",
            "epoch 905| loss: 41853132544.0| val_0_rmse: 90217.48854|  0:01:50s\n",
            "epoch 906| loss: 45950157312.0| val_0_rmse: 92526.19216|  0:01:50s\n",
            "epoch 907| loss: 52109098496.0| val_0_rmse: 92971.10339|  0:01:50s\n",
            "epoch 908| loss: 36413582848.0| val_0_rmse: 92361.53197|  0:01:50s\n",
            "epoch 909| loss: 39389034496.0| val_0_rmse: 91904.5561|  0:01:51s\n",
            "epoch 910| loss: 35693101056.0| val_0_rmse: 91610.99406|  0:01:51s\n",
            "epoch 911| loss: 40677099008.0| val_0_rmse: 91302.40809|  0:01:51s\n",
            "epoch 912| loss: 36091259648.0| val_0_rmse: 90131.58628|  0:01:51s\n",
            "epoch 913| loss: 51325954048.0| val_0_rmse: 89571.83515|  0:01:51s\n",
            "epoch 914| loss: 41279739648.0| val_0_rmse: 88939.73671|  0:01:51s\n",
            "epoch 915| loss: 39544333312.0| val_0_rmse: 89517.20001|  0:01:51s\n",
            "epoch 916| loss: 41550775040.0| val_0_rmse: 89444.67432|  0:01:51s\n",
            "epoch 917| loss: 47331865600.0| val_0_rmse: 90928.09271|  0:01:52s\n",
            "epoch 918| loss: 45686591488.0| val_0_rmse: 89819.48497|  0:01:52s\n",
            "epoch 919| loss: 48847805440.0| val_0_rmse: 90236.54196|  0:01:52s\n",
            "epoch 920| loss: 47820262400.0| val_0_rmse: 91883.3145|  0:01:52s\n",
            "epoch 921| loss: 46996303360.0| val_0_rmse: 92076.71051|  0:01:52s\n",
            "epoch 922| loss: 49320195840.0| val_0_rmse: 92493.05714|  0:01:52s\n",
            "epoch 923| loss: 53085543936.0| val_0_rmse: 92949.19617|  0:01:52s\n",
            "epoch 924| loss: 43028063488.0| val_0_rmse: 93268.3173|  0:01:53s\n",
            "epoch 925| loss: 42634063872.0| val_0_rmse: 93742.69198|  0:01:53s\n",
            "epoch 926| loss: 40659474944.0| val_0_rmse: 93394.32603|  0:01:53s\n",
            "epoch 927| loss: 43425242880.0| val_0_rmse: 93517.71977|  0:01:53s\n",
            "epoch 928| loss: 48111278848.0| val_0_rmse: 92605.21439|  0:01:53s\n",
            "epoch 929| loss: 36433387520.0| val_0_rmse: 91171.20593|  0:01:53s\n",
            "epoch 930| loss: 44072791296.0| val_0_rmse: 92145.32117|  0:01:53s\n",
            "epoch 931| loss: 49649697536.0| val_0_rmse: 92331.33454|  0:01:53s\n",
            "epoch 932| loss: 46729761792.0| val_0_rmse: 92728.36625|  0:01:53s\n",
            "epoch 933| loss: 44844013056.0| val_0_rmse: 90776.80949|  0:01:54s\n",
            "epoch 934| loss: 51216386304.0| val_0_rmse: 88703.29088|  0:01:54s\n",
            "epoch 935| loss: 43606795264.0| val_0_rmse: 87355.56078|  0:01:54s\n",
            "epoch 936| loss: 39395108864.0| val_0_rmse: 87717.67923|  0:01:54s\n",
            "epoch 937| loss: 46049333248.0| val_0_rmse: 87664.46631|  0:01:54s\n",
            "epoch 938| loss: 50998736896.0| val_0_rmse: 87381.28596|  0:01:54s\n",
            "epoch 939| loss: 45000281088.0| val_0_rmse: 87487.44453|  0:01:54s\n",
            "epoch 940| loss: 49806549760.0| val_0_rmse: 87939.73017|  0:01:54s\n",
            "epoch 941| loss: 42881108992.0| val_0_rmse: 89377.44496|  0:01:54s\n",
            "epoch 942| loss: 39099440128.0| val_0_rmse: 90103.52879|  0:01:55s\n",
            "epoch 943| loss: 45717002112.0| val_0_rmse: 90676.80627|  0:01:55s\n",
            "epoch 944| loss: 37633963520.0| val_0_rmse: 91482.92199|  0:01:55s\n",
            "epoch 945| loss: 41818815488.0| val_0_rmse: 92347.45137|  0:01:55s\n",
            "epoch 946| loss: 45207001600.0| val_0_rmse: 89770.12419|  0:01:55s\n",
            "epoch 947| loss: 42598762496.0| val_0_rmse: 89268.37815|  0:01:55s\n",
            "epoch 948| loss: 40437689600.0| val_0_rmse: 90812.02508|  0:01:55s\n",
            "\n",
            "Early stopping occurred at epoch 948 with best_epoch = 848 and best_val_0_rmse = 85408.53958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=1000,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90_YoFpRIWf5",
        "outputId": "2f407c15-b592-40ef-a7fc-f2f3dcde0a96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 51982.95,\n",
              " 'mse': 7294618633.08,\n",
              " 'rmse': 85408.54,\n",
              " 'mae_upperbound_tolerance': -34091.39,\n",
              " 'rmse_upperbound_tolerance': -57241.09,\n",
              " 'mse_upperbound_tolerance': -4649934043.64}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-t3sLTz2_l"
      },
      "source": [
        "### 3.2 TabTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzb7bu_Nz6CD",
        "outputId": "1219a11e-d901-456c-846d-5aeaca20ebe0"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "num_features = X_train_scaled.shape[1]\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = TabTransformer(num_features).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.MSELoss()  # Use MSELoss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=100, gamma=0.001)\n",
        "\n",
        "# Converting data to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)  # Ensure target is of shape [batch_size, 1]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor)\n",
        "    loss = criterion(output, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV-ur7Y50EHz",
        "outputId": "a7e803ce-9d68-44b4-f54f-058a0c4b9cff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 59625.35,\n",
              " 'mse': 12370708007.22,\n",
              " 'rmse': 111223.68,\n",
              " 'mae_upperbound_tolerance': -41733.79,\n",
              " 'rmse_upperbound_tolerance': -83056.23,\n",
              " 'mse_upperbound_tolerance': -9726023417.77}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation using evaluate function\n",
        "y_test_tensor = y_test.values  # Ensure y_test is in NumPy array format\n",
        "\n",
        "# Call the evaluate function with the predict method we defined\n",
        "eval_values = evaluate(\n",
        "    model=model, \n",
        "    X_test=X_test_scaled, \n",
        "    y_test=y_test_tensor, \n",
        "    threshold=0.3,  # Adjust as needed\n",
        "    mode=\"regression\"\n",
        ")\n",
        "\n",
        "evaluate_dict[\"TabTransformer\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkz7qc00cAj"
      },
      "source": [
        "### 3.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCDkn4of0dk0",
        "outputId": "6bb1e433-300d-4084-e58f-6ea2d56d8e38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10\n",
        "X_train_seq, y_train_seq = create_dataset(X_train_scaled, y_train, time_steps)\n",
        "X_test_seq, y_test_seq = create_dataset(X_test_scaled, y_test, time_steps)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    # Layer 1\n",
        "    LSTM(128, activation='relu', input_shape=(time_steps, X_train.shape[1]), \n",
        "         return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 2\n",
        "    LSTM(64, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 3\n",
        "    LSTM(32, activation='relu', return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Layer 4\n",
        "    LSTM(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Dense layers\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Dense(8, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile mô hình với learning rate schedule\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=100,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_seq, y_test_seq),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87GTF3Wp0mIz",
        "outputId": "e09dea23-8e0e-4712-fdc0-333b5381e1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 54755.88,\n",
              " 'mse': 11340180368.68,\n",
              " 'rmse': 106490.28,\n",
              " 'mae_upperbound_tolerance': -38323.33,\n",
              " 'rmse_upperbound_tolerance': -79089.06,\n",
              " 'mse_upperbound_tolerance': -8837423844.39}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_seq, y_test_seq, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "kfyHYmdP0sUr",
        "outputId": "843ee88f-94fa-4d58-c242-56d1f86fcc7d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_24cfa_row0_col1, #T_24cfa_row0_col2, #T_24cfa_row0_col3, #T_24cfa_row0_col4, #T_24cfa_row0_col5, #T_24cfa_row0_col6 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_24cfa\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_24cfa_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_24cfa_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_24cfa_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_24cfa_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_24cfa_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_24cfa_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_24cfa_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_24cfa_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_24cfa_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_24cfa_row0_col1\" class=\"data row0 col1\" >51982.95</td>\n",
              "      <td id=\"T_24cfa_row0_col2\" class=\"data row0 col2\" >7294618633.08</td>\n",
              "      <td id=\"T_24cfa_row0_col3\" class=\"data row0 col3\" >85408.54</td>\n",
              "      <td id=\"T_24cfa_row0_col4\" class=\"data row0 col4\" >-34091.39</td>\n",
              "      <td id=\"T_24cfa_row0_col5\" class=\"data row0 col5\" >-57241.09</td>\n",
              "      <td id=\"T_24cfa_row0_col6\" class=\"data row0 col6\" >-4649934043.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_24cfa_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_24cfa_row1_col0\" class=\"data row1 col0\" >TabTransformer</td>\n",
              "      <td id=\"T_24cfa_row1_col1\" class=\"data row1 col1\" >59625.35</td>\n",
              "      <td id=\"T_24cfa_row1_col2\" class=\"data row1 col2\" >12370708007.22</td>\n",
              "      <td id=\"T_24cfa_row1_col3\" class=\"data row1 col3\" >111223.68</td>\n",
              "      <td id=\"T_24cfa_row1_col4\" class=\"data row1 col4\" >-41733.79</td>\n",
              "      <td id=\"T_24cfa_row1_col5\" class=\"data row1 col5\" >-83056.23</td>\n",
              "      <td id=\"T_24cfa_row1_col6\" class=\"data row1 col6\" >-9726023417.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_24cfa_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_24cfa_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_24cfa_row2_col1\" class=\"data row2 col1\" >54755.88</td>\n",
              "      <td id=\"T_24cfa_row2_col2\" class=\"data row2 col2\" >11340180368.68</td>\n",
              "      <td id=\"T_24cfa_row2_col3\" class=\"data row2 col3\" >106490.28</td>\n",
              "      <td id=\"T_24cfa_row2_col4\" class=\"data row2 col4\" >-38323.33</td>\n",
              "      <td id=\"T_24cfa_row2_col5\" class=\"data row2 col5\" >-79089.06</td>\n",
              "      <td id=\"T_24cfa_row2_col6\" class=\"data row2 col6\" >-8837423844.39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1dfa5d09d90>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Fezxju1LBT"
      },
      "source": [
        "TabNet hoạt động tốt với target 1, target 2 và 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
