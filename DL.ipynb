{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "CEPK3_hihBRf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('STORM_preprocessed_medianfill_1.csv', index_col=0)"
      ],
      "metadata": {
        "id": "ppt-nRO6iYj-"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target 1 : TotalDeaths**"
      ],
      "metadata": {
        "id": "kuFr44h5jMpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "\n",
        "X = df[ATTRIBUTES]\n",
        "y = df[LINEAR_TARGETS[0]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "zZBHiCmei0l2"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TabNet**"
      ],
      "metadata": {
        "id": "Egd9rns0m0Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW12phLun-XY",
        "outputId": "e738e974-95b6-42e4-9861-656ff734f5e1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# Initialize TabNet Regressor\n",
        "reg = TabNetRegressor()\n",
        "\n",
        "# Train the model\n",
        "reg.fit(\n",
        "    X_train.values, y_train.values.reshape(-1, 1),\n",
        "    eval_set=[(X_test.values, y_test.values.reshape(-1, 1))],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4puM7fcnHwn",
        "outputId": "c6c255e4-46fc-43e9-a5ef-59bf63b41c09"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 275648.84229| val_0_rmse: 597.82883|  0:00:00s\n",
            "epoch 1  | loss: 274991.08685| val_0_rmse: 603.84134|  0:00:00s\n",
            "epoch 2  | loss: 269967.74829| val_0_rmse: 769.86386|  0:00:00s\n",
            "epoch 3  | loss: 273765.76123| val_0_rmse: 658.80863|  0:00:00s\n",
            "epoch 4  | loss: 267876.98798| val_0_rmse: 624.7017|  0:00:00s\n",
            "epoch 5  | loss: 271841.46069| val_0_rmse: 608.09086|  0:00:00s\n",
            "epoch 6  | loss: 270977.59723| val_0_rmse: 611.33247|  0:00:00s\n",
            "epoch 7  | loss: 60917.63281| val_0_rmse: 610.20857|  0:00:00s\n",
            "epoch 8  | loss: 269494.67648| val_0_rmse: 612.50961|  0:00:00s\n",
            "epoch 9  | loss: 268754.3208| val_0_rmse: 604.35402|  0:00:00s\n",
            "epoch 10 | loss: 59609.57074| val_0_rmse: 601.77483|  0:00:01s\n",
            "epoch 11 | loss: 262947.07831| val_0_rmse: 590.98906|  0:00:01s\n",
            "epoch 12 | loss: 258476.49295| val_0_rmse: 600.46178|  0:00:01s\n",
            "epoch 13 | loss: 262432.14478| val_0_rmse: 595.19218|  0:00:01s\n",
            "epoch 14 | loss: 256387.64868| val_0_rmse: 595.27197|  0:00:01s\n",
            "epoch 15 | loss: 263034.09619| val_0_rmse: 582.86642|  0:00:01s\n",
            "epoch 16 | loss: 257515.11618| val_0_rmse: 584.10055|  0:00:01s\n",
            "epoch 17 | loss: 261895.81714| val_0_rmse: 584.80592|  0:00:01s\n",
            "epoch 18 | loss: 260575.87463| val_0_rmse: 609.98898|  0:00:01s\n",
            "epoch 19 | loss: 257436.56433| val_0_rmse: 605.8218|  0:00:02s\n",
            "epoch 20 | loss: 56520.31592| val_0_rmse: 584.30812|  0:00:02s\n",
            "epoch 21 | loss: 248637.21188| val_0_rmse: 597.67462|  0:00:02s\n",
            "epoch 22 | loss: 240870.54514| val_0_rmse: 596.77181|  0:00:02s\n",
            "epoch 23 | loss: 255051.1731| val_0_rmse: 592.67769|  0:00:02s\n",
            "epoch 24 | loss: 54438.16669| val_0_rmse: 595.99631|  0:00:02s\n",
            "epoch 25 | loss: 241583.40942| val_0_rmse: 583.45395|  0:00:02s\n",
            "epoch 26 | loss: 241710.11768| val_0_rmse: 598.29227|  0:00:02s\n",
            "epoch 27 | loss: 199127.88574| val_0_rmse: 600.43877|  0:00:03s\n",
            "epoch 28 | loss: 59289.9082| val_0_rmse: 598.22725|  0:00:03s\n",
            "epoch 29 | loss: 242275.448| val_0_rmse: 581.34134|  0:00:03s\n",
            "epoch 30 | loss: 237243.46826| val_0_rmse: 581.48601|  0:00:03s\n",
            "epoch 31 | loss: 236819.13464| val_0_rmse: 580.87535|  0:00:03s\n",
            "epoch 32 | loss: 224799.25781| val_0_rmse: 581.4163|  0:00:03s\n",
            "epoch 33 | loss: 231082.60571| val_0_rmse: 583.42642|  0:00:03s\n",
            "epoch 34 | loss: 230207.58728| val_0_rmse: 585.06367|  0:00:04s\n",
            "epoch 35 | loss: 52096.30664| val_0_rmse: 585.71478|  0:00:04s\n",
            "epoch 36 | loss: 228703.28088| val_0_rmse: 586.44372|  0:00:04s\n",
            "epoch 37 | loss: 54177.01575| val_0_rmse: 587.81288|  0:00:04s\n",
            "epoch 38 | loss: 50781.63062| val_0_rmse: 589.47001|  0:00:04s\n",
            "epoch 39 | loss: 250555.80786| val_0_rmse: 589.93911|  0:00:04s\n",
            "epoch 40 | loss: 229566.78174| val_0_rmse: 590.48533|  0:00:04s\n",
            "epoch 41 | loss: 223983.94534| val_0_rmse: 590.86086|  0:00:05s\n",
            "epoch 42 | loss: 220313.90857| val_0_rmse: 589.79696|  0:00:05s\n",
            "epoch 43 | loss: 183250.54559| val_0_rmse: 595.30851|  0:00:05s\n",
            "epoch 44 | loss: 220031.78296| val_0_rmse: 601.31481|  0:00:05s\n",
            "epoch 45 | loss: 211051.31372| val_0_rmse: 603.37275|  0:00:05s\n",
            "epoch 46 | loss: 216981.77539| val_0_rmse: 605.97816|  0:00:05s\n",
            "epoch 47 | loss: 214459.59613| val_0_rmse: 590.28877|  0:00:05s\n",
            "epoch 48 | loss: 220596.59705| val_0_rmse: 592.83434|  0:00:05s\n",
            "epoch 49 | loss: 201379.30701| val_0_rmse: 596.00889|  0:00:05s\n",
            "epoch 50 | loss: 206214.37451| val_0_rmse: 596.27345|  0:00:05s\n",
            "epoch 51 | loss: 200698.56812| val_0_rmse: 596.3015|  0:00:05s\n",
            "epoch 52 | loss: 205029.84131| val_0_rmse: 600.63322|  0:00:06s\n",
            "epoch 53 | loss: 209670.79407| val_0_rmse: 619.4145|  0:00:06s\n",
            "epoch 54 | loss: 188675.49915| val_0_rmse: 623.61797|  0:00:06s\n",
            "epoch 55 | loss: 196758.77686| val_0_rmse: 622.81719|  0:00:06s\n",
            "epoch 56 | loss: 178070.53979| val_0_rmse: 620.27555|  0:00:06s\n",
            "epoch 57 | loss: 207646.68213| val_0_rmse: 615.57958|  0:00:06s\n",
            "epoch 58 | loss: 165129.59009| val_0_rmse: 598.51146|  0:00:06s\n",
            "epoch 59 | loss: 161530.6333| val_0_rmse: 599.06374|  0:00:06s\n",
            "epoch 60 | loss: 231760.9353| val_0_rmse: 599.73678|  0:00:06s\n",
            "epoch 61 | loss: 185153.42358| val_0_rmse: 598.31146|  0:00:06s\n",
            "epoch 62 | loss: 180687.25952| val_0_rmse: 599.52666|  0:00:06s\n",
            "epoch 63 | loss: 176426.45557| val_0_rmse: 606.57029|  0:00:07s\n",
            "epoch 64 | loss: 40126.12451| val_0_rmse: 613.09078|  0:00:07s\n",
            "epoch 65 | loss: 13235.03192| val_0_rmse: 610.83794|  0:00:07s\n",
            "epoch 66 | loss: 41649.46558| val_0_rmse: 601.84805|  0:00:07s\n",
            "epoch 67 | loss: 199713.68359| val_0_rmse: 592.23926|  0:00:07s\n",
            "epoch 68 | loss: 194084.24048| val_0_rmse: 602.38972|  0:00:07s\n",
            "epoch 69 | loss: 180791.43799| val_0_rmse: 586.24262|  0:00:07s\n",
            "epoch 70 | loss: 48436.02734| val_0_rmse: 562.67289|  0:00:07s\n",
            "epoch 71 | loss: 198702.39233| val_0_rmse: 582.40042|  0:00:07s\n",
            "epoch 72 | loss: 171392.58209| val_0_rmse: 557.31801|  0:00:07s\n",
            "epoch 73 | loss: 225797.21558| val_0_rmse: 549.6993|  0:00:07s\n",
            "epoch 74 | loss: 62525.97339| val_0_rmse: 544.90243|  0:00:08s\n",
            "epoch 75 | loss: 190019.5813| val_0_rmse: 555.00225|  0:00:08s\n",
            "epoch 76 | loss: 51227.25342| val_0_rmse: 577.79703|  0:00:08s\n",
            "epoch 77 | loss: 225449.85327| val_0_rmse: 587.06833|  0:00:08s\n",
            "epoch 78 | loss: 217289.68127| val_0_rmse: 591.1363|  0:00:08s\n",
            "epoch 79 | loss: 183998.51379| val_0_rmse: 588.47504|  0:00:08s\n",
            "epoch 80 | loss: 228951.88965| val_0_rmse: 590.21374|  0:00:08s\n",
            "epoch 81 | loss: 184625.30664| val_0_rmse: 598.26653|  0:00:08s\n",
            "epoch 82 | loss: 223100.27588| val_0_rmse: 600.81863|  0:00:08s\n",
            "epoch 83 | loss: 220380.34961| val_0_rmse: 600.76471|  0:00:08s\n",
            "epoch 84 | loss: 47991.19824| val_0_rmse: 598.33133|  0:00:09s\n",
            "epoch 85 | loss: 185184.47949| val_0_rmse: 597.71267|  0:00:09s\n",
            "epoch 86 | loss: 222367.04443| val_0_rmse: 597.11895|  0:00:09s\n",
            "epoch 87 | loss: 218807.49951| val_0_rmse: 599.75649|  0:00:09s\n",
            "epoch 88 | loss: 207659.37854| val_0_rmse: 605.45778|  0:00:09s\n",
            "epoch 89 | loss: 211814.53076| val_0_rmse: 606.13012|  0:00:09s\n",
            "epoch 90 | loss: 230440.98755| val_0_rmse: 601.87202|  0:00:09s\n",
            "epoch 91 | loss: 211442.43933| val_0_rmse: 595.77851|  0:00:09s\n",
            "epoch 92 | loss: 220840.23175| val_0_rmse: 589.16177|  0:00:09s\n",
            "epoch 93 | loss: 185502.80804| val_0_rmse: 590.11432|  0:00:09s\n",
            "epoch 94 | loss: 223851.07568| val_0_rmse: 591.15793|  0:00:09s\n",
            "epoch 95 | loss: 249927.77832| val_0_rmse: 600.42599|  0:00:09s\n",
            "epoch 96 | loss: 62370.62842| val_0_rmse: 608.85249|  0:00:10s\n",
            "epoch 97 | loss: 241597.91943| val_0_rmse: 605.73001|  0:00:10s\n",
            "epoch 98 | loss: 57812.44263| val_0_rmse: 602.59313|  0:00:10s\n",
            "epoch 99 | loss: 246337.10974| val_0_rmse: 601.59708|  0:00:10s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 74 and best_val_0_rmse = 544.90243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def evaluate_regression(model, X_test, y_test):\n",
        "    \"\"\"Evaluates a regression model's performance.\n",
        "\n",
        "    Args:\n",
        "        model (object): The trained regression model.\n",
        "        X_test (array-like): The input features for testing.\n",
        "        y_test (array-like): The true labels for testing.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics:\n",
        "              - 'rmse': Root Mean Squared Error (rounded to 2 decimal places)\n",
        "              - 'mae': Mean Absolute Error (rounded to 2 decimal places)\n",
        "              - 'r2': R² Score (rounded to 2 decimal places)\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'rmse': round(rmse, 2),\n",
        "        'mae': round(mae, 2),\n",
        "        'r2': round(r2, 2)\n",
        "    }"
      ],
      "metadata": {
        "id": "uHhiXGwtofwo"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "results_1 = evaluate_regression(reg, X_test.values, y_test.values)\n",
        "\n",
        "# Display the results\n",
        "print(results_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu6jfMA5o9lu",
        "outputId": "c7052931-d54d-45b0-f3ca-d83be849b3f7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rmse': 544.9, 'mae': 142.73, 'r2': 0.16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đánh giá : mô hình có độ sai lệch khá đáng kể, chưa giải thích tốt sự biến thiên của dữ liệu"
      ],
      "metadata": {
        "id": "maVzdzAUvKrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet**"
      ],
      "metadata": {
        "id": "-DoMqm61wcy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "-PoqYrJ4wNzQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the residual block\n",
        "def residual_block(x, filters):\n",
        "    shortcut = x\n",
        "    x = layers.Dense(filters, activation='relu')(x)\n",
        "    x = layers.Dense(filters)(x)\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Build the ResNet model\n",
        "def build_resnet(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "\n",
        "    # Stack of residual blocks\n",
        "    for _ in range(3):  # Number of residual blocks\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    # Output layer for regression\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    model_res = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "input_shape = (X_train_scaled.shape[1],)  # Number of features\n",
        "model_res = build_resnet(input_shape)\n",
        "model_res.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model_res.summary()\n",
        "\n",
        "# Fit the model\n",
        "model_res.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FgsT-glJvicR",
        "outputId": "332c41c7-ad01-4506-a2e9-6d69970616d9"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_84 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m576\u001b[0m │ input_layer_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dense_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dense_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_27 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ dense_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_27             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ add_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_88 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dense_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_28 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_28             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_89 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ activation_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_90 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dense_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_29 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ activation_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_29             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_91 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ dense_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_27             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_28             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ activation_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ activation_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_29             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,601\u001b[0m (100.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,601</span> (100.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,601\u001b[0m (100.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,601</span> (100.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 7234.8271 - val_loss: 203788656.0000\n",
            "Epoch 2/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9530.7568 - val_loss: 232960464.0000\n",
            "Epoch 3/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5629.0928 - val_loss: 161450080.0000\n",
            "Epoch 4/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3707.8176 - val_loss: 145227312.0000\n",
            "Epoch 5/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5712.4844 - val_loss: 169632064.0000\n",
            "Epoch 6/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5018.6353 - val_loss: 144603168.0000\n",
            "Epoch 7/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3046.2612 - val_loss: 125304808.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3279.3450 - val_loss: 168246112.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4618.1714 - val_loss: 215666336.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4335.1802 - val_loss: 204873728.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3672.6262 - val_loss: 195879456.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7730.2207 - val_loss: 196335536.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4098.9365 - val_loss: 196583680.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2325.7427 - val_loss: 165582608.0000\n",
            "Epoch 15/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7662.7031 - val_loss: 160163152.0000\n",
            "Epoch 16/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9739.0508 - val_loss: 207310864.0000\n",
            "Epoch 17/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3505.6836 - val_loss: 228808560.0000\n",
            "Epoch 18/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2484.5293 - val_loss: 137570944.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3581.0986 - val_loss: 169950064.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2941.3435 - val_loss: 196601152.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2376.5078 - val_loss: 174153680.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3652.1006 - val_loss: 140464064.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2394.2266 - val_loss: 162116048.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2271.6863 - val_loss: 177697456.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1403.7869 - val_loss: 158890416.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2941.0532 - val_loss: 178142528.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5902.7280 - val_loss: 148079920.0000\n",
            "Epoch 28/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2184.4548 - val_loss: 156837440.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2116.3938 - val_loss: 205239824.0000\n",
            "Epoch 30/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1232.5352 - val_loss: 127108512.0000\n",
            "Epoch 31/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2364.2400 - val_loss: 151211120.0000\n",
            "Epoch 32/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3040.1243 - val_loss: 196523088.0000\n",
            "Epoch 33/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2956.5820 - val_loss: 146026496.0000\n",
            "Epoch 34/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4135.1509 - val_loss: 152511216.0000\n",
            "Epoch 35/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1750.2185 - val_loss: 177482608.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1425.9099 - val_loss: 147582496.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1532.2731 - val_loss: 159231888.0000\n",
            "Epoch 38/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1811.6947 - val_loss: 136847584.0000\n",
            "Epoch 39/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2201.8274 - val_loss: 137853952.0000\n",
            "Epoch 40/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3783.8237 - val_loss: 170967520.0000\n",
            "Epoch 41/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1576.6488 - val_loss: 145174112.0000\n",
            "Epoch 42/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3650.5510 - val_loss: 119179376.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3092.9175 - val_loss: 160748448.0000\n",
            "Epoch 44/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1902.2405 - val_loss: 152720288.0000\n",
            "Epoch 45/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 948.5831 - val_loss: 116496328.0000\n",
            "Epoch 46/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1120.9342 - val_loss: 120982776.0000\n",
            "Epoch 47/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1097.4883 - val_loss: 129230640.0000\n",
            "Epoch 48/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1655.8182 - val_loss: 116956560.0000\n",
            "Epoch 49/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1680.2230 - val_loss: 125169024.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2539.2292 - val_loss: 125861696.0000\n",
            "Epoch 51/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1250.5356 - val_loss: 116564704.0000\n",
            "Epoch 52/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 887.7941 - val_loss: 120561248.0000\n",
            "Epoch 53/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1186.2312 - val_loss: 133925248.0000\n",
            "Epoch 54/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1033.1920 - val_loss: 112319568.0000\n",
            "Epoch 55/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1150.8442 - val_loss: 118515784.0000\n",
            "Epoch 56/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 908.6618 - val_loss: 130159256.0000\n",
            "Epoch 57/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1308.4241 - val_loss: 121536912.0000\n",
            "Epoch 58/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 955.7748 - val_loss: 104848944.0000\n",
            "Epoch 59/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 865.1533 - val_loss: 135590784.0000\n",
            "Epoch 60/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 887.0527 - val_loss: 108841872.0000\n",
            "Epoch 61/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 571.9005 - val_loss: 111657344.0000\n",
            "Epoch 62/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1244.7977 - val_loss: 129047480.0000\n",
            "Epoch 63/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 601.3607 - val_loss: 99333672.0000\n",
            "Epoch 64/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 772.4705 - val_loss: 94512264.0000\n",
            "Epoch 65/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 670.8365 - val_loss: 100166968.0000\n",
            "Epoch 66/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 620.1217 - val_loss: 110549224.0000\n",
            "Epoch 67/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 560.0513 - val_loss: 109291840.0000\n",
            "Epoch 68/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 519.0018 - val_loss: 106896208.0000\n",
            "Epoch 69/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 595.5744 - val_loss: 107586272.0000\n",
            "Epoch 70/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 617.7988 - val_loss: 103098272.0000\n",
            "Epoch 71/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 553.0505 - val_loss: 94177792.0000\n",
            "Epoch 72/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 347.7511 - val_loss: 101477768.0000\n",
            "Epoch 73/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 588.6476 - val_loss: 127185480.0000\n",
            "Epoch 74/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 508.3417 - val_loss: 82845176.0000\n",
            "Epoch 75/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 492.8580 - val_loss: 96799040.0000\n",
            "Epoch 76/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 510.6526 - val_loss: 107349128.0000\n",
            "Epoch 77/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 469.5930 - val_loss: 74798120.0000\n",
            "Epoch 78/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 590.4370 - val_loss: 86005872.0000\n",
            "Epoch 79/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 466.7868 - val_loss: 103393712.0000\n",
            "Epoch 80/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 486.2271 - val_loss: 89740912.0000\n",
            "Epoch 81/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 595.6777 - val_loss: 74708304.0000\n",
            "Epoch 82/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 474.9039 - val_loss: 103852080.0000\n",
            "Epoch 83/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 620.7184 - val_loss: 77091136.0000\n",
            "Epoch 84/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 631.0037 - val_loss: 74673416.0000\n",
            "Epoch 85/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 407.9759 - val_loss: 82268208.0000\n",
            "Epoch 86/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 935.5521 - val_loss: 99616872.0000\n",
            "Epoch 87/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 648.4373 - val_loss: 79667256.0000\n",
            "Epoch 88/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 551.2686 - val_loss: 77305608.0000\n",
            "Epoch 89/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 501.9132 - val_loss: 77506944.0000\n",
            "Epoch 90/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 554.9097 - val_loss: 86297544.0000\n",
            "Epoch 91/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 524.3923 - val_loss: 77551408.0000\n",
            "Epoch 92/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 770.8203 - val_loss: 68430232.0000\n",
            "Epoch 93/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1048.5073 - val_loss: 72444048.0000\n",
            "Epoch 94/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 864.9656 - val_loss: 53587008.0000\n",
            "Epoch 95/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1986.4429 - val_loss: 68521024.0000\n",
            "Epoch 96/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3578.8418 - val_loss: 95198336.0000\n",
            "Epoch 97/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6316.6470 - val_loss: 56814580.0000\n",
            "Epoch 98/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 28796.2910 - val_loss: 56616628.0000\n",
            "Epoch 99/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 101785.4844 - val_loss: 115391256.0000\n",
            "Epoch 100/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42389.4805 - val_loss: 31334334.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d2d41c20220>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "results_2 = evaluate_regression(model_res, X_test, y_test)\n",
        "\n",
        "# Display the results\n",
        "print(results_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cih7FdU4wtqn",
        "outputId": "3d74739a-2892-4cd4-b3bb-36d97f409cc1"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "{'rmse': 5597.71, 'mae': 5559.15, 'r2': -87.43}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sai số quá lớn, không giải thích được sự biến thiên, có thể đang bị overfitting"
      ],
      "metadata": {
        "id": "hN4Zg8-E3YuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple NN**"
      ],
      "metadata": {
        "id": "_Buq0aNn30Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "-gZZgVZO37yH"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Sequential Model\n",
        "model_simple = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=[X_train_scaled.shape[1]]),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_simple.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model_simple.summary()\n",
        "\n",
        "# Train the model\n",
        "model_simple.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1tvc_bV7329Z",
        "outputId": "dc6f263b-27c9-4d83-bc4f-9f00868faac5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_104 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m1,152\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_105 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_106 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_107 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,521\u001b[0m (45.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,521</span> (45.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,521\u001b[0m (45.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,521</span> (45.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 376461.0312 - val_loss: 428434.5312\n",
            "Epoch 2/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 179966.1719 - val_loss: 747374.5625\n",
            "Epoch 3/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 144014.0938 - val_loss: 1434318.7500\n",
            "Epoch 4/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 208460.7500 - val_loss: 2764322.2500\n",
            "Epoch 5/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 148407.6875 - val_loss: 5090626.0000\n",
            "Epoch 6/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 395843.2500 - val_loss: 9022145.0000\n",
            "Epoch 7/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 269512.0938 - val_loss: 14983600.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 125532.1406 - val_loss: 23210628.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 157465.5156 - val_loss: 35748508.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 268736.5312 - val_loss: 53006592.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 181728.9688 - val_loss: 74709760.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 202385.5938 - val_loss: 102715936.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 111863.3516 - val_loss: 137713088.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 183391.8438 - val_loss: 182160336.0000\n",
            "Epoch 15/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 435142.5938 - val_loss: 229225744.0000\n",
            "Epoch 16/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 205008.0156 - val_loss: 271174912.0000\n",
            "Epoch 17/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 264171.6250 - val_loss: 312672096.0000\n",
            "Epoch 18/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 131448.9219 - val_loss: 347888512.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 177961.1875 - val_loss: 378724288.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 146041.5156 - val_loss: 387186400.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 286364.5625 - val_loss: 391866304.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 220024.5469 - val_loss: 381918176.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 372940.6875 - val_loss: 360561600.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 347164.2500 - val_loss: 325074368.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 101710.0000 - val_loss: 287181792.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 391515.2812 - val_loss: 254299056.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 136062.6875 - val_loss: 219178960.0000\n",
            "Epoch 28/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 231698.5625 - val_loss: 186982144.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 107947.6953 - val_loss: 159966048.0000\n",
            "Epoch 30/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 162595.6562 - val_loss: 136567040.0000\n",
            "Epoch 31/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 133234.7656 - val_loss: 114711560.0000\n",
            "Epoch 32/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 83334.8906 - val_loss: 98499624.0000\n",
            "Epoch 33/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 157193.3594 - val_loss: 86520408.0000\n",
            "Epoch 34/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 114099.9297 - val_loss: 75478944.0000\n",
            "Epoch 35/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 348739.9375 - val_loss: 62215236.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 205826.1250 - val_loss: 52281712.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 219082.5312 - val_loss: 45481288.0000\n",
            "Epoch 38/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 130917.6328 - val_loss: 37111700.0000\n",
            "Epoch 39/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 315353.7812 - val_loss: 31151034.0000\n",
            "Epoch 40/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 321420.4688 - val_loss: 27093058.0000\n",
            "Epoch 41/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 201443.2812 - val_loss: 24317004.0000\n",
            "Epoch 42/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 145225.7500 - val_loss: 23101586.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 185425.4688 - val_loss: 22202298.0000\n",
            "Epoch 44/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 115007.2422 - val_loss: 22169270.0000\n",
            "Epoch 45/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 107679.8438 - val_loss: 21963560.0000\n",
            "Epoch 46/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 188941.9062 - val_loss: 20582982.0000\n",
            "Epoch 47/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 106451.6406 - val_loss: 19457728.0000\n",
            "Epoch 48/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 316171.0000 - val_loss: 18787252.0000\n",
            "Epoch 49/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 82465.5703 - val_loss: 17949260.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 115221.7266 - val_loss: 18316212.0000\n",
            "Epoch 51/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 229817.3125 - val_loss: 19150512.0000\n",
            "Epoch 52/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 141516.5469 - val_loss: 19558976.0000\n",
            "Epoch 53/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 187912.8594 - val_loss: 22766762.0000\n",
            "Epoch 54/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 197061.2969 - val_loss: 23658866.0000\n",
            "Epoch 55/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 144460.1875 - val_loss: 21508782.0000\n",
            "Epoch 56/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 114284.8438 - val_loss: 19702850.0000\n",
            "Epoch 57/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 84258.3047 - val_loss: 18649524.0000\n",
            "Epoch 58/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 108970.4141 - val_loss: 17653652.0000\n",
            "Epoch 59/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 83091.9062 - val_loss: 15946015.0000\n",
            "Epoch 60/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 136665.1562 - val_loss: 12051754.0000\n",
            "Epoch 61/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 181869.7500 - val_loss: 10947629.0000\n",
            "Epoch 62/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 216925.7188 - val_loss: 8761002.0000\n",
            "Epoch 63/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 147487.7500 - val_loss: 6788597.0000\n",
            "Epoch 64/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 110930.3516 - val_loss: 4732061.0000\n",
            "Epoch 65/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 113004.2656 - val_loss: 2656366.2500\n",
            "Epoch 66/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 139789.5469 - val_loss: 953683.1250\n",
            "Epoch 67/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 183241.8281 - val_loss: 445837.5000\n",
            "Epoch 68/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 211188.5938 - val_loss: 475709.9062\n",
            "Epoch 69/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 209858.0312 - val_loss: 669702.6875\n",
            "Epoch 70/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 135860.6562 - val_loss: 855276.7500\n",
            "Epoch 71/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 133470.2656 - val_loss: 1026758.2500\n",
            "Epoch 72/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 143197.7031 - val_loss: 932779.6875\n",
            "Epoch 73/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 203665.4844 - val_loss: 958237.6250\n",
            "Epoch 74/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 141807.0312 - val_loss: 859869.0000\n",
            "Epoch 75/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 264050.7812 - val_loss: 802141.7500\n",
            "Epoch 76/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 169459.5625 - val_loss: 894726.7500\n",
            "Epoch 77/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 97610.5938 - val_loss: 889182.8125\n",
            "Epoch 78/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 92110.0469 - val_loss: 1011268.1875\n",
            "Epoch 79/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 140074.4062 - val_loss: 1170396.5000\n",
            "Epoch 80/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 251186.2969 - val_loss: 1188841.8750\n",
            "Epoch 81/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 260084.3906 - val_loss: 1153260.0000\n",
            "Epoch 82/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 130617.3594 - val_loss: 996111.5625\n",
            "Epoch 83/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 114277.9766 - val_loss: 826677.5000\n",
            "Epoch 84/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 253226.1094 - val_loss: 808058.6875\n",
            "Epoch 85/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 166244.8438 - val_loss: 826760.7500\n",
            "Epoch 86/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 140422.2656 - val_loss: 657365.5000\n",
            "Epoch 87/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 105242.6641 - val_loss: 514884.4375\n",
            "Epoch 88/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 127202.1562 - val_loss: 488030.0938\n",
            "Epoch 89/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 87281.7422 - val_loss: 853900.6250\n",
            "Epoch 90/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 268608.1250 - val_loss: 1057125.7500\n",
            "Epoch 91/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 127279.2734 - val_loss: 1769586.7500\n",
            "Epoch 92/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 154979.9219 - val_loss: 2413234.5000\n",
            "Epoch 93/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 67091.0391 - val_loss: 3992401.7500\n",
            "Epoch 94/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 93713.3516 - val_loss: 3682618.5000\n",
            "Epoch 95/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 70044.6953 - val_loss: 4366964.5000\n",
            "Epoch 96/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 86149.3984 - val_loss: 6822344.0000\n",
            "Epoch 97/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 152435.2969 - val_loss: 11685003.0000\n",
            "Epoch 98/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 101871.3984 - val_loss: 25062154.0000\n",
            "Epoch 99/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 98059.2344 - val_loss: 38460984.0000\n",
            "Epoch 100/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 128082.8750 - val_loss: 54102008.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d2d417c7400>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "results_3 = evaluate_regression(model_simple, X_test, y_test)\n",
        "\n",
        "# Display the results\n",
        "print(results_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlEby8414Nw2",
        "outputId": "cfc51730-e657-408e-b405-96117ce0376d"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "{'rmse': 7355.41, 'mae': 7231.63, 'r2': -151.68}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cùng một vấn đề"
      ],
      "metadata": {
        "id": "C5o-MLiq6XBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "IZaqw9CC6v25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout"
      ],
      "metadata": {
        "id": "wOnzPY_IE5lo"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TIME_STEPS = 1  # You can change this value based on your needs\n",
        "X_train_lstm, y_train_lstm = create_dataset(pd.DataFrame(X_train_scaled), pd.Series(y_train), TIME_STEPS)\n",
        "X_test_lstm, y_test_lstm = create_dataset(pd.DataFrame(X_test_scaled), pd.Series(y_test), TIME_STEPS)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], X_test_lstm.shape[2]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))  # Output layer for regression (adjust this based on the number of targets)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUiNFT7nE2Co",
        "outputId": "0d5924a2-1c7b-494d-9a68-39ed3701fa08"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 278259.5625 - val_loss: 386140.3438\n",
            "Epoch 2/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 172526.3281 - val_loss: 386135.7500\n",
            "Epoch 3/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 268747.6562 - val_loss: 386130.6562\n",
            "Epoch 4/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 188537.5625 - val_loss: 386125.5000\n",
            "Epoch 5/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 389057.6875 - val_loss: 386120.1562\n",
            "Epoch 6/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 164200.3125 - val_loss: 386114.8750\n",
            "Epoch 7/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 238250.6719 - val_loss: 386109.7500\n",
            "Epoch 8/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 121464.6562 - val_loss: 386104.5625\n",
            "Epoch 9/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 387809.5000 - val_loss: 386098.9375\n",
            "Epoch 10/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 194506.2188 - val_loss: 386093.2500\n",
            "Epoch 11/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 149026.5938 - val_loss: 386087.3750\n",
            "Epoch 12/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 248688.0000 - val_loss: 386081.1562\n",
            "Epoch 13/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 378147.6875 - val_loss: 386074.9688\n",
            "Epoch 14/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 442932.8750 - val_loss: 386068.3125\n",
            "Epoch 15/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 206177.7031 - val_loss: 386061.8750\n",
            "Epoch 16/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 441742.9375 - val_loss: 386055.1250\n",
            "Epoch 17/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 108823.8203 - val_loss: 386048.3750\n",
            "Epoch 18/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 249398.5469 - val_loss: 386040.5625\n",
            "Epoch 19/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 130981.1797 - val_loss: 386033.0625\n",
            "Epoch 20/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 218586.7969 - val_loss: 386024.6562\n",
            "Epoch 21/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 147742.0156 - val_loss: 386016.2812\n",
            "Epoch 22/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 445167.9375 - val_loss: 386007.3125\n",
            "Epoch 23/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 207156.9844 - val_loss: 385998.8750\n",
            "Epoch 24/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 156987.2344 - val_loss: 385990.6250\n",
            "Epoch 25/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 274169.4375 - val_loss: 385981.2500\n",
            "Epoch 26/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 119758.6719 - val_loss: 385972.1250\n",
            "Epoch 27/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 163485.6562 - val_loss: 385961.4062\n",
            "Epoch 28/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 168577.7969 - val_loss: 385950.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 413517.3750 - val_loss: 385937.1250\n",
            "Epoch 30/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 178038.9844 - val_loss: 385925.3750\n",
            "Epoch 31/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 405123.6562 - val_loss: 385912.4375\n",
            "Epoch 32/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 238189.1875 - val_loss: 385900.4688\n",
            "Epoch 33/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 389295.8438 - val_loss: 385887.0938\n",
            "Epoch 34/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 144117.3906 - val_loss: 385874.2188\n",
            "Epoch 35/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 108657.1719 - val_loss: 385860.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 211830.9844 - val_loss: 385843.6562\n",
            "Epoch 37/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 124949.6562 - val_loss: 385827.6562\n",
            "Epoch 38/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 388318.3438 - val_loss: 385809.9062\n",
            "Epoch 39/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 112128.6875 - val_loss: 385793.4375\n",
            "Epoch 40/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 278886.1250 - val_loss: 385775.0938\n",
            "Epoch 41/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 444509.1250 - val_loss: 385756.7188\n",
            "Epoch 42/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 186938.5625 - val_loss: 385738.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 121585.0703 - val_loss: 385719.7188\n",
            "Epoch 44/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 241452.4688 - val_loss: 385698.5938\n",
            "Epoch 45/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 98009.9062 - val_loss: 385678.4062\n",
            "Epoch 46/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 238076.0000 - val_loss: 385656.3125\n",
            "Epoch 47/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 150955.5156 - val_loss: 385633.0625\n",
            "Epoch 48/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 159963.5000 - val_loss: 385610.9062\n",
            "Epoch 49/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 147999.9688 - val_loss: 385587.3438\n",
            "Epoch 50/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 246635.5156 - val_loss: 385561.1250\n",
            "Epoch 51/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 202350.2812 - val_loss: 385535.8750\n",
            "Epoch 52/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 394075.7812 - val_loss: 385508.9688\n",
            "Epoch 53/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 183484.5625 - val_loss: 385483.8750\n",
            "Epoch 54/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 439484.6875 - val_loss: 385456.6875\n",
            "Epoch 55/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 160186.3125 - val_loss: 385428.2188\n",
            "Epoch 56/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 171727.2656 - val_loss: 385398.9688\n",
            "Epoch 57/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 154603.2188 - val_loss: 385368.0938\n",
            "Epoch 58/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 307418.0312 - val_loss: 385334.4062\n",
            "Epoch 59/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 446104.8750 - val_loss: 385303.2188\n",
            "Epoch 60/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 198313.2031 - val_loss: 385274.1875\n",
            "Epoch 61/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 130614.8281 - val_loss: 385246.0938\n",
            "Epoch 62/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 137241.6875 - val_loss: 385214.5000\n",
            "Epoch 63/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 200617.8906 - val_loss: 385179.6250\n",
            "Epoch 64/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 188912.8594 - val_loss: 385145.8750\n",
            "Epoch 65/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 393126.0625 - val_loss: 385108.7500\n",
            "Epoch 66/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 237425.4062 - val_loss: 385071.6250\n",
            "Epoch 67/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 207913.5469 - val_loss: 385034.6250\n",
            "Epoch 68/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 205852.0312 - val_loss: 384998.8438\n",
            "Epoch 69/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 162075.3750 - val_loss: 384963.6875\n",
            "Epoch 70/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 310384.0000 - val_loss: 384926.4062\n",
            "Epoch 71/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 239544.4688 - val_loss: 384886.6250\n",
            "Epoch 72/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 97731.0234 - val_loss: 384851.7500\n",
            "Epoch 73/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 243323.0625 - val_loss: 384812.1250\n",
            "Epoch 74/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 208061.2656 - val_loss: 384770.1562\n",
            "Epoch 75/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 261657.2500 - val_loss: 384729.0625\n",
            "Epoch 76/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 169553.1875 - val_loss: 384690.1562\n",
            "Epoch 77/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 158332.8906 - val_loss: 384651.8750\n",
            "Epoch 78/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 100278.7266 - val_loss: 384615.5000\n",
            "Epoch 79/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 207529.7969 - val_loss: 384569.3750\n",
            "Epoch 80/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 96254.0469 - val_loss: 384528.7500\n",
            "Epoch 81/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 135220.5625 - val_loss: 384479.4062\n",
            "Epoch 82/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 234260.2969 - val_loss: 384430.7188\n",
            "Epoch 83/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 133424.1094 - val_loss: 384387.0000\n",
            "Epoch 84/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 247678.4375 - val_loss: 384343.7812\n",
            "Epoch 85/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 187625.3438 - val_loss: 384298.6250\n",
            "Epoch 86/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 300908.1875 - val_loss: 384252.4375\n",
            "Epoch 87/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 255255.9062 - val_loss: 384208.9375\n",
            "Epoch 88/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 387533.5000 - val_loss: 384165.6562\n",
            "Epoch 89/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 187509.2188 - val_loss: 384122.3438\n",
            "Epoch 90/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 380139.7812 - val_loss: 384078.8125\n",
            "Epoch 91/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 185487.6406 - val_loss: 384035.5312\n",
            "Epoch 92/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 148869.4531 - val_loss: 383991.6250\n",
            "Epoch 93/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 123101.7578 - val_loss: 383946.8750\n",
            "Epoch 94/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 121203.5000 - val_loss: 383899.6562\n",
            "Epoch 95/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 196298.2656 - val_loss: 383846.1875\n",
            "Epoch 96/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 219285.8125 - val_loss: 383797.2500\n",
            "Epoch 97/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 243803.8906 - val_loss: 383751.6562\n",
            "Epoch 98/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 249805.2969 - val_loss: 383704.8750\n",
            "Epoch 99/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 275251.2188 - val_loss: 383656.2188\n",
            "Epoch 100/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 143089.6406 - val_loss: 383610.5312\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "evaluation_metrics = evaluate_regression(model, X_test_lstm, y_test_lstm)\n",
        "# Display the results\n",
        "print(evaluation_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Y2WbQOFHcm",
        "outputId": "25124c12-5da8-4bca-93f1-d1eb44e33eba"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "{'rmse': 619.36, 'mae': 144.78, 'r2': -0.05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target 2 : NoInjured**"
      ],
      "metadata": {
        "id": "tK8WWhA7GlL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "\n",
        "X = df[ATTRIBUTES]\n",
        "y = df[LINEAR_TARGETS[1]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7nDb17DIGveo"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qua 4 model đã thử thì nhận thấy TabNet cho ra kết quả tốt nhất tính đến thời điểm hiện tại nên quyết định dùng tương tự cho các target còn lại"
      ],
      "metadata": {
        "id": "T63nyHtcHEav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TabNet Regressor\n",
        "reg_2 = TabNetRegressor()\n",
        "\n",
        "# Train the model\n",
        "reg_2.fit(\n",
        "    X_train.values, y_train.values.reshape(-1, 1),\n",
        "    eval_set=[(X_test.values, y_test.values.reshape(-1, 1))],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k993xgTmHRnv",
        "outputId": "ae4fe553-efb7-4520-f3db-482522907f99"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 36780.60132| val_0_rmse: 898.77783|  0:00:00s\n",
            "epoch 1  | loss: 30328.0672| val_0_rmse: 727.93183|  0:00:00s\n",
            "epoch 2  | loss: 37122.86316| val_0_rmse: 530.11816|  0:00:00s\n",
            "epoch 3  | loss: 28632.47192| val_0_rmse: 616.61393|  0:00:00s\n",
            "epoch 4  | loss: 39843.71173| val_0_rmse: 823.05195|  0:00:00s\n",
            "epoch 5  | loss: 36546.33862| val_0_rmse: 595.58399|  0:00:00s\n",
            "epoch 6  | loss: 39422.11987| val_0_rmse: 828.90607|  0:00:00s\n",
            "epoch 7  | loss: 34363.52478| val_0_rmse: 608.85554|  0:00:00s\n",
            "epoch 8  | loss: 38283.04907| val_0_rmse: 580.49125|  0:00:00s\n",
            "epoch 9  | loss: 38398.02673| val_0_rmse: 526.7911|  0:00:00s\n",
            "epoch 10 | loss: 37460.18542| val_0_rmse: 437.28665|  0:00:01s\n",
            "epoch 11 | loss: 34438.10913| val_0_rmse: 447.37077|  0:00:01s\n",
            "epoch 12 | loss: 29068.36884| val_0_rmse: 438.05758|  0:00:01s\n",
            "epoch 13 | loss: 28073.38892| val_0_rmse: 431.91003|  0:00:01s\n",
            "epoch 14 | loss: 32136.65759| val_0_rmse: 476.99857|  0:00:01s\n",
            "epoch 15 | loss: 31006.41809| val_0_rmse: 1200.29353|  0:00:01s\n",
            "epoch 16 | loss: 32383.17432| val_0_rmse: 1611.74329|  0:00:01s\n",
            "epoch 17 | loss: 30764.5874| val_0_rmse: 1893.14025|  0:00:01s\n",
            "epoch 18 | loss: 16800.78857| val_0_rmse: 1790.09718|  0:00:01s\n",
            "epoch 19 | loss: 28336.4248| val_0_rmse: 1719.41323|  0:00:02s\n",
            "epoch 20 | loss: 28408.35938| val_0_rmse: 790.55833|  0:00:02s\n",
            "epoch 21 | loss: 26523.37653| val_0_rmse: 486.46731|  0:00:02s\n",
            "epoch 22 | loss: 28578.25073| val_0_rmse: 446.6718|  0:00:02s\n",
            "epoch 23 | loss: 21346.9082| val_0_rmse: 449.33261|  0:00:02s\n",
            "epoch 24 | loss: 27070.04773| val_0_rmse: 421.45906|  0:00:02s\n",
            "epoch 25 | loss: 29742.71655| val_0_rmse: 421.67675|  0:00:02s\n",
            "epoch 26 | loss: 31525.96509| val_0_rmse: 436.37759|  0:00:02s\n",
            "epoch 27 | loss: 27629.46082| val_0_rmse: 423.05644|  0:00:02s\n",
            "epoch 28 | loss: 28182.1062| val_0_rmse: 415.35801|  0:00:02s\n",
            "epoch 29 | loss: 29659.02002| val_0_rmse: 419.81965|  0:00:02s\n",
            "epoch 30 | loss: 22754.599| val_0_rmse: 425.76868|  0:00:03s\n",
            "epoch 31 | loss: 30406.32837| val_0_rmse: 424.85509|  0:00:03s\n",
            "epoch 32 | loss: 31241.80359| val_0_rmse: 423.13865|  0:00:03s\n",
            "epoch 33 | loss: 29785.1911| val_0_rmse: 1772.51717|  0:00:03s\n",
            "epoch 34 | loss: 26831.37805| val_0_rmse: 2349.12612|  0:00:03s\n",
            "epoch 35 | loss: 25254.00366| val_0_rmse: 1159.69469|  0:00:03s\n",
            "epoch 36 | loss: 29127.41663| val_0_rmse: 490.05215|  0:00:03s\n",
            "epoch 37 | loss: 30016.44617| val_0_rmse: 600.11253|  0:00:03s\n",
            "epoch 38 | loss: 20787.0675| val_0_rmse: 424.10461|  0:00:03s\n",
            "epoch 39 | loss: 26613.88477| val_0_rmse: 716.3678|  0:00:03s\n",
            "epoch 40 | loss: 28291.89127| val_0_rmse: 969.3012|  0:00:04s\n",
            "epoch 41 | loss: 30483.90881| val_0_rmse: 594.64269|  0:00:04s\n",
            "epoch 42 | loss: 25931.60327| val_0_rmse: 421.1715|  0:00:04s\n",
            "epoch 43 | loss: 25718.22607| val_0_rmse: 445.39586|  0:00:04s\n",
            "epoch 44 | loss: 25813.86707| val_0_rmse: 437.05216|  0:00:04s\n",
            "epoch 45 | loss: 27314.67896| val_0_rmse: 428.03687|  0:00:04s\n",
            "epoch 46 | loss: 26036.89807| val_0_rmse: 441.2538|  0:00:04s\n",
            "epoch 47 | loss: 23634.28333| val_0_rmse: 421.31591|  0:00:04s\n",
            "epoch 48 | loss: 19786.42352| val_0_rmse: 435.02332|  0:00:05s\n",
            "epoch 49 | loss: 24540.57507| val_0_rmse: 439.07881|  0:00:05s\n",
            "epoch 50 | loss: 24770.1134| val_0_rmse: 441.36222|  0:00:05s\n",
            "epoch 51 | loss: 25907.79053| val_0_rmse: 438.92013|  0:00:05s\n",
            "epoch 52 | loss: 24233.7088| val_0_rmse: 442.3617|  0:00:05s\n",
            "epoch 53 | loss: 20361.33374| val_0_rmse: 439.17698|  0:00:05s\n",
            "epoch 54 | loss: 27398.75916| val_0_rmse: 442.04705|  0:00:05s\n",
            "epoch 55 | loss: 25711.00989| val_0_rmse: 440.94302|  0:00:06s\n",
            "epoch 56 | loss: 27025.11023| val_0_rmse: 443.63873|  0:00:06s\n",
            "epoch 57 | loss: 14604.672| val_0_rmse: 443.50565|  0:00:06s\n",
            "epoch 58 | loss: 23711.27649| val_0_rmse: 441.35414|  0:00:06s\n",
            "epoch 59 | loss: 12143.92725| val_0_rmse: 444.37448|  0:00:06s\n",
            "epoch 60 | loss: 26379.57349| val_0_rmse: 443.56706|  0:00:06s\n",
            "epoch 61 | loss: 25452.88251| val_0_rmse: 440.42654|  0:00:06s\n",
            "epoch 62 | loss: 18707.34167| val_0_rmse: 439.98857|  0:00:07s\n",
            "epoch 63 | loss: 16796.0722| val_0_rmse: 440.5897|  0:00:07s\n",
            "epoch 64 | loss: 16997.90515| val_0_rmse: 441.21088|  0:00:07s\n",
            "epoch 65 | loss: 20706.33716| val_0_rmse: 449.19249|  0:00:07s\n",
            "epoch 66 | loss: 24029.08252| val_0_rmse: 450.88537|  0:00:07s\n",
            "epoch 67 | loss: 24452.20508| val_0_rmse: 451.2487|  0:00:07s\n",
            "epoch 68 | loss: 20144.02893| val_0_rmse: 451.19006|  0:00:07s\n",
            "epoch 69 | loss: 21434.39026| val_0_rmse: 452.90289|  0:00:07s\n",
            "epoch 70 | loss: 20348.95508| val_0_rmse: 452.76598|  0:00:08s\n",
            "epoch 71 | loss: 15506.82446| val_0_rmse: 452.59137|  0:00:08s\n",
            "epoch 72 | loss: 21074.40576| val_0_rmse: 452.74961|  0:00:08s\n",
            "epoch 73 | loss: 17818.28198| val_0_rmse: 453.32156|  0:00:08s\n",
            "epoch 74 | loss: 18583.65088| val_0_rmse: 454.01084|  0:00:08s\n",
            "epoch 75 | loss: 18261.82617| val_0_rmse: 453.9581|  0:00:08s\n",
            "epoch 76 | loss: 15220.99805| val_0_rmse: 453.74474|  0:00:08s\n",
            "epoch 77 | loss: 17295.21179| val_0_rmse: 453.55597|  0:00:08s\n",
            "epoch 78 | loss: 20401.32855| val_0_rmse: 453.96759|  0:00:08s\n",
            "epoch 79 | loss: 15582.03564| val_0_rmse: 454.73237|  0:00:08s\n",
            "epoch 80 | loss: 21570.59595| val_0_rmse: 455.49497|  0:00:09s\n",
            "epoch 81 | loss: 16788.47852| val_0_rmse: 455.62704|  0:00:09s\n",
            "epoch 82 | loss: 19115.10645| val_0_rmse: 455.31302|  0:00:09s\n",
            "epoch 83 | loss: 15020.9989| val_0_rmse: 453.89472|  0:00:09s\n",
            "epoch 84 | loss: 16989.52478| val_0_rmse: 452.19329|  0:00:09s\n",
            "epoch 85 | loss: 17359.1012| val_0_rmse: 451.71274|  0:00:09s\n",
            "epoch 86 | loss: 16867.56262| val_0_rmse: 451.03024|  0:00:09s\n",
            "epoch 87 | loss: 11943.7041| val_0_rmse: 450.67629|  0:00:09s\n",
            "epoch 88 | loss: 15522.97412| val_0_rmse: 447.48765|  0:00:09s\n",
            "epoch 89 | loss: 14513.14099| val_0_rmse: 442.31277|  0:00:09s\n",
            "epoch 90 | loss: 13181.18152| val_0_rmse: 440.40151|  0:00:10s\n",
            "epoch 91 | loss: 14208.47229| val_0_rmse: 441.14356|  0:00:10s\n",
            "epoch 92 | loss: 12845.26501| val_0_rmse: 437.82749|  0:00:10s\n",
            "epoch 93 | loss: 16347.36389| val_0_rmse: 436.05697|  0:00:10s\n",
            "epoch 94 | loss: 10744.41357| val_0_rmse: 432.3614|  0:00:10s\n",
            "epoch 95 | loss: 24087.20447| val_0_rmse: 446.00987|  0:00:10s\n",
            "epoch 96 | loss: 21864.93005| val_0_rmse: 448.58476|  0:00:10s\n",
            "epoch 97 | loss: 19083.60437| val_0_rmse: 447.81723|  0:00:10s\n",
            "epoch 98 | loss: 16626.81775| val_0_rmse: 452.48869|  0:00:10s\n",
            "epoch 99 | loss: 11214.41638| val_0_rmse: 435.06202|  0:00:10s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 28 and best_val_0_rmse = 415.35801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "results_5 = evaluate_regression(reg_2, X_test.values, y_test.values)\n",
        "\n",
        "# Display the results\n",
        "print(results_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnNoTqhKHhwI",
        "outputId": "febb0bc3-cef0-45b4-9b4e-e0e280a08586"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rmse': 286200.35, 'mae': 123548.4, 'r2': -0.23}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target 3 : TotalDamageAdjusted(000US$)**"
      ],
      "metadata": {
        "id": "jj0f8LzHH8uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "\n",
        "X = df[ATTRIBUTES]\n",
        "y = df[LINEAR_TARGETS[2]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "2URG3A9BIDNv"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TabNet Regressor\n",
        "reg_3 = TabNetRegressor()\n",
        "\n",
        "# Train the model\n",
        "reg_3.fit(\n",
        "    X_train.values, y_train.values.reshape(-1, 1),\n",
        "    eval_set=[(X_test.values, y_test.values.reshape(-1, 1))],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBp4rhG-IIWw",
        "outputId": "f0b3fbd4-d74f-471b-d080-8546f2f4a5d3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 59829126144.0| val_0_rmse: 286109.74289|  0:00:00s\n",
            "epoch 1  | loss: 59332948992.0| val_0_rmse: 285418.81337|  0:00:00s\n",
            "epoch 2  | loss: 49645234176.0| val_0_rmse: 285411.34264|  0:00:00s\n",
            "epoch 3  | loss: 48122540032.0| val_0_rmse: 285986.31638|  0:00:00s\n",
            "epoch 4  | loss: 58960413696.0| val_0_rmse: 286011.34011|  0:00:00s\n",
            "epoch 5  | loss: 43847360000.0| val_0_rmse: 286300.15076|  0:00:00s\n",
            "epoch 6  | loss: 60881985536.0| val_0_rmse: 286145.4567|  0:00:00s\n",
            "epoch 7  | loss: 53245045248.0| val_0_rmse: 286295.2832|  0:00:00s\n",
            "epoch 8  | loss: 56217275392.0| val_0_rmse: 286293.32935|  0:00:00s\n",
            "epoch 9  | loss: 51271887872.0| val_0_rmse: 286259.51593|  0:00:00s\n",
            "epoch 10 | loss: 53665156096.0| val_0_rmse: 286268.26689|  0:00:01s\n",
            "epoch 11 | loss: 45710773248.0| val_0_rmse: 286245.17359|  0:00:01s\n",
            "epoch 12 | loss: 52665954560.0| val_0_rmse: 286273.14794|  0:00:01s\n",
            "epoch 13 | loss: 56987848192.0| val_0_rmse: 286207.11298|  0:00:01s\n",
            "epoch 14 | loss: 46395379712.0| val_0_rmse: 286184.72684|  0:00:01s\n",
            "epoch 15 | loss: 58568796160.0| val_0_rmse: 286174.25934|  0:00:01s\n",
            "epoch 16 | loss: 60696413824.0| val_0_rmse: 286160.3471|  0:00:01s\n",
            "epoch 17 | loss: 42858558976.0| val_0_rmse: 286135.75451|  0:00:01s\n",
            "epoch 18 | loss: 50618232832.0| val_0_rmse: 286042.77309|  0:00:01s\n",
            "epoch 19 | loss: 48347199488.0| val_0_rmse: 286017.9453|  0:00:01s\n",
            "epoch 20 | loss: 56587015168.0| val_0_rmse: 285997.40552|  0:00:02s\n",
            "epoch 21 | loss: 54219022080.0| val_0_rmse: 285951.84334|  0:00:02s\n",
            "epoch 22 | loss: 51140575488.0| val_0_rmse: 285910.66178|  0:00:02s\n",
            "epoch 23 | loss: 59398414848.0| val_0_rmse: 285863.7021|  0:00:02s\n",
            "epoch 24 | loss: 52380492032.0| val_0_rmse: 285839.19322|  0:00:02s\n",
            "epoch 25 | loss: 52207621120.0| val_0_rmse: 285924.13787|  0:00:02s\n",
            "epoch 26 | loss: 56492109824.0| val_0_rmse: 286011.08266|  0:00:02s\n",
            "epoch 27 | loss: 57420597248.0| val_0_rmse: 286171.83067|  0:00:02s\n",
            "epoch 28 | loss: 54949243392.0| val_0_rmse: 286166.7801|  0:00:02s\n",
            "epoch 29 | loss: 53715966976.0| val_0_rmse: 286169.55443|  0:00:02s\n",
            "epoch 30 | loss: 58975169536.0| val_0_rmse: 286157.3532|  0:00:03s\n",
            "epoch 31 | loss: 55652300800.0| val_0_rmse: 285880.6656|  0:00:03s\n",
            "epoch 32 | loss: 56688435456.0| val_0_rmse: 285595.65334|  0:00:03s\n",
            "epoch 33 | loss: 57816999936.0| val_0_rmse: 285308.44526|  0:00:03s\n",
            "epoch 34 | loss: 52901445760.0| val_0_rmse: 285302.65633|  0:00:03s\n",
            "epoch 35 | loss: 54286083072.0| val_0_rmse: 285370.81684|  0:00:03s\n",
            "epoch 36 | loss: 58289104384.0| val_0_rmse: 285695.23379|  0:00:03s\n",
            "epoch 37 | loss: 52083172352.0| val_0_rmse: 285963.65405|  0:00:03s\n",
            "epoch 38 | loss: 50166989568.0| val_0_rmse: 285911.34792|  0:00:03s\n",
            "epoch 39 | loss: 51975458816.0| val_0_rmse: 285901.47847|  0:00:03s\n",
            "epoch 40 | loss: 60597211136.0| val_0_rmse: 285912.01136|  0:00:03s\n",
            "epoch 41 | loss: 47429200384.0| val_0_rmse: 285959.23434|  0:00:04s\n",
            "epoch 42 | loss: 40489396992.0| val_0_rmse: 286005.59332|  0:00:04s\n",
            "epoch 43 | loss: 58447134976.0| val_0_rmse: 285996.39874|  0:00:04s\n",
            "epoch 44 | loss: 51470286848.0| val_0_rmse: 285992.98751|  0:00:04s\n",
            "epoch 45 | loss: 60681954816.0| val_0_rmse: 286000.22412|  0:00:04s\n",
            "epoch 46 | loss: 51864183808.0| val_0_rmse: 286030.28958|  0:00:04s\n",
            "epoch 47 | loss: 59940928512.0| val_0_rmse: 285690.26146|  0:00:04s\n",
            "epoch 48 | loss: 48435270400.0| val_0_rmse: 285525.92738|  0:00:04s\n",
            "epoch 49 | loss: 46865676288.0| val_0_rmse: 285362.4307|  0:00:04s\n",
            "epoch 50 | loss: 41426473984.0| val_0_rmse: 285127.61479|  0:00:04s\n",
            "epoch 51 | loss: 52998473216.0| val_0_rmse: 284987.62684|  0:00:05s\n",
            "epoch 52 | loss: 57177634304.0| val_0_rmse: 284881.18159|  0:00:05s\n",
            "epoch 53 | loss: 54281307648.0| val_0_rmse: 284849.82451|  0:00:05s\n",
            "epoch 54 | loss: 60304947200.0| val_0_rmse: 284850.94416|  0:00:05s\n",
            "epoch 55 | loss: 50533465856.0| val_0_rmse: 284876.28603|  0:00:05s\n",
            "epoch 56 | loss: 54025037824.0| val_0_rmse: 284894.10888|  0:00:05s\n",
            "epoch 57 | loss: 54374448384.0| val_0_rmse: 285057.46042|  0:00:05s\n",
            "epoch 58 | loss: 51033155584.0| val_0_rmse: 285088.42086|  0:00:05s\n",
            "epoch 59 | loss: 54289246208.0| val_0_rmse: 285104.46695|  0:00:05s\n",
            "epoch 60 | loss: 52528785408.0| val_0_rmse: 285182.16709|  0:00:06s\n",
            "epoch 61 | loss: 59739977728.0| val_0_rmse: 285601.08336|  0:00:06s\n",
            "epoch 62 | loss: 54396044288.0| val_0_rmse: 285564.44813|  0:00:06s\n",
            "epoch 63 | loss: 52543840768.0| val_0_rmse: 285574.97905|  0:00:06s\n",
            "epoch 64 | loss: 51824267776.0| val_0_rmse: 285611.85284|  0:00:06s\n",
            "epoch 65 | loss: 55703272960.0| val_0_rmse: 285619.87696|  0:00:06s\n",
            "epoch 66 | loss: 55153334784.0| val_0_rmse: 285611.86888|  0:00:06s\n",
            "epoch 67 | loss: 53896668160.0| val_0_rmse: 285595.52613|  0:00:06s\n",
            "epoch 68 | loss: 59214528512.0| val_0_rmse: 285528.30968|  0:00:07s\n",
            "epoch 69 | loss: 54411928576.0| val_0_rmse: 285360.49055|  0:00:07s\n",
            "epoch 70 | loss: 49465454080.0| val_0_rmse: 285121.37694|  0:00:07s\n",
            "epoch 71 | loss: 46150043648.0| val_0_rmse: 284732.75599|  0:00:07s\n",
            "epoch 72 | loss: 52202720256.0| val_0_rmse: 284699.37154|  0:00:07s\n",
            "epoch 73 | loss: 51191116288.0| val_0_rmse: 284759.66649|  0:00:07s\n",
            "epoch 74 | loss: 38053654016.0| val_0_rmse: 284755.90021|  0:00:07s\n",
            "epoch 75 | loss: 58751170560.0| val_0_rmse: 284752.88165|  0:00:08s\n",
            "epoch 76 | loss: 47909617664.0| val_0_rmse: 284697.00637|  0:00:08s\n",
            "epoch 77 | loss: 50515949568.0| val_0_rmse: 284686.08828|  0:00:08s\n",
            "epoch 78 | loss: 59957147648.0| val_0_rmse: 284683.3618|  0:00:08s\n",
            "epoch 79 | loss: 55316537344.0| val_0_rmse: 284710.83001|  0:00:08s\n",
            "epoch 80 | loss: 48082763776.0| val_0_rmse: 284780.49493|  0:00:08s\n",
            "epoch 81 | loss: 57829539840.0| val_0_rmse: 284859.46556|  0:00:09s\n",
            "epoch 82 | loss: 59641593856.0| val_0_rmse: 284929.70129|  0:00:09s\n",
            "epoch 83 | loss: 54005000192.0| val_0_rmse: 284993.20528|  0:00:09s\n",
            "epoch 84 | loss: 52890089216.0| val_0_rmse: 285005.16628|  0:00:09s\n",
            "epoch 85 | loss: 57374179840.0| val_0_rmse: 285029.50311|  0:00:09s\n",
            "epoch 86 | loss: 56242891776.0| val_0_rmse: 285070.3947|  0:00:09s\n",
            "epoch 87 | loss: 43342412800.0| val_0_rmse: 285079.67322|  0:00:09s\n",
            "epoch 88 | loss: 48995908096.0| val_0_rmse: 285112.23884|  0:00:09s\n",
            "epoch 89 | loss: 58571587072.0| val_0_rmse: 285129.20136|  0:00:09s\n",
            "epoch 90 | loss: 56823916544.0| val_0_rmse: 285096.55609|  0:00:10s\n",
            "epoch 91 | loss: 58277111296.0| val_0_rmse: 284996.15292|  0:00:10s\n",
            "epoch 92 | loss: 50503574528.0| val_0_rmse: 284909.73977|  0:00:10s\n",
            "epoch 93 | loss: 42013431808.0| val_0_rmse: 284917.21857|  0:00:10s\n",
            "epoch 94 | loss: 56486916096.0| val_0_rmse: 284916.42159|  0:00:10s\n",
            "epoch 95 | loss: 54564267008.0| val_0_rmse: 284900.90682|  0:00:10s\n",
            "epoch 96 | loss: 54833530880.0| val_0_rmse: 284838.68187|  0:00:10s\n",
            "epoch 97 | loss: 57423449600.0| val_0_rmse: 285207.01207|  0:00:10s\n",
            "epoch 98 | loss: 49340632576.0| val_0_rmse: 285147.22862|  0:00:10s\n",
            "epoch 99 | loss: 51522119680.0| val_0_rmse: 285078.34533|  0:00:10s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 78 and best_val_0_rmse = 284683.3618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "results_6 = evaluate_regression(reg_3, X_test.values, y_test.values)\n",
        "\n",
        "# Display the results\n",
        "print(results_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90_YoFpRIWf5",
        "outputId": "9f686264-919d-4c5e-ddd6-15d67ba79614"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rmse': 284683.36, 'mae': 122522.9, 'r2': -0.22}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}