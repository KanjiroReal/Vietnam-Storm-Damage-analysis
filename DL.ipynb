{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CEPK3_hihBRf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from src.utils import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ppt-nRO6iYj-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/STORM_preprocessed_medianfill_1.csv', index_col=0) # 200 column\n",
        "evaluate_dict = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1faU7Q2HUFo"
      },
      "source": [
        "| Mạng Deep Learning     | Điểm Mạnh                                                 | Điểm Yếu                                               | Khi Nào Dùng                                      |\n",
        "|------------------------|-----------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------|\n",
        "| **MLP (Multi-Layer Perceptron)** | - Đơn giản, dễ triển khai<br>- Hiệu quả cho dữ liệu nhỏ, đơn giản | - Không tốt cho dữ liệu có cấu trúc phức tạp<br>- Dễ overfitting với dữ liệu lớn | - Dữ liệu tabular (bảng)<br>- Khi không cần xử lý dữ liệu tuần tự hoặc không gian |\n",
        "| **RNN (Recurrent Neural Network)** | - Tốt cho dữ liệu tuần tự<br>- Phân tích chuỗi thời gian hiệu quả | - Khó huấn luyện do vanishing gradient<br>- Chậm khi xử lý chuỗi dài | - Dự báo chuỗi thời gian<br>- Phân tích lịch sử giao dịch hoặc chuỗi sự kiện |\n",
        "| **LSTM (Long Short-Term Memory)** | - Giải quyết vanishing gradient của RNN<br>- Ghi nhớ thông tin dài hạn tốt | - Tốn nhiều tài nguyên tính toán<br>- Khó tinh chỉnh | - Chuỗi thời gian dài<br>- Khi cần ghi nhớ các sự kiện quan trọng từ xa |\n",
        "| **GRU (Gated Recurrent Unit)** | - Nhẹ và nhanh hơn LSTM<br>- Hiệu quả với chuỗi ngắn | - Khả năng biểu diễn thông tin dài hạn kém hơn LSTM | - Khi cần tốc độ nhanh hơn LSTM<br>- Chuỗi thời gian ngắn |\n",
        "| **CNN (Convolutional Neural Network)** | - Khả năng trích xuất đặc trưng mạnh<br>- Phù hợp với dữ liệu hình ảnh và không gian | - Không hiệu quả với dữ liệu tuần tự | - Hồi quy trên hình ảnh (VD: dự đoán giá từ ảnh)<br>- Phân tích dữ liệu không gian |\n",
        "| **ResNet (Residual Network)** | - Khả năng xử lý mạng sâu mà không gặp vanishing gradient<br>- Hiệu quả trong trích xuất đặc trưng phức tạp | - Tốn nhiều tài nguyên tính toán<br>- Cần nhiều dữ liệu để tránh overfitting | - Khi cần xây dựng mạng sâu<br>- Dự đoán hoặc phân loại trên dữ liệu hình ảnh phức tạp |\n",
        "| **Transformers**       | - Xử lý tốt cả dữ liệu tuần tự và không gian<br>- Hiệu quả với dữ liệu lớn | - Tốn nhiều tài nguyên<br>- Cần dữ liệu lớn để huấn luyện tốt | - Dự báo chuỗi thời gian dài hạn<br>- Xử lý ngôn ngữ tự nhiên và dự báo chuỗi phức tạp |\n",
        "| **TabNet**             | - Hiệu quả với dữ liệu bảng (tabular)<br>- Có thể giải thích mô hình nhờ cơ chế chú ý (attention) | - Khó tinh chỉnh và tối ưu<br>- Cần nhiều dữ liệu hơn so với MLP | - Khi cần mô hình vừa mạnh vừa có thể giải thích<br>- Phù hợp với các bài toán dữ liệu bảng phức tạp |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Selection for Hurricane Data Regression\n",
        "\n",
        "1. **`TabNet`** \n",
        "- TabNet is highly effective for **structured tabular data**, such as historical hurricane records where features may include wind speed, pressure, sea surface temperature, and atmospheric conditions.\n",
        "- It uses a **sequential attention mechanism** that allows the model to focus on relevant features at different steps, improving interpretability.\n",
        "- Unlike traditional neural networks, TabNet balances both **accuracy and interpretability**, making it useful when understanding the contribution of each feature to predictions is essential.\n",
        "\n",
        "1. **`ResNet (Residual Network)`**\n",
        "- ResNet is a powerful architecture for **deep learning models**, especially for complex data representations. Its **residual connections** prevent gradient loss, making it effective for deeper networks.\n",
        "- While originally designed for image data, ResNet has been adapted for other types of data where deeper architectures are needed to capture complex patterns.\n",
        "\n",
        "\n",
        "1. **`LSTM (Long Short-Term Memory)`**\n",
        "- LSTM networks are ideal for **time-series data** because they can **remember long-term dependencies** and handle sequential relationships effectively. \n",
        "- Unlike standard RNNs, LSTM avoids the **vanishing gradient problem**, making it suitable for learning patterns in long-term hurricane data.\n",
        "- Since hurricanes are influenced by **seasonal cycles and long-term climatic trends**, LSTM is a great choice to capture these **temporal dependencies** across years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuFr44h5jMpG"
      },
      "source": [
        "## 1. Target 1 : TotalDeaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zZBHiCmei0l2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "CATEGORICAL_TARGETS = ['Flood', 'Slide']\n",
        "\n",
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[0]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd9rns0m0Og"
      },
      "source": [
        "### 1.1. TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4puM7fcnHwn",
        "outputId": "db745d5c-f147-4155-fb9f-b4389ba8c7dc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_tabnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtab_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabNetRegressor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aKmijHWE6Px",
        "outputId": "bd38f02c-8242-425d-fc4d-a43ac8e1c572"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 48.32,\n",
              " 'mse': 6010.4,\n",
              " 'rmse': 77.53,\n",
              " 'mae_upperbound_tolerance': -32.68,\n",
              " 'rmse_upperbound_tolerance': -51.47,\n",
              " 'mse_upperbound_tolerance': -3747.03}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DoMqm61wcy_"
      },
      "source": [
        "### 1.2. ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-PoqYrJ4wNzQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgsT-glJvicR",
        "outputId": "45a40988-1ce8-4199-8ed5-6358cef36177"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23ab41815b0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def residual_block(x, units):\n",
        "    shortcut = x\n",
        "    x = layers.Dense(units, activation='relu')(x)\n",
        "    x = layers.Dense(units)(x)  # No activation for the second layer\n",
        "    x = layers.add([x, shortcut])  # Add the shortcut\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_resnet(input_shape, output_units):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "\n",
        "    # Add several residual blocks\n",
        "    for _ in range(3):  # Adjust the number of blocks as needed\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(output_units)(x)  # For regression, no activation here\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "resnet_model = build_resnet(input_shape=(X_train_scaled.shape[1],), output_units=1)\n",
        "resnet_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_loss\", mode=\"min\")\n",
        "\n",
        "resnet_model.fit(X_train_scaled, y_train_np, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_np), callbacks=[checkpoint], verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cih7FdU4wtqn",
        "outputId": "6a8824d3-9875-45cf-e87e-0b608f84ac7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023AA84B4360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 97ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023AA84B4360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 204.56,\n",
              " 'mse': 458741.26,\n",
              " 'rmse': 677.3,\n",
              " 'mae_upperbound_tolerance': -188.91,\n",
              " 'rmse_upperbound_tolerance': -651.25,\n",
              " 'mse_upperbound_tolerance': -456477.89}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(resnet_model, X_test_scaled, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"ResNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZaqw9CC6v25"
      },
      "source": [
        "### 1.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wOnzPY_IE5lo"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUiNFT7nE2Co",
        "outputId": "b59f0392-1c37-4ec9-8211-ba6d8778df0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23ab879d730>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TIME_STEPS = 1  # You can change this value based on your needs\n",
        "X_train_lstm, y_train_lstm = create_dataset(pd.DataFrame(X_train_scaled), pd.Series(y_train), TIME_STEPS)\n",
        "X_test_lstm, y_test_lstm = create_dataset(pd.DataFrame(X_test_scaled), pd.Series(y_test), TIME_STEPS)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], X_test_lstm.shape[2]))\n",
        "\n",
        "# Build the LSTM model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Backbone\n",
        "model.add(LSTM(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# fully connected\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Y2WbQOFHcm",
        "outputId": "735923cf-cef7-4d57-d0a7-d477edf9c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 45.44,\n",
              " 'mse': 8351.36,\n",
              " 'rmse': 91.39,\n",
              " 'mae_upperbound_tolerance': -31.02,\n",
              " 'rmse_upperbound_tolerance': -66.05,\n",
              " 'mse_upperbound_tolerance': -6212.31}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_lstm, y_test_lstm, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Ujlhh40euzAT",
        "outputId": "e835ab0d-cbbc-4acc-831e-d545481b52a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e6c46_row0_col2, #T_e6c46_row0_col3, #T_e6c46_row0_col5, #T_e6c46_row0_col6, #T_e6c46_row2_col1, #T_e6c46_row2_col4 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e6c46\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e6c46_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_e6c46_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_e6c46_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_e6c46_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_e6c46_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_e6c46_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_e6c46_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e6c46_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_e6c46_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_e6c46_row0_col1\" class=\"data row0 col1\" >48.32</td>\n",
              "      <td id=\"T_e6c46_row0_col2\" class=\"data row0 col2\" >6010.40</td>\n",
              "      <td id=\"T_e6c46_row0_col3\" class=\"data row0 col3\" >77.53</td>\n",
              "      <td id=\"T_e6c46_row0_col4\" class=\"data row0 col4\" >-32.68</td>\n",
              "      <td id=\"T_e6c46_row0_col5\" class=\"data row0 col5\" >-51.47</td>\n",
              "      <td id=\"T_e6c46_row0_col6\" class=\"data row0 col6\" >-3747.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e6c46_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_e6c46_row1_col0\" class=\"data row1 col0\" >ResNet</td>\n",
              "      <td id=\"T_e6c46_row1_col1\" class=\"data row1 col1\" >204.56</td>\n",
              "      <td id=\"T_e6c46_row1_col2\" class=\"data row1 col2\" >458741.26</td>\n",
              "      <td id=\"T_e6c46_row1_col3\" class=\"data row1 col3\" >677.30</td>\n",
              "      <td id=\"T_e6c46_row1_col4\" class=\"data row1 col4\" >-188.91</td>\n",
              "      <td id=\"T_e6c46_row1_col5\" class=\"data row1 col5\" >-651.25</td>\n",
              "      <td id=\"T_e6c46_row1_col6\" class=\"data row1 col6\" >-456477.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e6c46_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_e6c46_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_e6c46_row2_col1\" class=\"data row2 col1\" >45.44</td>\n",
              "      <td id=\"T_e6c46_row2_col2\" class=\"data row2 col2\" >8351.36</td>\n",
              "      <td id=\"T_e6c46_row2_col3\" class=\"data row2 col3\" >91.39</td>\n",
              "      <td id=\"T_e6c46_row2_col4\" class=\"data row2 col4\" >-31.02</td>\n",
              "      <td id=\"T_e6c46_row2_col5\" class=\"data row2 col5\" >-66.05</td>\n",
              "      <td id=\"T_e6c46_row2_col6\" class=\"data row2 col6\" >-6212.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x23aae9c9310>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8WWhA7GlL4"
      },
      "source": [
        "## 2. Target 2 : NoInjured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7nDb17DIGveo"
      },
      "outputs": [],
      "source": [
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "CATEGORICAL_TARGETS = ['Flood', 'Slide']\n",
        "\n",
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[1]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23NDk8RjzFl7"
      },
      "source": [
        "### 2.1. TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k993xgTmHRnv",
        "outputId": "1ceb5d1b-400b-4b00-f0de-ca79553093e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 51176.87402| val_0_rmse: 419.46671|  0:00:00s\n",
            "epoch 1  | loss: 41880.02051| val_0_rmse: 419.15594|  0:00:00s\n",
            "epoch 2  | loss: 43763.24365| val_0_rmse: 418.67334|  0:00:00s\n",
            "epoch 3  | loss: 32999.54761| val_0_rmse: 417.40374|  0:00:00s\n",
            "epoch 4  | loss: 41008.11987| val_0_rmse: 415.96654|  0:00:00s\n",
            "epoch 5  | loss: 43509.63452| val_0_rmse: 415.42674|  0:00:01s\n",
            "epoch 6  | loss: 47634.48047| val_0_rmse: 414.59451|  0:00:01s\n",
            "epoch 7  | loss: 35576.15259| val_0_rmse: 416.31958|  0:00:01s\n",
            "epoch 8  | loss: 35163.25983| val_0_rmse: 415.82687|  0:00:01s\n",
            "epoch 9  | loss: 41436.69873| val_0_rmse: 414.20771|  0:00:01s\n",
            "epoch 10 | loss: 35672.5542| val_0_rmse: 412.48017|  0:00:01s\n",
            "epoch 11 | loss: 42418.5011| val_0_rmse: 410.94681|  0:00:02s\n",
            "epoch 12 | loss: 42225.78857| val_0_rmse: 410.6045|  0:00:02s\n",
            "epoch 13 | loss: 29692.9375| val_0_rmse: 410.91237|  0:00:02s\n",
            "epoch 14 | loss: 37681.67114| val_0_rmse: 411.19666|  0:00:02s\n",
            "epoch 15 | loss: 36882.25058| val_0_rmse: 410.53452|  0:00:02s\n",
            "epoch 16 | loss: 35383.07214| val_0_rmse: 410.74692|  0:00:02s\n",
            "epoch 17 | loss: 32890.95392| val_0_rmse: 410.27528|  0:00:02s\n",
            "epoch 18 | loss: 34934.38342| val_0_rmse: 410.24523|  0:00:03s\n",
            "epoch 19 | loss: 35082.21045| val_0_rmse: 408.45718|  0:00:03s\n",
            "epoch 20 | loss: 36401.43201| val_0_rmse: 407.54777|  0:00:03s\n",
            "epoch 21 | loss: 37058.03918| val_0_rmse: 406.59276|  0:00:03s\n",
            "epoch 22 | loss: 38375.1203| val_0_rmse: 402.64464|  0:00:03s\n",
            "epoch 23 | loss: 33260.5885| val_0_rmse: 395.78454|  0:00:03s\n",
            "epoch 24 | loss: 35079.8667| val_0_rmse: 394.81771|  0:00:03s\n",
            "epoch 25 | loss: 22224.27979| val_0_rmse: 395.04845|  0:00:04s\n",
            "epoch 26 | loss: 34370.70239| val_0_rmse: 393.58073|  0:00:04s\n",
            "epoch 27 | loss: 33937.43384| val_0_rmse: 394.17884|  0:00:04s\n",
            "epoch 28 | loss: 33337.86938| val_0_rmse: 398.50479|  0:00:04s\n",
            "epoch 29 | loss: 31974.66541| val_0_rmse: 407.42052|  0:00:04s\n",
            "epoch 30 | loss: 26231.62061| val_0_rmse: 412.67617|  0:00:04s\n",
            "epoch 31 | loss: 28714.83704| val_0_rmse: 413.19205|  0:00:04s\n",
            "epoch 32 | loss: 36156.06323| val_0_rmse: 412.77185|  0:00:04s\n",
            "epoch 33 | loss: 38720.45898| val_0_rmse: 412.56402|  0:00:05s\n",
            "epoch 34 | loss: 32102.61108| val_0_rmse: 411.93079|  0:00:05s\n",
            "epoch 35 | loss: 29691.41748| val_0_rmse: 411.43889|  0:00:05s\n",
            "epoch 36 | loss: 33805.34778| val_0_rmse: 411.77732|  0:00:05s\n",
            "epoch 37 | loss: 29341.46094| val_0_rmse: 412.36352|  0:00:05s\n",
            "epoch 38 | loss: 32764.28503| val_0_rmse: 412.18086|  0:00:05s\n",
            "epoch 39 | loss: 31983.60657| val_0_rmse: 411.23265|  0:00:05s\n",
            "epoch 40 | loss: 31470.11652| val_0_rmse: 410.92172|  0:00:06s\n",
            "epoch 41 | loss: 18681.86804| val_0_rmse: 411.77918|  0:00:06s\n",
            "epoch 42 | loss: 28257.11865| val_0_rmse: 411.90167|  0:00:06s\n",
            "epoch 43 | loss: 17831.68054| val_0_rmse: 409.86852|  0:00:06s\n",
            "epoch 44 | loss: 35684.45703| val_0_rmse: 407.70369|  0:00:06s\n",
            "epoch 45 | loss: 35444.94495| val_0_rmse: 406.11621|  0:00:06s\n",
            "epoch 46 | loss: 30713.15027| val_0_rmse: 405.30503|  0:00:06s\n",
            "epoch 47 | loss: 36018.27563| val_0_rmse: 405.80628|  0:00:06s\n",
            "epoch 48 | loss: 31877.98926| val_0_rmse: 406.53388|  0:00:07s\n",
            "epoch 49 | loss: 36208.7981| val_0_rmse: 408.17167|  0:00:07s\n",
            "epoch 50 | loss: 21654.21362| val_0_rmse: 411.97113|  0:00:07s\n",
            "epoch 51 | loss: 18299.51318| val_0_rmse: 412.15326|  0:00:07s\n",
            "epoch 52 | loss: 31273.90283| val_0_rmse: 411.85576|  0:00:07s\n",
            "epoch 53 | loss: 28204.24731| val_0_rmse: 413.01924|  0:00:07s\n",
            "epoch 54 | loss: 38028.27209| val_0_rmse: 413.02962|  0:00:07s\n",
            "epoch 55 | loss: 20077.5697| val_0_rmse: 411.95453|  0:00:07s\n",
            "epoch 56 | loss: 15698.51172| val_0_rmse: 412.05247|  0:00:08s\n",
            "epoch 57 | loss: 27020.24683| val_0_rmse: 409.59844|  0:00:08s\n",
            "epoch 58 | loss: 28902.7926| val_0_rmse: 411.81671|  0:00:08s\n",
            "epoch 59 | loss: 27366.37793| val_0_rmse: 414.40637|  0:00:08s\n",
            "epoch 60 | loss: 37489.61133| val_0_rmse: 417.01558|  0:00:08s\n",
            "epoch 61 | loss: 34430.40234| val_0_rmse: 416.78725|  0:00:08s\n",
            "epoch 62 | loss: 31038.45032| val_0_rmse: 416.09537|  0:00:08s\n",
            "epoch 63 | loss: 34631.6499| val_0_rmse: 413.35278|  0:00:09s\n",
            "epoch 64 | loss: 30188.08984| val_0_rmse: 406.91832|  0:00:09s\n",
            "epoch 65 | loss: 19307.80823| val_0_rmse: 403.63769|  0:00:09s\n",
            "epoch 66 | loss: 32381.13049| val_0_rmse: 403.26563|  0:00:09s\n",
            "epoch 67 | loss: 31989.65625| val_0_rmse: 405.56367|  0:00:09s\n",
            "epoch 68 | loss: 35170.0957| val_0_rmse: 410.631 |  0:00:09s\n",
            "epoch 69 | loss: 31910.21875| val_0_rmse: 411.65101|  0:00:09s\n",
            "epoch 70 | loss: 34030.49683| val_0_rmse: 410.89405|  0:00:10s\n",
            "epoch 71 | loss: 28785.198| val_0_rmse: 408.9553|  0:00:10s\n",
            "epoch 72 | loss: 30864.74231| val_0_rmse: 403.87621|  0:00:10s\n",
            "epoch 73 | loss: 34215.78503| val_0_rmse: 403.31369|  0:00:10s\n",
            "epoch 74 | loss: 34290.62561| val_0_rmse: 403.57923|  0:00:10s\n",
            "epoch 75 | loss: 29283.38892| val_0_rmse: 403.66895|  0:00:10s\n",
            "epoch 76 | loss: 26377.69897| val_0_rmse: 406.51209|  0:00:10s\n",
            "epoch 77 | loss: 30224.7998| val_0_rmse: 409.97646|  0:00:11s\n",
            "epoch 78 | loss: 34566.44849| val_0_rmse: 414.09033|  0:00:11s\n",
            "epoch 79 | loss: 32430.25537| val_0_rmse: 411.83428|  0:00:11s\n",
            "epoch 80 | loss: 29796.5896| val_0_rmse: 412.26317|  0:00:11s\n",
            "epoch 81 | loss: 23517.98364| val_0_rmse: 411.18655|  0:00:11s\n",
            "epoch 82 | loss: 22744.83801| val_0_rmse: 410.53375|  0:00:11s\n",
            "epoch 83 | loss: 31091.05957| val_0_rmse: 410.49791|  0:00:11s\n",
            "epoch 84 | loss: 19553.1001| val_0_rmse: 410.35723|  0:00:11s\n",
            "epoch 85 | loss: 34011.21362| val_0_rmse: 411.94588|  0:00:11s\n",
            "epoch 86 | loss: 29647.44629| val_0_rmse: 412.92142|  0:00:12s\n",
            "epoch 87 | loss: 34663.7251| val_0_rmse: 413.29277|  0:00:12s\n",
            "epoch 88 | loss: 34026.67456| val_0_rmse: 413.6566|  0:00:12s\n",
            "epoch 89 | loss: 28275.57605| val_0_rmse: 412.95591|  0:00:12s\n",
            "epoch 90 | loss: 27822.87891| val_0_rmse: 412.68651|  0:00:12s\n",
            "epoch 91 | loss: 31797.76538| val_0_rmse: 411.93329|  0:00:12s\n",
            "epoch 92 | loss: 31570.19092| val_0_rmse: 412.02439|  0:00:12s\n",
            "epoch 93 | loss: 32744.81787| val_0_rmse: 411.60489|  0:00:12s\n",
            "epoch 94 | loss: 21099.53906| val_0_rmse: 410.16014|  0:00:13s\n",
            "epoch 95 | loss: 23152.81494| val_0_rmse: 409.44937|  0:00:13s\n",
            "epoch 96 | loss: 31145.15234| val_0_rmse: 406.37037|  0:00:13s\n",
            "epoch 97 | loss: 20398.70898| val_0_rmse: 404.9072|  0:00:13s\n",
            "epoch 98 | loss: 18224.30859| val_0_rmse: 402.39479|  0:00:13s\n",
            "epoch 99 | loss: 28332.38623| val_0_rmse: 405.1376|  0:00:13s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 26 and best_val_0_rmse = 393.58073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnNoTqhKHhwI",
        "outputId": "bb320e74-2a19-4ba5-9758-7e544876e6ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 122.22,\n",
              " 'mse': 154905.79,\n",
              " 'rmse': 393.58,\n",
              " 'mae_upperbound_tolerance': -88.02,\n",
              " 'rmse_upperbound_tolerance': -272.35,\n",
              " 'mse_upperbound_tolerance': -105915.95}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1qM7N4Ux48L"
      },
      "source": [
        "### 2.2. ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpUIFeqfx898",
        "outputId": "6e32cfe4-786c-45a0-b98e-f161cf8fe8a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23abac1bcb0>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def residual_block(x, units):\n",
        "    shortcut = x\n",
        "    x = layers.Dense(units, activation='relu')(x)\n",
        "    x = layers.Dense(units)(x)  # No activation for the second layer\n",
        "    x = layers.add([x, shortcut])  # Add the shortcut\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_resnet(input_shape, output_units):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "\n",
        "    # Add several residual blocks\n",
        "    for _ in range(3):  # Adjust the number of blocks as needed\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(output_units)(x)  # For regression, no activation here\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "resnet_model = build_resnet(input_shape=(X_train_scaled.shape[1],), output_units=1)\n",
        "resnet_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_loss\", mode=\"min\")\n",
        "\n",
        "resnet_model.fit(X_train_scaled, y_train_np, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_np), callbacks=[checkpoint], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LInkQlhRySzJ",
        "outputId": "093d78be-b012-4c85-bb3f-43949f2fc407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 127.81,\n",
              " 'mse': 168930.73,\n",
              " 'rmse': 411.01,\n",
              " 'mae_upperbound_tolerance': -93.61,\n",
              " 'rmse_upperbound_tolerance': -289.78,\n",
              " 'mse_upperbound_tolerance': -119940.89}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(resnet_model, X_test_scaled, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"ResNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSRGFMTOykFb"
      },
      "source": [
        "### 2.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lvHQX96ylny",
        "outputId": "a7d70c9c-374a-41c3-e053-7d1458e7f17e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23abb10b470>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TIME_STEPS = 1  # You can change this value based on your needs\n",
        "X_train_lstm, y_train_lstm = create_dataset(pd.DataFrame(X_train_scaled), pd.Series(y_train), TIME_STEPS)\n",
        "X_test_lstm, y_test_lstm = create_dataset(pd.DataFrame(X_test_scaled), pd.Series(y_test), TIME_STEPS)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], X_test_lstm.shape[2]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))  # Output layer for regression (adjust this based on the number of targets)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrJOug6Uyxi6",
        "outputId": "47997b3f-ed9f-49a4-9243-c50a24c0908f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 111.92,\n",
              " 'mse': 176573.78,\n",
              " 'rmse': 420.21,\n",
              " 'mae_upperbound_tolerance': -76.94,\n",
              " 'rmse_upperbound_tolerance': -297.4,\n",
              " 'mse_upperbound_tolerance': -126298.8}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_lstm, y_test_lstm, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Yw95neRXy4Y7",
        "outputId": "33b2c672-9cdb-4e7c-adc5-fac4b97046b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_10785_row0_col2, #T_10785_row0_col3, #T_10785_row0_col5, #T_10785_row0_col6, #T_10785_row2_col1, #T_10785_row2_col4 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_10785\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_10785_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_10785_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_10785_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_10785_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_10785_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_10785_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_10785_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_10785_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_10785_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_10785_row0_col1\" class=\"data row0 col1\" >122.22</td>\n",
              "      <td id=\"T_10785_row0_col2\" class=\"data row0 col2\" >154905.79</td>\n",
              "      <td id=\"T_10785_row0_col3\" class=\"data row0 col3\" >393.58</td>\n",
              "      <td id=\"T_10785_row0_col4\" class=\"data row0 col4\" >-88.02</td>\n",
              "      <td id=\"T_10785_row0_col5\" class=\"data row0 col5\" >-272.35</td>\n",
              "      <td id=\"T_10785_row0_col6\" class=\"data row0 col6\" >-105915.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_10785_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_10785_row1_col0\" class=\"data row1 col0\" >ResNet</td>\n",
              "      <td id=\"T_10785_row1_col1\" class=\"data row1 col1\" >127.81</td>\n",
              "      <td id=\"T_10785_row1_col2\" class=\"data row1 col2\" >168930.73</td>\n",
              "      <td id=\"T_10785_row1_col3\" class=\"data row1 col3\" >411.01</td>\n",
              "      <td id=\"T_10785_row1_col4\" class=\"data row1 col4\" >-93.61</td>\n",
              "      <td id=\"T_10785_row1_col5\" class=\"data row1 col5\" >-289.78</td>\n",
              "      <td id=\"T_10785_row1_col6\" class=\"data row1 col6\" >-119940.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_10785_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_10785_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_10785_row2_col1\" class=\"data row2 col1\" >111.92</td>\n",
              "      <td id=\"T_10785_row2_col2\" class=\"data row2 col2\" >176573.78</td>\n",
              "      <td id=\"T_10785_row2_col3\" class=\"data row2 col3\" >420.21</td>\n",
              "      <td id=\"T_10785_row2_col4\" class=\"data row2 col4\" >-76.94</td>\n",
              "      <td id=\"T_10785_row2_col5\" class=\"data row2 col5\" >-297.40</td>\n",
              "      <td id=\"T_10785_row2_col6\" class=\"data row2 col6\" >-126298.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x23ab42e4500>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj0f8LzHH8uP"
      },
      "source": [
        "## 3. Target 3 : TotalDamageAdjusted(000US$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2URG3A9BIDNv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "LINEAR_TARGETS = [\"TotalDeaths\", \"NoInjured\", \"TotalDamageAdjusted(000US$)\"]\n",
        "ATTRIBUTES = ['Year', 'Month', 'MainLandfallLocation', 'OFDAResponse', 'Appeal', 'Declaration', 'LandfallMagnitude(kph)', 'LandfallPressure(mb)']\n",
        "CATEGORICAL_TARGETS = ['Flood', 'Slide']\n",
        "\n",
        "X = df[ATTRIBUTES + CATEGORICAL_TARGETS]\n",
        "y = df[LINEAR_TARGETS[2]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z31iX3jz0Mr"
      },
      "source": [
        "### 3.1 TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBp4rhG-IIWw",
        "outputId": "d4fbc1cd-c6e6-4eb3-9704-1bcfa99dbaf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 80834277376.0| val_0_rmse: 111230.12582|  0:00:00s\n",
            "epoch 1  | loss: 71588611072.0| val_0_rmse: 111229.40948|  0:00:00s\n",
            "epoch 2  | loss: 74787424256.0| val_0_rmse: 111228.98446|  0:00:00s\n",
            "epoch 3  | loss: 58221058048.0| val_0_rmse: 111227.06913|  0:00:00s\n",
            "epoch 4  | loss: 66301756416.0| val_0_rmse: 111226.37414|  0:00:00s\n",
            "epoch 5  | loss: 69256152064.0| val_0_rmse: 111224.56016|  0:00:00s\n",
            "epoch 6  | loss: 79422047232.0| val_0_rmse: 111222.99023|  0:00:01s\n",
            "epoch 7  | loss: 67533618176.0| val_0_rmse: 111219.85812|  0:00:01s\n",
            "epoch 8  | loss: 66835597824.0| val_0_rmse: 111219.45679|  0:00:01s\n",
            "epoch 9  | loss: 71169016320.0| val_0_rmse: 111216.7922|  0:00:01s\n",
            "epoch 10 | loss: 65461849088.0| val_0_rmse: 111213.91041|  0:00:01s\n",
            "epoch 11 | loss: 72556908032.0| val_0_rmse: 111209.91132|  0:00:01s\n",
            "epoch 12 | loss: 71420956160.0| val_0_rmse: 111205.8656|  0:00:01s\n",
            "epoch 13 | loss: 75314334208.0| val_0_rmse: 111203.0334|  0:00:02s\n",
            "epoch 14 | loss: 72367283456.0| val_0_rmse: 111198.49997|  0:00:02s\n",
            "epoch 15 | loss: 61482630400.0| val_0_rmse: 111195.82362|  0:00:02s\n",
            "epoch 16 | loss: 62321979392.0| val_0_rmse: 111190.88415|  0:00:02s\n",
            "epoch 17 | loss: 67939318016.0| val_0_rmse: 111187.65591|  0:00:02s\n",
            "epoch 18 | loss: 74865619968.0| val_0_rmse: 111181.94386|  0:00:02s\n",
            "epoch 19 | loss: 65791080448.0| val_0_rmse: 111178.9051|  0:00:02s\n",
            "epoch 20 | loss: 72349287424.0| val_0_rmse: 111176.2129|  0:00:03s\n",
            "epoch 21 | loss: 73964510208.0| val_0_rmse: 111172.47047|  0:00:03s\n",
            "epoch 22 | loss: 70710339584.0| val_0_rmse: 111167.86199|  0:00:03s\n",
            "epoch 23 | loss: 72998544384.0| val_0_rmse: 111156.48346|  0:00:03s\n",
            "epoch 24 | loss: 69637264384.0| val_0_rmse: 111150.58081|  0:00:03s\n",
            "epoch 25 | loss: 73888274432.0| val_0_rmse: 111148.15846|  0:00:03s\n",
            "epoch 26 | loss: 54994511872.0| val_0_rmse: 111130.65409|  0:00:03s\n",
            "epoch 27 | loss: 71380950528.0| val_0_rmse: 111122.83759|  0:00:04s\n",
            "epoch 28 | loss: 72448652288.0| val_0_rmse: 111129.6427|  0:00:04s\n",
            "epoch 29 | loss: 66708634112.0| val_0_rmse: 111127.07773|  0:00:04s\n",
            "epoch 30 | loss: 78538004480.0| val_0_rmse: 111118.10327|  0:00:04s\n",
            "epoch 31 | loss: 71028066304.0| val_0_rmse: 111108.78881|  0:00:04s\n",
            "epoch 32 | loss: 75747348480.0| val_0_rmse: 111104.2334|  0:00:04s\n",
            "epoch 33 | loss: 62321982976.0| val_0_rmse: 111114.32604|  0:00:04s\n",
            "epoch 34 | loss: 66430896640.0| val_0_rmse: 111120.65604|  0:00:05s\n",
            "epoch 35 | loss: 76973990912.0| val_0_rmse: 111116.51713|  0:00:05s\n",
            "epoch 36 | loss: 77486182400.0| val_0_rmse: 111106.48588|  0:00:05s\n",
            "epoch 37 | loss: 72823698944.0| val_0_rmse: 111089.38196|  0:00:05s\n",
            "epoch 38 | loss: 58547960832.0| val_0_rmse: 111072.59214|  0:00:05s\n",
            "epoch 39 | loss: 79497234688.0| val_0_rmse: 111057.40393|  0:00:05s\n",
            "epoch 40 | loss: 76907774208.0| val_0_rmse: 111042.2693|  0:00:05s\n",
            "epoch 41 | loss: 58571419648.0| val_0_rmse: 111040.83545|  0:00:05s\n",
            "epoch 42 | loss: 68631631872.0| val_0_rmse: 111047.73652|  0:00:06s\n",
            "epoch 43 | loss: 76559139840.0| val_0_rmse: 111040.31789|  0:00:06s\n",
            "epoch 44 | loss: 67573677824.0| val_0_rmse: 110979.65092|  0:00:06s\n",
            "epoch 45 | loss: 74531934208.0| val_0_rmse: 110964.49666|  0:00:06s\n",
            "epoch 46 | loss: 70739461120.0| val_0_rmse: 110951.08403|  0:00:06s\n",
            "epoch 47 | loss: 69389285888.0| val_0_rmse: 110937.80417|  0:00:06s\n",
            "epoch 48 | loss: 69241845760.0| val_0_rmse: 110923.4663|  0:00:06s\n",
            "epoch 49 | loss: 77071841280.0| val_0_rmse: 110901.33277|  0:00:06s\n",
            "epoch 50 | loss: 64247735040.0| val_0_rmse: 110881.97558|  0:00:07s\n",
            "epoch 51 | loss: 55594684928.0| val_0_rmse: 110873.18276|  0:00:07s\n",
            "epoch 52 | loss: 55026292224.0| val_0_rmse: 110859.06436|  0:00:07s\n",
            "epoch 53 | loss: 78401127424.0| val_0_rmse: 110833.04468|  0:00:07s\n",
            "epoch 54 | loss: 79245910528.0| val_0_rmse: 110813.41116|  0:00:07s\n",
            "epoch 55 | loss: 66670285824.0| val_0_rmse: 110791.16939|  0:00:07s\n",
            "epoch 56 | loss: 68246264832.0| val_0_rmse: 110749.46526|  0:00:08s\n",
            "epoch 57 | loss: 68650950144.0| val_0_rmse: 110696.4182|  0:00:08s\n",
            "epoch 58 | loss: 73458408448.0| val_0_rmse: 110667.8158|  0:00:08s\n",
            "epoch 59 | loss: 78398124032.0| val_0_rmse: 110688.35971|  0:00:08s\n",
            "epoch 60 | loss: 71767926528.0| val_0_rmse: 110670.59489|  0:00:08s\n",
            "epoch 61 | loss: 79344068608.0| val_0_rmse: 110642.31095|  0:00:08s\n",
            "epoch 62 | loss: 69485454336.0| val_0_rmse: 110715.418|  0:00:08s\n",
            "epoch 63 | loss: 75878661120.0| val_0_rmse: 110735.04187|  0:00:09s\n",
            "epoch 64 | loss: 66757720576.0| val_0_rmse: 110712.63956|  0:00:09s\n",
            "epoch 65 | loss: 68264519680.0| val_0_rmse: 110674.48557|  0:00:09s\n",
            "epoch 66 | loss: 79144005632.0| val_0_rmse: 110657.86656|  0:00:09s\n",
            "epoch 67 | loss: 67715133184.0| val_0_rmse: 110701.07082|  0:00:09s\n",
            "epoch 68 | loss: 65396771840.0| val_0_rmse: 110624.71675|  0:00:09s\n",
            "epoch 69 | loss: 69591901184.0| val_0_rmse: 110618.55988|  0:00:09s\n",
            "epoch 70 | loss: 68499985408.0| val_0_rmse: 110592.39299|  0:00:10s\n",
            "epoch 71 | loss: 65623320064.0| val_0_rmse: 110528.06029|  0:00:10s\n",
            "epoch 72 | loss: 76860219392.0| val_0_rmse: 110521.62418|  0:00:10s\n",
            "epoch 73 | loss: 78865899776.0| val_0_rmse: 110532.92783|  0:00:10s\n",
            "epoch 74 | loss: 76928542720.0| val_0_rmse: 110506.45582|  0:00:10s\n",
            "epoch 75 | loss: 62750684160.0| val_0_rmse: 110468.59457|  0:00:10s\n",
            "epoch 76 | loss: 76053073920.0| val_0_rmse: 110398.47307|  0:00:10s\n",
            "epoch 77 | loss: 61969806080.0| val_0_rmse: 110309.8475|  0:00:11s\n",
            "epoch 78 | loss: 70638442240.0| val_0_rmse: 110270.38272|  0:00:11s\n",
            "epoch 79 | loss: 60084745216.0| val_0_rmse: 110280.83728|  0:00:11s\n",
            "epoch 80 | loss: 75413356544.0| val_0_rmse: 110288.72931|  0:00:11s\n",
            "epoch 81 | loss: 73154498560.0| val_0_rmse: 110296.65816|  0:00:11s\n",
            "epoch 82 | loss: 74754054144.0| val_0_rmse: 110288.31578|  0:00:11s\n",
            "epoch 83 | loss: 66126390272.0| val_0_rmse: 110261.94003|  0:00:11s\n",
            "epoch 84 | loss: 65899717632.0| val_0_rmse: 110237.84158|  0:00:12s\n",
            "epoch 85 | loss: 75913819136.0| val_0_rmse: 110176.27697|  0:00:12s\n",
            "epoch 86 | loss: 71216305152.0| val_0_rmse: 110127.4764|  0:00:12s\n",
            "epoch 87 | loss: 71845545216.0| val_0_rmse: 110118.60678|  0:00:12s\n",
            "epoch 88 | loss: 74519265792.0| val_0_rmse: 110113.31409|  0:00:12s\n",
            "epoch 89 | loss: 71674893312.0| val_0_rmse: 110237.70214|  0:00:12s\n",
            "epoch 90 | loss: 72472463360.0| val_0_rmse: 110217.55587|  0:00:13s\n",
            "epoch 91 | loss: 79953707264.0| val_0_rmse: 110084.48933|  0:00:13s\n",
            "epoch 92 | loss: 65710239744.0| val_0_rmse: 110166.41579|  0:00:13s\n",
            "epoch 93 | loss: 72742767104.0| val_0_rmse: 110066.88811|  0:00:13s\n",
            "epoch 94 | loss: 73466309632.0| val_0_rmse: 110143.20126|  0:00:13s\n",
            "epoch 95 | loss: 68898066944.0| val_0_rmse: 110101.34634|  0:00:13s\n",
            "epoch 96 | loss: 72310904832.0| val_0_rmse: 110077.79927|  0:00:13s\n",
            "epoch 97 | loss: 73864062976.0| val_0_rmse: 109971.95009|  0:00:13s\n",
            "epoch 98 | loss: 68589188096.0| val_0_rmse: 110005.761|  0:00:14s\n",
            "epoch 99 | loss: 64843800064.0| val_0_rmse: 110041.91669|  0:00:14s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_rmse = 109971.95009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Convert data to NumPy arrays (if they are not already)\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = y_train.values.reshape(-1, 1)\n",
        "y_test_np = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Define the TabNet Regressor\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# Train the model with verbose set to 0\n",
        "tabnet_model.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=100,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90_YoFpRIWf5",
        "outputId": "2f407c15-b592-40ef-a7fc-f2f3dcde0a96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mae': 58275.8,\n",
              " 'mse': 12093829806.1,\n",
              " 'rmse': 109971.95,\n",
              " 'mae_upperbound_tolerance': -40384.24,\n",
              " 'rmse_upperbound_tolerance': -81804.5,\n",
              " 'mse_upperbound_tolerance': -9449145216.65}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(tabnet_model, X_test_np, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict = {}\n",
        "evaluate_dict[\"TabNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-t3sLTz2_l"
      },
      "source": [
        "### 3.2 ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzb7bu_Nz6CD",
        "outputId": "1219a11e-d901-456c-846d-5aeaca20ebe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23ac061c6b0>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def residual_block(x, units):\n",
        "    shortcut = x\n",
        "    x = layers.Dense(units, activation='relu')(x)\n",
        "    x = layers.Dense(units)(x)  # No activation for the second layer\n",
        "    x = layers.add([x, shortcut])  # Add the shortcut\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_resnet(input_shape, output_units):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "\n",
        "    # Add several residual blocks\n",
        "    for _ in range(3):  # Adjust the number of blocks as needed\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(output_units)(x)  # For regression, no activation here\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "resnet_model = build_resnet(input_shape=(X_train_scaled.shape[1],), output_units=1)\n",
        "resnet_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_loss\", mode=\"min\")\n",
        "\n",
        "resnet_model.fit(X_train_scaled, y_train_np, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test_np), callbacks=[checkpoint], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV-ur7Y50EHz",
        "outputId": "a7e803ce-9d68-44b4-f54f-058a0c4b9cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 88226.12,\n",
              " 'mse': 21733006445.02,\n",
              " 'rmse': 147421.19,\n",
              " 'mae_upperbound_tolerance': -70334.56,\n",
              " 'rmse_upperbound_tolerance': -119253.73,\n",
              " 'mse_upperbound_tolerance': -19088321855.58}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(resnet_model, X_test_scaled, y_test_np, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"ResNet\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkz7qc00cAj"
      },
      "source": [
        "### 3.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCDkn4of0dk0",
        "outputId": "6bb1e433-300d-4084-e58f-6ea2d56d8e38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anhmi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23ac3b42d20>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reshape the data for LSTM\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y.iloc[i + time_steps])  # Corresponding y value\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TIME_STEPS = 1  # You can change this value based on your needs\n",
        "X_train_lstm, y_train_lstm = create_dataset(pd.DataFrame(X_train_scaled), pd.Series(y_train), TIME_STEPS)\n",
        "X_test_lstm, y_test_lstm = create_dataset(pd.DataFrame(X_test_scaled), pd.Series(y_test), TIME_STEPS)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], X_test_lstm.shape[2]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))  # Output layer for regression (adjust this based on the number of targets)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test_lstm), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87GTF3Wp0mIz",
        "outputId": "e09dea23-8e0e-4712-fdc0-333b5381e1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mae': 60492.84,\n",
              " 'mse': 12691205641.19,\n",
              " 'rmse': 112655.25,\n",
              " 'mae_upperbound_tolerance': -42337.64,\n",
              " 'rmse_upperbound_tolerance': -84144.33,\n",
              " 'mse_upperbound_tolerance': -9981629863.11}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_values = evaluate(model, X_test_lstm, y_test_lstm, threshold=0.3, mode=\"regression\")\n",
        "evaluate_dict[\"LSTM\"] = eval_values\n",
        "\n",
        "eval_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "kfyHYmdP0sUr",
        "outputId": "843ee88f-94fa-4d58-c242-56d1f86fcc7d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_5ffb4_row0_col1, #T_5ffb4_row0_col2, #T_5ffb4_row0_col3, #T_5ffb4_row0_col4, #T_5ffb4_row0_col5, #T_5ffb4_row0_col6 {\n",
              "  color: red;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_5ffb4\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_5ffb4_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
              "      <th id=\"T_5ffb4_level0_col1\" class=\"col_heading level0 col1\" >mae</th>\n",
              "      <th id=\"T_5ffb4_level0_col2\" class=\"col_heading level0 col2\" >mse</th>\n",
              "      <th id=\"T_5ffb4_level0_col3\" class=\"col_heading level0 col3\" >rmse</th>\n",
              "      <th id=\"T_5ffb4_level0_col4\" class=\"col_heading level0 col4\" >mae_upperbound_tolerance</th>\n",
              "      <th id=\"T_5ffb4_level0_col5\" class=\"col_heading level0 col5\" >rmse_upperbound_tolerance</th>\n",
              "      <th id=\"T_5ffb4_level0_col6\" class=\"col_heading level0 col6\" >mse_upperbound_tolerance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_5ffb4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_5ffb4_row0_col0\" class=\"data row0 col0\" >TabNet</td>\n",
              "      <td id=\"T_5ffb4_row0_col1\" class=\"data row0 col1\" >58275.80</td>\n",
              "      <td id=\"T_5ffb4_row0_col2\" class=\"data row0 col2\" >12093829806.10</td>\n",
              "      <td id=\"T_5ffb4_row0_col3\" class=\"data row0 col3\" >109971.95</td>\n",
              "      <td id=\"T_5ffb4_row0_col4\" class=\"data row0 col4\" >-40384.24</td>\n",
              "      <td id=\"T_5ffb4_row0_col5\" class=\"data row0 col5\" >-81804.50</td>\n",
              "      <td id=\"T_5ffb4_row0_col6\" class=\"data row0 col6\" >-9449145216.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5ffb4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_5ffb4_row1_col0\" class=\"data row1 col0\" >ResNet</td>\n",
              "      <td id=\"T_5ffb4_row1_col1\" class=\"data row1 col1\" >88226.12</td>\n",
              "      <td id=\"T_5ffb4_row1_col2\" class=\"data row1 col2\" >21733006445.02</td>\n",
              "      <td id=\"T_5ffb4_row1_col3\" class=\"data row1 col3\" >147421.19</td>\n",
              "      <td id=\"T_5ffb4_row1_col4\" class=\"data row1 col4\" >-70334.56</td>\n",
              "      <td id=\"T_5ffb4_row1_col5\" class=\"data row1 col5\" >-119253.73</td>\n",
              "      <td id=\"T_5ffb4_row1_col6\" class=\"data row1 col6\" >-19088321855.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5ffb4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_5ffb4_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
              "      <td id=\"T_5ffb4_row2_col1\" class=\"data row2 col1\" >60495.14</td>\n",
              "      <td id=\"T_5ffb4_row2_col2\" class=\"data row2 col2\" >12691482300.88</td>\n",
              "      <td id=\"T_5ffb4_row2_col3\" class=\"data row2 col3\" >112656.48</td>\n",
              "      <td id=\"T_5ffb4_row2_col4\" class=\"data row2 col4\" >-42339.94</td>\n",
              "      <td id=\"T_5ffb4_row2_col5\" class=\"data row2 col5\" >-84145.56</td>\n",
              "      <td id=\"T_5ffb4_row2_col6\" class=\"data row2 col6\" >-9981906522.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x23abafb5b50>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare metrics value\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['color: red' if v else '' for v in is_max]\n",
        "\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['color: red' if v else '' for v in is_min]\n",
        "\n",
        "def highlight_row(row, selected_method):\n",
        "    return ['background-color: black;' if row['Method'] in selected_method else ''\n",
        "            for _ in row]\n",
        "\n",
        "selected_method = [model.__class__.__name__]\n",
        "eval_value_df = pd.DataFrame(evaluate_dict).T.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "\n",
        "eval_value_df = (\n",
        "    eval_value_df.style\n",
        "    .apply(highlight_max, subset=[\"mae_upperbound_tolerance\", \"rmse_upperbound_tolerance\", \"mse_upperbound_tolerance\"])\n",
        "    .apply(highlight_min, subset=[\"mae\", \"mse\", \"rmse\"])\n",
        "    .apply(lambda row: highlight_row(row, selected_method), axis=1 )\n",
        "    .format(precision=2)\n",
        ")\n",
        "\n",
        "eval_value_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Fezxju1LBT"
      },
      "source": [
        "TabNet hoạt động tốt với target 1, target 2 và 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
